{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification of whisper-distil.ipynb to handle bulk transcriptions of an entire directory. Added a file renaming function to remove spaces from any audio filename (as ffmpeg will cut off after the space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need cuda for vastly faster transcription\n",
    "#47 seconds for a full 30 min podcast! Insanely fast\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/distil-whisper\n",
    "# conda activate py310\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(device)\n",
    "\n",
    "# biggest model for highest quality\n",
    "model_id = \"distil-whisper/distil-large-v2\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True, \n",
    "    use_safetensors=True,\n",
    "    attn_implementation=\"flash_attention_2\")        # updated\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all audio files with spaces in their name\n",
    "# poe.com assisted code\n",
    "# Specify the directory where the files are located\n",
    "#directory = '/var/home/fraser/machine_learning/whisper.cpp/samples/'\n",
    "directory = '/var/home/fraser/Music/Voice_Memos/'\n",
    "\n",
    "# Get a list of all audio files, .m4a, .mp3, and .wav files, in the directory\n",
    "files = glob.glob(os.path.join(directory, '*.m4a')) + \\\n",
    "        glob.glob(os.path.join(directory, '*.mp3')) + \\\n",
    "        glob.glob(os.path.join(directory, '*.ogg')) + \\\n",
    "        glob.glob(os.path.join(directory, '*.wav'))\n",
    "\n",
    "# Iterate over the files (use this approach also for directory transcription)\n",
    "for file in files:\n",
    "    # If the file name contains a space\n",
    "    if ' ' in file:\n",
    "        # Replace the spaces with hyphens\n",
    "        new_name = file.replace(' ', '-')\n",
    "        # Rename the file\n",
    "        os.rename(file, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all audio files and transcribe them:\n",
    "for file in files:\n",
    "    audio_file = file\n",
    "    # convert audio file to 16-bit wav format required by whisper\n",
    "    output_file = audio_file + '-output.wav'\n",
    "    print(audio_file)\n",
    "    print(output_file)\n",
    "\n",
    "    # convert audio_file then transcribe to text\n",
    "    # overwrites existing file with same name with yes_command\n",
    "    try:\n",
    "        yes_command = f'echo \"y\" | '\n",
    "        subprocess.run([yes_command + 'ffmpeg' + ' -i ' +  audio_file + ' -ar 16000 -ac 1 -c:a pcm_s16le ' \n",
    "                        + output_file], shell=True, check=True)\n",
    "        print(\"Audio coverted successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Audio convertion failed with error {e.returncode}.\")\n",
    "\n",
    "    # pipeline\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        max_new_tokens=128,\n",
    "        chunk_length_s=15,\n",
    "        batch_size=16,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device\n",
    "    )\n",
    "    result_local = pipe(output_file)\n",
    "\n",
    "    # save transcript as a .md file\n",
    "    saved_txt=result_local[\"text\"]\n",
    "    f = open(output_file + \".md\", \"a\")\n",
    "    f.write(saved_txt)\n",
    "    f.close()\n",
    "    print(\"saved:\", f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.9s for the same file under whisper-distil with CUDA that took whisper 4m 44 seconds!\n",
    "43s for a full podcast of over 30 minutes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
