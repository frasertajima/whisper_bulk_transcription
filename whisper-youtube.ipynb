{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CUDA enabled whisper.cpp. Setup is outlined in https://github.com/ggerganov/whisper.cpp. YouTube extraction code from OPENVINO example notebook 227-whisper-nnfc-quantize.ipynb.\n",
    "\n",
    "CAUTION: MAKE SURE TO BACKUP AUDIO FILES IF THEY HAVE SPACES IN THEIR NAMES AS THEY WILL BE RENAMED (AND THE METADATA ALTERED). For file names without spaces, the original file is not renamed and a copy is made in a compatible audio format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from pytube import YouTube\n",
    "from utils import get_audio\n",
    "from utils import prepare_srt\n",
    "from pathlib import Path\n",
    "\n",
    "np.set_printoptions(linewidth=50)\n",
    "\n",
    "# change to reflect your local directory and file name\n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "directory = home_directory + '/machine_learning/whisper.cpp/samples/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eUuGdh3nBGo\n",
      "Downloading video https://youtu.be/eUuGdh3nBGo?feature=shared started\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/eUuGdh3nBGo.mp4\n"
     ]
    }
   ],
   "source": [
    "# copy and youtube share link below:\n",
    "VIDEO_LINK = \"https://youtu.be/eUuGdh3nBGo?feature=shared\"\n",
    "name = VIDEO_LINK[17:-15]\n",
    "print(name)\n",
    "\n",
    "link = widgets.Text(\n",
    "    value=VIDEO_LINK,\n",
    "    placeholder=\"Type link for video\",\n",
    "    description=\"Video:\",\n",
    "    disabled=False\n",
    ")\n",
    "link\n",
    "\n",
    "print(f\"Downloading video {link.value} started\")\n",
    "output_file = Path(directory + name + \".mp4\")\n",
    "yt = YouTube(link.value)\n",
    "yt.streams.get_lowest_resolution().download(filename=output_file)\n",
    "print(f\"Video saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/eUuGdh3nBGo.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:17:33.91, start: 0.000000, bitrate: 178 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 78 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 96 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/eUuGdh3nBGo.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  123648kB time=01:06:00.83 bitrate= 255.7kbits/s speed=1.32e+03x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size=  145435kB time=01:17:33.90 bitrate= 256.0kbits/s speed=1.33e+03x    \n",
      "video:0kB audio:145435kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000052%\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "extracted_audio_file = name + '.wav'\n",
    "\n",
    "def extract_audio(video_path, audio_path):\n",
    "    yes_command = f'echo \"y\" | '\n",
    "    command = yes_command + \"ffmpeg -i {} -vn -acodec pcm_s16le -ar 16000 -ac 1 {}\".format(video_path, audio_path)\n",
    "    subprocess.call(command, shell=True)\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    extract_audio(output_file, directory + extracted_audio_file)\n",
    "    print(\"Audio coverted successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Audio convertion failed with error {e.returncode}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/eUuGdh3nBGo.wav' (74462494 samples, 4653.9 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:13.000]   So welcome, this is going further with CUDA for Python programmers, as the name suggests this won't make too much sense unless you've got started with CUDA for Python programmers.\n",
      "[00:00:13.000 --> 00:00:19.000]   The good news is that I have a video called getting started with CUDA for Python programmers.\n",
      "[00:00:19.000 --> 00:00:28.000]   So start there, it's only a bit over an hour long, you might be surprised at how quick and easy it is to get started if you haven't.\n",
      "[00:00:28.000 --> 00:00:40.000]   So assuming that you have got started, today we're going to be looking at the most important next step of taking advantage of CUDA,\n",
      "[00:00:40.000 --> 00:00:49.000]   which is we've already learnt to take advantage of the thousands of threads that you can run simultaneously on a GPU.\n",
      "[00:00:49.000 --> 00:00:54.000]   Today we're going to learn how to take advantage of the incredibly fast memory.\n",
      "[00:00:54.000 --> 00:01:03.000]   So in the up to now, although we haven't really talked about it, the memory we've been using is what's called global memory.\n",
      "[00:01:03.000 --> 00:01:05.000]   It's basically think of this.\n",
      "[00:01:05.000 --> 00:01:14.000]   So if we have a, so this is the same book we looked at last week, which we do recommend programmingly programming massively parallel processes,\n",
      "[00:01:14.000 --> 00:01:21.000]   and the stuff we're covering today is largely covered in chapter five of that book.\n",
      "[00:01:21.000 --> 00:01:31.000]   In this CUDA mode series, there's a lecture from Thomas Fineman, which goes through this and the previous chapter in some detail.\n",
      "[00:01:31.000 --> 00:01:34.000]   And so that's actually a good video to watch.\n",
      "[00:01:34.000 --> 00:01:37.000]   Maybe after this one or before this one, either order is fine.\n",
      "[00:01:37.000 --> 00:01:42.000]   They're covering civil mode material in different ways.\n",
      "[00:01:42.000 --> 00:01:49.000]   The key thing to understand is so that we're looking at here today is that this, if we look at this box here,\n",
      "[00:01:49.000 --> 00:01:52.000]   which is basically, you can think of it as a GPU.\n",
      "[00:01:52.000 --> 00:01:55.000]   And in the GPU, you have global memory.\n",
      "[00:01:55.000 --> 00:02:05.000]   Global memory is basically what we've always been using so far when we just put, when we say like dark CUDA in PyTorch,\n",
      "[00:02:05.000 --> 00:02:09.000]   it's actually putting the tensor into global memory.\n",
      "[00:02:09.000 --> 00:02:15.000]   Global memory is pretty fast compared to some other types of memory we might be used to,\n",
      "[00:02:15.000 --> 00:02:19.000]   but it's far from the quickest memory available on the GPU.\n",
      "[00:02:19.000 --> 00:02:25.000]   In fact, this shared memory is much faster.\n",
      "[00:02:25.000 --> 00:02:32.000]   Shared memory, however, is not global, which is to say that not all of the threads can see it.\n",
      "[00:02:32.000 --> 00:02:42.000]   In fact, as this box indicates, shared memory is something that's only available\n",
      "[00:02:42.000 --> 00:02:52.000]   to the threads on a specific streaming multiprocessor, SM, or in CUDA programming world within a block.\n",
      "[00:02:52.000 --> 00:02:56.000]   So all of the different threads in a block can access shared memory.\n",
      "[00:02:56.000 --> 00:03:07.000]   And the reason we care about this is that shared memory is about 10 times faster than global memory.\n",
      "[00:03:07.000 --> 00:03:15.000]   And because CUDA with all of its simultaneous, thousands of threads running at the same time is so incredibly quick,\n",
      "[00:03:15.000 --> 00:03:22.000]   because GPUs are so incredibly quick, the speed at which you access memory turns out to matter a whole lot.\n",
      "[00:03:22.000 --> 00:03:29.000]   So being able to use this shared memory effectively is as important as being able to use the thousands of threads\n",
      "[00:03:29.000 --> 00:03:32.000]   at the same time simultaneously is important.\n",
      "[00:03:32.000 --> 00:03:39.000]   So in the last lecture, we focused on how to use all of those threads, and today we'll focus on how to use shared memory.\n",
      "[00:03:39.000 --> 00:03:50.000]   Those two things will get you quite a long way in terms of creating pretty fast CUDA code.\n",
      "[00:03:50.000 --> 00:04:07.000]   Okay, so the repo for these lectures is the CUDA mode repo, and specifically the CUDA mode slash lectures repo,\n",
      "[00:04:07.000 --> 00:04:10.000]   and in there you'll find there's a lecture five.\n",
      "[00:04:10.000 --> 00:04:15.000]   You don't have to have seen all the lectures beforehand, but they're certainly all useful.\n",
      "[00:04:15.000 --> 00:04:18.000]   Just need to have seen lecture three, which is the one I mentioned.\n",
      "[00:04:18.000 --> 00:04:27.000]   Lecture five is where today's notebook will be found, and here it is.\n",
      "[00:04:27.000 --> 00:04:39.000]   Okay, so one thing I just mentioned that I've added is a little utils.py where some of the stuff that we used last time,\n",
      "[00:04:39.000 --> 00:04:45.000]   and we'll use quite a bit, I've just put it all into a script so we can access it multiple times.\n",
      "[00:04:45.000 --> 00:04:52.000]   So we've got the C div, which is ceiling division function, the little load inline wrapper called load CUDA,\n",
      "[00:04:52.000 --> 00:05:00.000]   and the little kind of prefix we have that has the hash includes and the stuff we're going to need there.\n",
      "[00:05:00.000 --> 00:05:05.000]   And so you'll see that we're going to import those here.\n",
      "[00:05:05.000 --> 00:05:12.000]   Other than that, we're going to import all the usual stuff that we like to use.\n",
      "[00:05:12.000 --> 00:05:20.000]   Last time we used simple namespace, but I actually thought let's make things closer to,\n",
      "[00:05:20.000 --> 00:05:30.000]   let's make things closer to how CUDA does things, and let's create a little thing called Dim3.\n",
      "[00:05:30.000 --> 00:05:38.000]   So this is a 3D grid with an X, a Y, and a Z using the handy little Python named tuple functionality.\n",
      "[00:05:38.000 --> 00:05:51.000]   So here's a nice way for us, and we can, just like in CUDA, provide as many of the dimensions as we want.\n",
      "[00:05:51.000 --> 00:05:58.000]   Today we'll be doing two dimensional grids, so there'll be implicit Z equals one.\n",
      "[00:05:58.000 --> 00:06:04.000]   So we can access these as D dot X and D dot Y, for example.\n",
      "[00:06:04.000 --> 00:06:13.000]   Like before, we'll use wellitzer to print stuff from CUDA if we want to.\n",
      "[00:06:13.000 --> 00:06:18.000]   CUDA launch blocking is helpful for debugging, so you can turn that on if you want it.\n",
      "[00:06:18.000 --> 00:06:32.000]   And so today we're going to do a matrix modification of a 5120 by 256 matrix M1 by a 256 by 5120 matrix M2.\n",
      "[00:06:32.000 --> 00:06:38.000]   And this approach of going from, actually it's not true.\n",
      "[00:06:38.000 --> 00:06:40.000]   Okay, yep.\n",
      "[00:06:40.000 --> 00:06:49.000]   So like before, we're going to start by looking at pure Python, and so pure Python is going to be really, really slow.\n",
      "[00:06:49.000 --> 00:06:59.000]   So to handle that we're going to create a sample of the first matrix with the first four rows and a sample of the second matrix with the first four columns.\n",
      "[00:06:59.000 --> 00:07:01.000]   And so we'll use that for our pure Python example.\n",
      "[00:07:01.000 --> 00:07:14.000]   All right, so just to remind you what we've already done in the past is we created this simple kernel runner that goes through every block and every thread.\n",
      "[00:07:14.000 --> 00:07:22.000]   Not real blocks and threads, they're actually just integers and calls some kernel, which is not actually a kernel, it's just a function.\n",
      "[00:07:22.000 --> 00:07:29.000]   And I'm going to use stem three now, so now that we've got it to pass into that, and so this was our previous matrix multiplication.\n",
      "[00:07:29.000 --> 00:07:45.000]   We grab the row, we grab the column from the indexes we passed in, we have a guard, and we then accumulated our product for whatever particular row and column we're filling in.\n",
      "[00:07:45.000 --> 00:08:01.000]   So this is basically the ignore the extra details here, but conceptually, we're just doing this dot product, for example, to fill in.\n",
      "[00:08:01.000 --> 00:08:15.000]   This is so if we're filling in this here, then this is R, and this is C.\n",
      "[00:08:15.000 --> 00:08:29.000]   So this is comma C, and so we're doing the dot product between that column and that row.\n",
      "[00:08:29.000 --> 00:08:40.000]   And so that's what this looping is here, so I is going through all of the elements of the row in the column and multiplying adding and then putting that into the output.\n",
      "[00:08:40.000 --> 00:08:52.000]   So that's what we do, we have a so-called kernel, and then we created something that would call the kernel by calling our kernel runner, passing in the function.\n",
      "[00:08:52.000 --> 00:09:06.000]   We need our blocks and our threads per block, which are just Dim three tuples, and then we pass in our flattened data and our any other information that's required.\n",
      "[00:09:06.000 --> 00:09:22.000]   And so we can check that that matrix multiplication result using these small sample versions is close to the PyTorch version.\n",
      "[00:09:22.000 --> 00:09:36.000]   And it is, and then we also looked at the CUDA version and the CUDA version we created by pasting the kernel into chat GPT, and it's bad out something which we hardly had to change at all to get this.\n",
      "[00:09:36.000 --> 00:09:50.000]   And the kernel runner also looks very similar, except that the syntax for calling a kernel in CUDA is different with this weird triple angle bracket.\n",
      "[00:09:50.000 --> 00:10:01.000]   To make life a little bit simpler for myself, you might remember before we had the CPP source where we would copy and paste that into a string.\n",
      "[00:10:01.000 --> 00:10:10.000]   I got a bit bored of doing that manually, so I created a little get signature function that just uses a regular expression to automatically find that line of code.\n",
      "[00:10:10.000 --> 00:10:21.000]   And so for the rest of this lesson, I will be getting this CPP source automatically, and that way I don't have to worry about changing it.\n",
      "[00:10:21.000 --> 00:10:31.000]   But you can see that red jacks is just returning the necessary line of code, plus the semicolon.\n",
      "[00:10:31.000 --> 00:10:35.000]   So that makes life a little bit simpler, and I like being simple.\n",
      "[00:10:35.000 --> 00:10:41.000]   Okay, so then we go load CUDA, it ran very quickly because I've already compiled this once and PyTorch caches that.\n",
      "[00:10:41.000 --> 00:10:55.000]   And this is actually another change I made since last time, is that in the load CUDA function, if you don't pass in a name, then it uses the function's name.\n",
      "[00:10:55.000 --> 00:11:07.000]   And that means that PyTorch will cache different versions of your code with different names for the various different things you're doing, so you won't lose your cache each time, so that's handy as well.\n",
      "[00:11:07.000 --> 00:11:17.000]   Okay, so now we can use the full matrices because we're going to be nice and fast, we need them to be contiguous in CUDA, so we'll create M1C and M2C for that.\n",
      "[00:11:17.000 --> 00:11:28.000]   And they should be similar to the result of PyTorch doing it, and it takes about six milliseconds.\n",
      "[00:11:28.000 --> 00:11:41.000]   One thing I wondered about is how long of that six milliseconds was it actually running the matrix modification compared to doing all this other stuff before it.\n",
      "[00:11:41.000 --> 00:11:51.000]   So I just commented out those two lines of code and re-ran it, and that took about 50 microseconds, so that's 0.05 milliseconds.\n",
      "[00:11:51.000 --> 00:12:00.000]   So very little of this time is kind of overhead, most of this is actually doing a matrix modification, so I think that's a encouraging start.\n",
      "[00:12:00.000 --> 00:12:06.000]   Okay, so how do we take advantage of shared memory?\n",
      "[00:12:06.000 --> 00:12:25.000]   The problem here is that in our inner loop here, M and N are global memory, and so in this loop that's happening K times, we are reading from global memory again and again and again.\n",
      "[00:12:25.000 --> 00:12:34.000]   And that is a bit of a bummer, so there's a better thing we could do, which is instead we could use shared memory.\n",
      "[00:12:34.000 --> 00:12:54.000]   Now the problem is that shared memory is quite small, so we can't just dump everything into shared memory for every single thread, because we've got lots and lots of threads running at the same time, or I should say for every block.\n",
      "[00:12:54.000 --> 00:13:06.000]   So we've got lots of blocks running at the same time, if every one of them had the entire matrices in memory for every block, that's going to be an enormous amount of data, and that's going to be far too much for our GPU to handle.\n",
      "[00:13:06.000 --> 00:13:21.000]   So to deal with that, what we do is we do something called tiling, and a tile is basically, so we're going to pick a tile width of 16, so here it says tile width here we're going to use 16.\n",
      "[00:13:21.000 --> 00:13:47.000]   We basically say, okay if we're going to calculate this comma c thing here, right, instead of doing the entire dot product of all of this row, and all of this column, what we could instead do is just grab the first little bit of that row.\n",
      "[00:13:47.000 --> 00:13:52.000]   And the first little bit of that column.\n",
      "[00:13:52.000 --> 00:14:05.000]   We could take the dot product of those, and put them into our comma c, and then we could do that again for the next tile across, and the next tile across and so forth, and this is what this dot here is.\n",
      "[00:14:05.000 --> 00:14:21.000]   And the next tile across, and so then eventually we get to this bit of the row by this bit of the column, take the dot product of those and add them up to the existing comma c output we've already got.\n",
      "[00:14:21.000 --> 00:14:37.000]   And so that's just, it's doing exactly the same thing, but rather than doing the dot product all at once, we're doing it one step at a time.\n",
      "[00:14:37.000 --> 00:14:49.000]   It's not interesting of itself, but what is interesting is you might notice that, let's say for calculating this bit here.\n",
      "[00:14:49.000 --> 00:15:05.000]   Let's say, so this is thread zero comma zero, we can do the same thing, we can take the first little bit of this, and the first little bit of this, and take their dot product, and that gives us the first piece.\n",
      "[00:15:05.000 --> 00:15:21.000]   We need of that one, we can do that again and again and again until eventually we get to this one, we do this bit times this bit, and we keep going all the way to the end until there's the final tile at the end.\n",
      "[00:15:21.000 --> 00:15:27.000]   And once we've done that for all of the bits eventually we're going to have the correct answer in zero comma zero.\n",
      "[00:15:27.000 --> 00:15:39.000]   Why is that interesting? Well, it's interesting because we could reorder this, rather than doing the whole first little bit of this row, and then the next bit of that row and the next bit of that row and the next bit of that row.\n",
      "[00:15:39.000 --> 00:15:54.000]   Instead, what we could do is we could calculate zero comma zero for the first tile, and then we could calculate zero comma one for the first tile.\n",
      "[00:15:54.000 --> 00:16:04.000]   And notice this zero comma one, it's exactly the same row as we had before, right, but a different column.\n",
      "[00:16:04.000 --> 00:16:10.000]   Now with a normal kind of CPU style thinking, you'd say like, oh, maybe this will be in the cache, so this could be faster.\n",
      "[00:16:10.000 --> 00:16:24.000]   And that doesn't work in GPU programming and GPU programming, we instead use shared memory, so we could have put this into shared memory, and if we had done so, then the second time we use it, we don't have to read it from global memory, it's already there in shared memory.\n",
      "[00:16:24.000 --> 00:16:39.000]   And then the same thing will happen when we get to the second row, right, we could put that into shared memory, and then we go the second row of that tile times the first column of that tile is needed to do.\n",
      "[00:16:39.000 --> 00:16:41.000]   One comma zero.\n",
      "[00:16:41.000 --> 00:16:48.000]   And if you think about it, we've already accessed the first column of the tile in N.\n",
      "[00:16:48.000 --> 00:16:54.000]   So if we put that in shared memory as well, then we won't have to get that from global memory either.\n",
      "[00:16:54.000 --> 00:17:09.000]   So maybe you see where this is going, what we're going to be able to do actually is before we do any work is we'll put this whole tile into shared memory, and we'll put this whole tile into shared memory.\n",
      "[00:17:09.000 --> 00:17:21.000]   And then we'll take the matrix modification of the two tiles, and that will give us all of the first pieces of the entire tile output.\n",
      "[00:17:21.000 --> 00:17:35.000]   And then we'll do the same for the tile one to the right of this one, and one underneath this one, and we'll take the matrix product of those and add it to this again, and so forth until eventually again, we get up to here, we put that whole tile\n",
      "[00:17:35.000 --> 00:17:46.000]   into shared memory, we put that whole tile into shared memory, we take the matrix product, which again, remember, it's just lots and lots of dot products, the column and row dot products.\n",
      "[00:17:46.000 --> 00:17:52.000]   And so all of those are going to be able to use shared memory, and we can we add them to the outputs here.\n",
      "[00:17:52.000 --> 00:18:01.000]   And so once we eventually do that for all of the tiles, we will have finished calculating these outputs.\n",
      "[00:18:01.000 --> 00:18:11.000]   So how many times did we read from global memory, each of the input elements only got read from global memory once.\n",
      "[00:18:11.000 --> 00:18:19.000]   And the soon as we grabbed it, all we did with it was we put it into shared memory, and then the actual dot product was entirely done from shared memory.\n",
      "[00:18:19.000 --> 00:18:28.000]   And that's how we make this faster.\n",
      "[00:18:28.000 --> 00:18:41.000]   So, to do that, let's use Python, plain Python.\n",
      "[00:18:41.000 --> 00:18:52.000]   And we're going to basically try to design something in Python that looks a lot like how CUDA is going to do it, and then we're going to auto generate CUDA just like we have in the past.\n",
      "[00:18:52.000 --> 00:19:05.000]   So, in CUDA, the kind of maximally flexible way to do things is what's called dynamic shared memory, where you tell CUDA how much shared memory you're going to want.\n",
      "[00:19:05.000 --> 00:19:15.000]   And it puts it aside for you, and then in basically one contiguous block with a pointer to that block that you will have access to, which is the same as an array.\n",
      "[00:19:15.000 --> 00:19:19.000]   And then you basically grab from that block any of the pieces you want.\n",
      "[00:19:19.000 --> 00:19:32.000]   In Python, we can do exactly the same kind of thing by using a trick, which is true for both NumPy arrays and PyTorch Tenses, which is that views into those tenses are writable.\n",
      "[00:19:32.000 --> 00:19:44.000]   So, if we create a tensor of length five, and then we create a view of the first three elements, and of the last two elements, called B and C.\n",
      "[00:19:44.000 --> 00:19:58.000]   If we then modify B, it actually changes A, because they're a view of the same memory, and if we change C, it'll also change A.\n",
      "[00:19:58.000 --> 00:20:05.000]   And so, that's going to be handy for us, you'll see why in a moment, we're going to basically use our shared memory like that.\n",
      "[00:20:05.000 --> 00:20:19.000]   Now, the thing is, we've got to restructure our kernel runner a little bit, because we have two steps now.\n",
      "[00:20:19.000 --> 00:20:26.000]   Step number one is copy all of our input into shared memory, and then step two is take the dot product.\n",
      "[00:20:26.000 --> 00:20:36.000]   And so that doesn't quite work with our previous approach, because we just have one big loop, and we just have one thing that we do.\n",
      "[00:20:36.000 --> 00:20:44.000]   So, I've changed our kernel runner to create a shared memory kernel runner.\n",
      "[00:20:44.000 --> 00:20:51.000]   I've still got the same loop through blocks dot y, the same loop through blocks dot x, this is all pure Python again.\n",
      "[00:20:51.000 --> 00:21:00.000]   And here I'm going to create our shared memory, and so this is now going to be passed the shared memory into each function.\n",
      "[00:21:00.000 --> 00:21:04.000]   So, all of our threads are going to be have access to the same shared memory.\n",
      "[00:21:04.000 --> 00:21:17.000]   Now, we don't actually create the threads here, so instead, step number one is in my kernel, I'm actually going to do the loop through the threads manually.\n",
      "[00:21:17.000 --> 00:21:25.000]   We'll improve this in a moment, don't worry, it's pretty messy with all this kind of duplicate code, but at least it's nice and simple to understand.\n",
      "[00:21:25.000 --> 00:21:34.000]   So, first of all, let's just run this and confirm we get the same answer as before, and we do, so let's see what's happening.\n",
      "[00:21:34.000 --> 00:21:42.000]   The bit that does the running is exactly the same, except that I'm calling our new shared memory runner.\n",
      "[00:21:42.000 --> 00:21:52.000]   And I'm also telling it, the third thing you have to pass in is the shared size, is how much shared memory.\n",
      "[00:21:52.000 --> 00:22:04.000]   So, how much shared memory do we need? We need tile width times tile width, because that's the size of the tile, is tile width by tile width.\n",
      "[00:22:04.000 --> 00:22:08.000]   But we're going to need two of them, one for m, and one for n.\n",
      "[00:22:08.000 --> 00:22:14.000]   So, the amount of shared memory we need is tile width times tile width times 2.\n",
      "[00:22:14.000 --> 00:22:18.000]   So, that's what this is, tile width times tile width times 2.\n",
      "[00:22:18.000 --> 00:22:25.000]   So, that's going to be passed in as the shared memory size, and that will be constructed here, torch.zerus.\n",
      "[00:22:25.000 --> 00:22:46.000]   Okay, so, that shared then gets passed into our kernel, our pretend kernel, and it's just one big continuous block of memory, so we have to grab the first share size bits, and that will be our m shared memory.\n",
      "[00:22:46.000 --> 00:22:56.000]   So, our two inputs are m and n, and this, everything from there onwards is going to be our n shared memory.\n",
      "[00:22:56.000 --> 00:23:15.000]   So, then what we do is we loop through, this is exactly the same as we have before, in fact, I should use cdiv here to make it a bit more obvious what's going on cdiv.\n",
      "[00:23:15.000 --> 00:23:32.000]   So, we go through every element in the dot product we're going to need, and so the indexing starts to get a bit complicated here, so pH is what the book, and therefore we will use, which is basically the index of what tile are we up to.\n",
      "[00:23:32.000 --> 00:23:56.000]   So, we loop through each tile, so we loop through each tile, so the number of tiles we'll need is the size of the k dimension, so that's the number of columns in m, or the number of rows in n, and then divide that by the tile width, and that tells you how many tiles will fit.\n",
      "[00:23:56.000 --> 00:24:11.000]   And we do a ceiling division to go all the way to the end. So then we need to know, so let's say we're doing again we're doing this r comma c1 here right.\n",
      "[00:24:11.000 --> 00:24:25.000]   So, we need to know where this is, where does it start, and the answer is that we've done pH lots of tiles so far.\n",
      "[00:24:25.000 --> 00:24:39.000]   Each one has jumped across TW or tile width, so this distance here is pH times tile width, and we're going to call that IDX.\n",
      "[00:24:39.000 --> 00:24:52.000]   So this is an important tip, I found I had a lot of trouble getting this to like settled in my head until I drew it all out, and wrote on my picture or whatever thing is.\n",
      "[00:24:52.000 --> 00:25:10.000]   So, and I've been doing this with a help also with my friend, Karen who works with me at answer.ai, and he found the same thing we were both like, our first attempts were both to do it just in code, and we did not get it working until we actually started drawing it out.\n",
      "[00:25:10.000 --> 00:25:24.000]   And that's when Karen and I actually were like, oh, okay, that all makes sense. So that's what IDX is, right. And so notice IDX is that, but it's also, because these are symmetric, it's also that.\n",
      "[00:25:24.000 --> 00:25:28.000]   That length is also IDX.\n",
      "[00:25:28.000 --> 00:25:41.000]   Okay, so now we need to fill in the shared memory so we've got two sets of threads, one to fill in the shared memory, and one to do the matrix product.\n",
      "[00:25:41.000 --> 00:25:54.000]   That in fuel shared to the dot products from shared.\n",
      "[00:25:54.000 --> 00:26:01.000]   Okay, so we did go through all of our threads, find out what row and column we're in.\n",
      "[00:26:01.000 --> 00:26:06.000]   So how do we find out what row and column we're in and again these are the things that get complicated.\n",
      "[00:26:06.000 --> 00:26:20.000]   So this is R, as we've already mentioned, so R is going to be equal to, look, there's two pieces of it, there's the IDX piece, goes from here to here, and then there's also an additional piece, which is from here to here.\n",
      "[00:26:20.000 --> 00:26:34.000]   What is that piece? Well, that piece is simply the coordinates of this grid location within the tile.\n",
      "[00:26:34.000 --> 00:26:38.000]   And so remember that we are looping through.\n",
      "[00:26:38.000 --> 00:26:44.000]   So block dim dot y and block dim dot x is the size of the tile.\n",
      "[00:26:44.000 --> 00:26:52.000]   Right, so that means that, all right, so we've got tile row and tile column.\n",
      "[00:26:52.000 --> 00:27:03.000]   And so that's what that is, so that's, so therefore this here is tile row, and this here is tile column.\n",
      "[00:27:03.000 --> 00:27:21.000]   And so therefore to find R we have to add together IDX plus TR, and here it is, IDX plus TR, and that needs to be less than the second dimension of the matrix.\n",
      "[00:27:21.000 --> 00:27:29.000]   And then here, we just need to index into it.\n",
      "[00:27:29.000 --> 00:27:37.000]   So if this was a two dimensional tensor, we could just do TR comma TC, but it's not, it's one dimensional.\n",
      "[00:27:37.000 --> 00:27:45.000]   So we have to flatten out our dimension, so it becomes TR times TW plus TC.\n",
      "[00:27:45.000 --> 00:27:58.000]   So this is filling in our M shared and N shared by going through all the possible elements of the tile and filling them all in.\n",
      "[00:27:58.000 --> 00:28:04.000]   Okay, so after this bunch of loops is complete.\n",
      "[00:28:04.000 --> 00:28:19.000]   MS and NS will simply contain a copy of the appropriate tile from M and N. And again, here the indexing we're doing is, so this remember is the kind of element that we're up to in terms of the column.\n",
      "[00:28:19.000 --> 00:28:29.000]   And this is the row that we're doing, but we have to do times C, sorry, times K in order to deal with the fact that we've flattened out our indexes.\n",
      "[00:28:29.000 --> 00:28:43.000]   If one thing to think about that you might have been wondering is what about this final tile that goes off the edge, so it's not big enough.\n",
      "[00:28:43.000 --> 00:28:44.000]   So what happens there?\n",
      "[00:28:44.000 --> 00:28:52.000]   So for that final tile, we put in zeros, so we call that padding.\n",
      "[00:28:52.000 --> 00:29:05.000]   And so they show that in the book here, so in this case they're doing a four by four matrix multiplication containing two by two grids.\n",
      "[00:29:05.000 --> 00:29:12.000]   And you can see here when we're doing this one, we've actually, sorry, it's a three by three using a two by two grid.\n",
      "[00:29:12.000 --> 00:29:25.000]   So we get to this piece here, it goes off the edge. So what happens when we go off the edge, we just put zeros in to the shared memory.\n",
      "[00:29:25.000 --> 00:29:35.000]   And so that means then when we do the dot product between this one here containing zeros, and this one here containing zeros, then the zeros can just be ignored.\n",
      "[00:29:35.000 --> 00:29:40.000]   They don't do anything because they're just zeros.\n",
      "[00:29:40.000 --> 00:29:51.000]   So that's why we put zeros in if we are outside the dimensions of the matrix for both M and N.\n",
      "[00:29:51.000 --> 00:29:55.000]   So now that is filled in our shared memory or our pretend shared memory.\n",
      "[00:29:55.000 --> 00:29:59.000]   I mean, it is shared memory, it's just not any faster because we're just in Python.\n",
      "[00:29:59.000 --> 00:30:06.000]   And so now we've done that, we can go through all the threads, again, find out what row and column we're in using exactly the same code.\n",
      "[00:30:06.000 --> 00:30:17.000]   And then we can go through our tile width and aggregate all of the bits of our product.\n",
      "[00:30:17.000 --> 00:30:20.000]   All right, so why is we aggregating through tile width?\n",
      "[00:30:20.000 --> 00:30:29.000]   Because the dot product will always be between tile width on this side and tile width on this side.\n",
      "[00:30:29.000 --> 00:30:37.000]   So everyone, every row from here and every column from here will be a size Tw.\n",
      "[00:30:37.000 --> 00:30:39.000]   So that's why we do that.\n",
      "[00:30:39.000 --> 00:30:42.000]   Okay, so.\n",
      "[00:30:42.000 --> 00:30:46.000]   Okay, so that's that.\n",
      "[00:30:46.000 --> 00:30:50.000]   Rather messy, tiled matrix modification in Python.\n",
      "[00:30:50.000 --> 00:30:52.000]   So then I, but like this is the place to start.\n",
      "[00:30:52.000 --> 00:30:58.000]   So if you don't understand anything, come back to here because you can run it through in the debugger, you can print out what.\n",
      "[00:30:58.000 --> 00:31:04.000]   The shared memory looks like, you know, you can make sure you understand exactly what's going on, because it's plain Python.\n",
      "[00:31:04.000 --> 00:31:17.000]   And so then all I've had did is I basically said, okay, well, effectively, that is saying, oh, run this bit of code as all the threads, and then run this bit of code as all the threads.\n",
      "[00:31:17.000 --> 00:31:27.000]   So just to refactor this a little bit, I created a run threads function that just says, okay, look through all the threads and call some function.\n",
      "[00:31:27.000 --> 00:31:30.000]   And so using this approach.\n",
      "[00:31:30.000 --> 00:31:36.000]   So with this function available, we can now change our loop.\n",
      "[00:31:36.000 --> 00:31:43.000]   So that instead of having to do this big for loop, we can just say, run.\n",
      "[00:31:43.000 --> 00:31:50.000]   This function in every thread and run this function in every thread.\n",
      "[00:31:50.000 --> 00:31:58.000]   And so then those functions just contain the two pieces. So this is now going to be closer to what the cuda code is going to look like.\n",
      "[00:31:58.000 --> 00:32:09.000]   The cuda code is going to have something that says, go through each tile, and then fill the shared using all the threads, and then do the product using all the threads.\n",
      "[00:32:09.000 --> 00:32:16.000]   Okay, so this is identical to the last one. We've just refactored out the loops.\n",
      "[00:32:16.000 --> 00:32:20.000]   So it's going to get a little bit closer to what the final cuda code will look like.\n",
      "[00:32:20.000 --> 00:32:23.000]   The thing that calls it is identical.\n",
      "[00:32:23.000 --> 00:32:29.000]   And of course, therefore, the results the same.\n",
      "[00:32:29.000 --> 00:32:33.000]   Are there any questions so far?\n",
      "[00:32:33.000 --> 00:32:45.000]   I think he asked about the relationship between blocks and the tile size.\n",
      "[00:32:45.000 --> 00:33:02.000]   Sure. Yeah, so in cuda, a block is a, as we learned in the last one of these lectures, a block is, and is just a kind of a conceptual thing that could the cuda programming model provides.\n",
      "[00:33:02.000 --> 00:33:12.000]   It's just a bunch of numbers, basically, that are passed to your function as block IDX.\n",
      "[00:33:12.000 --> 00:33:19.000]   And you know that all of the threads in a block will be running on the same SM, on the same streaming multiprocessor.\n",
      "[00:33:19.000 --> 00:33:23.000]   What you do with that information is entirely up to you.\n",
      "[00:33:23.000 --> 00:33:29.000]   And last time we did nothing with that information.\n",
      "[00:33:29.000 --> 00:33:37.000]   This time we're going to, we're taking advantage of it to say like, okay, well, everything in a block has access to the same shared memory.\n",
      "[00:33:37.000 --> 00:33:49.000]   So we decided that we will treat a block as something that is calculating one particular part of our output, a tile.\n",
      "[00:33:49.000 --> 00:33:52.000]   So that's what we called it. We just called it a tile.\n",
      "[00:33:52.000 --> 00:34:03.000]   So a tile is just a, is a semantic thing that we're using. And by mapping that semantic idea of a tile to the cuda programming models idea of a block.\n",
      "[00:34:03.000 --> 00:34:07.000]   And basically saying, okay, we're going to treat each block as a tile.\n",
      "[00:34:07.000 --> 00:34:12.000]   It's going to allow us to use one block to calculate one tile in our output.\n",
      "[00:34:12.000 --> 00:34:21.000]   And so therefore we're going to have one sort of shared memory for each block, which we're mapping to each tile in our output.\n",
      "[00:34:21.000 --> 00:34:33.000]   So you can kind of think of them as being the same thing, but they're kind of conceptually different.\n",
      "[00:34:33.000 --> 00:34:43.000]   Okay, so now we're going to make it even more cuda-like because actually cuda code does not have a thing called run threads.\n",
      "[00:34:43.000 --> 00:34:45.000]   It doesn't look like this.\n",
      "[00:34:45.000 --> 00:35:00.000]   Instead, in cuda code, there is no loop like this, but instead, all of these functions across all of these possible i0 and i1 coordinates are run at the same time.\n",
      "[00:35:00.000 --> 00:35:03.000]   I mean, not necessarily the same time, but they can be the same.\n",
      "[00:35:03.000 --> 00:35:06.000]   They can all be the same time or some subset of the same time.\n",
      "[00:35:06.000 --> 00:35:11.000]   Conceptually, for the programming model, you think of them as all running at the same time.\n",
      "[00:35:11.000 --> 00:35:15.000]   To do that in Python, we have to use threads.\n",
      "[00:35:15.000 --> 00:35:29.000]   Now, in real life, Python threads don't actually all run at the same time, except in certain situations, at least with the current version of Python, because there's a thing called the global interpreter lock.\n",
      "[00:35:29.000 --> 00:35:31.000]   They actually run one other the other.\n",
      "[00:35:31.000 --> 00:35:34.000]   But again, for the programming model, we can ignore that.\n",
      "[00:35:34.000 --> 00:35:39.000]   So we're just going to pretend that they actually are in parallel.\n",
      "[00:35:39.000 --> 00:35:47.000]   To create threads, we use Python's threading library that has a thread class.\n",
      "[00:35:47.000 --> 00:35:52.000]   Let me show you a couple of interesting things here.\n",
      "[00:35:52.000 --> 00:36:03.000]   I've got a function here called g that just prints whatever you pass it, and it prints the negative of whatever you pass it, and it prints whatever you pass it times 10.\n",
      "[00:36:03.000 --> 00:36:09.000]   I'm going to call g using a bunch of threads.\n",
      "[00:36:09.000 --> 00:36:14.000]   One convenient way to create and run a bunch of threads is with a thread pool executor.\n",
      "[00:36:14.000 --> 00:36:22.000]   This is going to create three threads and run them at the same time, or as much as that this Python can handle.\n",
      "[00:36:22.000 --> 00:36:35.000]   And so that thread pool dot map basically will run all the numbers from one up to num and call our function g.\n",
      "[00:36:35.000 --> 00:36:41.000]   So it'll call this three times.\n",
      "[00:36:41.000 --> 00:36:51.000]   So if I just comment out these mysterious lines of code, you can see what it does is it runs all of them.\n",
      "[00:36:51.000 --> 00:36:59.000]   For the first thread, and then all of them for the second thread, and then all of them for the third thread.\n",
      "[00:36:59.000 --> 00:37:12.000]   This is not going to work for us because we want all of our threads to first complete the task of fill in shared memory, and then all of them to complete the task of doing the dot product.\n",
      "[00:37:12.000 --> 00:37:24.000]   So we need to have a way to tell a thread to stop until all of the threads are up to this point, and in Python, that thing is called a barrier.\n",
      "[00:37:24.000 --> 00:37:34.000]   And so we can create a barrier, like so, and we can say create a barrier so that until three threads have hit that barrier, stop.\n",
      "[00:37:34.000 --> 00:37:40.000]   So that's what, and so then we're going to pass that in this sync barrier, SP sync barrier.\n",
      "[00:37:40.000 --> 00:37:50.000]   And so it's going to pause at the sync barrier until all the threads are here, and then pause at this sync barrier until all the threads are here.\n",
      "[00:37:50.000 --> 00:38:00.000]   And now if you run it, you can see they all complete the first task, and then they all complete the second task, and then they all complete the third task.\n",
      "[00:38:00.000 --> 00:38:07.000]   And you'll see they don't necessarily do it in the same order, because threads can happen in any order.\n",
      "[00:38:07.000 --> 00:38:22.000]   And so this is the trick, which is going to allow us to have a single loop, which everything in that loop first does the shared memory filling in task, and then does the dot product task.\n",
      "[00:38:22.000 --> 00:38:33.000]   So here is our new kernel runner, as before it goes to each block, as before it creates our shared memory.\n",
      "[00:38:33.000 --> 00:38:39.000]   And it's now going to create a synchronization barrier containing the number of threads.\n",
      "[00:38:39.000 --> 00:38:45.000]   So threads per block y times thread per block x is how many threads there will be.\n",
      "[00:38:45.000 --> 00:38:52.000]   And then we're going to create a whole bunch of threads, one for every y and one for every x.\n",
      "[00:38:52.000 --> 00:38:59.000]   If you haven't seen this before in Python, if you have two things in a list comprehension, it just does the Cartesian product of those.\n",
      "[00:38:59.000 --> 00:39:02.000]   This will go through every anything and y and everything in x.\n",
      "[00:39:02.000 --> 00:39:07.000]   And so O and P will be our two coordinates.\n",
      "[00:39:07.000 --> 00:39:21.000]   So create new thread, the function that it's going to call is whatever function you asked for, and the arguments are going to be the coordinates of the block, the coordinates of the thread.\n",
      "[00:39:21.000 --> 00:39:31.000]   And then we'll say how many threads per block, passing the shared memory, passing the synchronization barrier, and any arguments you requested.\n",
      "[00:39:31.000 --> 00:39:38.000]   And so now this looks like actually, as you'll see, like CUDA code.\n",
      "[00:39:38.000 --> 00:39:48.000]   We can figure out what our row and column is, using exactly the approach we saw before.\n",
      "[00:39:48.000 --> 00:40:07.000]   Although now our row and column are actually going to be based on block idx and block dim, because this is actually telling us whereabouts we are.\n",
      "[00:40:07.000 --> 00:40:10.000]   The shared memory is exactly the same as before.\n",
      "[00:40:10.000 --> 00:40:23.000]   And so again, we loop through all of our tiles, and again, we set the shared memory, just like before, but you'll notice now we don't need two separate loops.\n",
      "[00:40:23.000 --> 00:40:30.000]   We just do the set the shared memory piece, and then we say wait for the synchronization barrier.\n",
      "[00:40:30.000 --> 00:40:42.000]   So remember that this is happening for every output value in the tile simultaneously.\n",
      "[00:40:42.000 --> 00:40:45.000]   As far as the programming model is concerned, it's simultaneously.\n",
      "[00:40:45.000 --> 00:40:49.000]   In fact, in Python, it doesn't do a good job of actually paralyzing it.\n",
      "[00:40:49.000 --> 00:40:57.000]   And in CUDA, we don't know for sure if they're happening exactly the same time, but as far as the programming model is concerned, we should think of them as happening at the same time.\n",
      "[00:40:57.000 --> 00:41:04.000]   So all of these different coordinates are running conceptually at the same time.\n",
      "[00:41:04.000 --> 00:41:13.000]   And so when we hit wait here, that means that all of the threads have finished running those two lines of code.\n",
      "[00:41:13.000 --> 00:41:17.000]   And so now we know that MS and NS are filled in for that tile.\n",
      "[00:41:17.000 --> 00:41:20.000]   And so now we can go ahead and do the dot product.\n",
      "[00:41:20.000 --> 00:41:33.000]   And then once every thread has done its own dot product, we then need to stop and wait until they're all done.\n",
      "[00:41:33.000 --> 00:41:38.000]   And then once they're all done, we can go ahead and fill in the next tile shared memory.\n",
      "[00:41:38.000 --> 00:41:49.000]   This is very important to have this wait here, because if this wait wasn't here, then some of them will still be going ahead and doing the dot product and others will be replacing the shared memory.\n",
      "[00:41:49.000 --> 00:41:52.000]   And that was going to give you wrong answers.\n",
      "[00:41:52.000 --> 00:42:02.000]   So you have to wait after you've completed the shared memory filling in, and you have to wait after you've completed doing the dot product.\n",
      "[00:42:02.000 --> 00:42:06.000]   Okay, this code's identical to before.\n",
      "[00:42:06.000 --> 00:42:10.000]   And again, it's giving us the same answer, so that is a good sign.\n",
      "[00:42:10.000 --> 00:42:26.000]   So here's the cool thing, I then took this code, and I passed it into chat GPT, and I said convert the following Python code to CUDA C, and I pointed out that you can remove these from the argument list.\n",
      "[00:42:26.000 --> 00:42:28.000]   So we don't need those in the argument list.\n",
      "[00:42:28.000 --> 00:42:34.000]   I mean obviously you can manually remove this, but I just thought if I have one prompt that always works, I don't have to do anything manually.\n",
      "[00:42:34.000 --> 00:42:40.000]   I said change sync B dot wait to sync threads, and I said for creating shared.\n",
      "[00:42:40.000 --> 00:42:42.000]   So we'll talk about all this in a moment.\n",
      "[00:42:42.000 --> 00:42:47.000]   So basically told about the minor things that would have to change to convert the Python into CUDA C.\n",
      "[00:42:47.000 --> 00:42:52.000]   And the thing, it gave me work first time.\n",
      "[00:42:52.000 --> 00:43:00.000]   Although I did do some minor cleanups, but this is the code it created after my minor cleanups.\n",
      "[00:43:00.000 --> 00:43:11.000]   So you'll see now it's getting so it's converted the, it's recognizes that we need float arrays for our input and output matrices.\n",
      "[00:43:11.000 --> 00:43:17.000]   It's typed all of those things correctly.\n",
      "[00:43:17.000 --> 00:43:33.000]   And so I, in my clean up, I added a few things, so I've got now the, the tile column is thread ID X, the tile row, thread ID X dot Y, and then we've got R and C, just like before.\n",
      "[00:43:33.000 --> 00:43:38.000]   Now CUDA, the way it does shared memory is a little bit weird.\n",
      "[00:43:38.000 --> 00:43:43.000]   It doesn't get passed in, just like thread ID X and block ID X don't get passed in.\n",
      "[00:43:43.000 --> 00:43:49.000]   You just have to put this magic incantation in exactly one line of code in your, in your kernel.\n",
      "[00:43:49.000 --> 00:43:52.000]   And so here it is, here's this one line of code.\n",
      "[00:43:52.000 --> 00:43:56.000]   And then following that, you say what data type you want your shared memory to be.\n",
      "[00:43:56.000 --> 00:43:59.000]   And then you say what you want to call it.\n",
      "[00:43:59.000 --> 00:44:01.000]   And that's an array.\n",
      "[00:44:01.000 --> 00:44:08.000]   So this is created something called MS, which is the pointer to the start of the shared memory that CUDA is going to create.\n",
      "[00:44:08.000 --> 00:44:12.000]   That's what X turned under shared means.\n",
      "[00:44:12.000 --> 00:44:15.000]   So MS is a pointer to the start of the shared memory.\n",
      "[00:44:15.000 --> 00:44:20.000]   We need NS to be a pointer to the start of the second half of the shared memory.\n",
      "[00:44:20.000 --> 00:44:31.000]   So go in tile width times tile width, because that will finish off the M part of the shared memory that's tile width by tile width.\n",
      "[00:44:31.000 --> 00:44:34.000]   And the second half is the N part, which is tile width by tile width.\n",
      "[00:44:34.000 --> 00:44:36.000]   So remember we did this in the Python version as well.\n",
      "[00:44:36.000 --> 00:44:42.000]   So if any of this is confusing, go back to the Python version and step through it in a debugger.\n",
      "[00:44:42.000 --> 00:44:45.000]   So now we've got MS and NS as our shared memory.\n",
      "[00:44:45.000 --> 00:44:50.000]   And then this is exactly the same as the Python version.\n",
      "[00:44:50.000 --> 00:44:53.000]   But we've now got this sync threads.\n",
      "[00:44:53.000 --> 00:45:04.000]   So sync threads, underscore, underscore, sync threads.\n",
      "[00:45:04.000 --> 00:45:10.000]   Underscore, underscore sync threads is identical to the sync B dot wait.\n",
      "[00:45:10.000 --> 00:45:20.000]   It says wait until all of the threads are finished doing the previous lines of code before any of them are allowed to do the next one.\n",
      "[00:45:20.000 --> 00:45:26.000]   Because this stuff's built into CUDA, we don't have to create a sync barrier object and pass it in and all that.\n",
      "[00:45:26.000 --> 00:45:30.000]   You just have this magic line of code.\n",
      "[00:45:30.000 --> 00:45:36.000]   So there's quite a bit of magic in CUDA like this extend under shared and like this underscore underscore sync threads.\n",
      "[00:45:36.000 --> 00:45:41.000]   But there's not too many pieces and you can see we're basically using them all here.\n",
      "[00:45:41.000 --> 00:45:49.000]   So the next part is then to call the kernel.\n",
      "[00:45:49.000 --> 00:45:54.000]   And so when we call the kernel, we've got the normal triple angle brackets blocks, threads per block.\n",
      "[00:45:54.000 --> 00:46:01.000]   And then we pass in one third argument to the triple angle brackets, which is how much shared memory do you want to create.\n",
      "[00:46:01.000 --> 00:46:07.000]   And so that is what's going to be used automatically when it creates this shared memory that we get a pointer to here.\n",
      "[00:46:07.000 --> 00:46:10.000]   That's how much shared memory it will create.\n",
      "[00:46:10.000 --> 00:46:13.000]   How much shared memory could you create?\n",
      "[00:46:13.000 --> 00:46:18.000]   Well, in this case, I've commented out this section, so ignore that for a moment.\n",
      "[00:46:18.000 --> 00:46:20.000]   For now, we're just going to do the same thing.\n",
      "[00:46:20.000 --> 00:46:30.000]   We're just going to make the tile width 16. So the amount of shared memory we need in bytes is tile width times tile width for M times two for N as well.\n",
      "[00:46:30.000 --> 00:46:37.000]   Time size of float because we don't want bytes. We want floats. So that's the amount of bytes of shared memory we need.\n",
      "[00:46:37.000 --> 00:46:42.000]   And that's what we pass in.\n",
      "[00:46:42.000 --> 00:46:58.000]   OK, so that's basically that. And so we can then run that and we can see that we get the same result, which is good.\n",
      "[00:46:58.000 --> 00:47:03.000]   One thing, though, which is a bit of a worry, is that our time is actually slightly worse.\n",
      "[00:47:03.000 --> 00:47:06.000]   It's gone from six milliseconds to six and a half milliseconds.\n",
      "[00:47:06.000 --> 00:47:19.000]   So we'll talk about that in a moment. I just want to mention one other thing that is in the book, they say, OK, for your size, you should write some function to calculate what size it should be.\n",
      "[00:47:19.000 --> 00:47:29.000]   But they never say how to do that. And so in future lectures, we'll be talking about how to think about things like this and how to design this correctly.\n",
      "[00:47:29.000 --> 00:47:36.000]   But in this commented out section here, you can see the basic idea. So this will work if you run it, even though it's not necessarily optimized.\n",
      "[00:47:36.000 --> 00:47:43.000]   So you can call the special function CUDA get device properties.\n",
      "[00:47:43.000 --> 00:47:51.000]   Passing in a structure to fill in. So and means a pointer to that. So that's like a reference to the structure to fill in.\n",
      "[00:47:51.000 --> 00:48:01.000]   And I think this is the device number if I remember correctly, and it will return back a structure containing a number of things, including max threads per block.\n",
      "[00:48:01.000 --> 00:48:06.000]   So and it'll also give you shared memory per block.\n",
      "[00:48:06.000 --> 00:48:17.000]   So you can use that to dynamically figure out threads per block and to dynamically figure out your tile width and stuff like that.\n",
      "[00:48:17.000 --> 00:48:26.000]   I'm not saying this is an optimized way to do any of those things. It's just an indicative kind of showing you how you can get all the pieces you can need to calculate that.\n",
      "[00:48:26.000 --> 00:48:37.000]   And so in later lectures, we will learn more about how to actually figure out what would be the optimal tile width and shared memory size and so forth.\n",
      "[00:48:37.000 --> 00:48:48.000]   But for now, I'm just using 16. Okay, so this is the mystery part. The mystery part is this is slower as we saw.\n",
      "[00:48:48.000 --> 00:48:59.000]   But if I take the exact same code and instead I use this thing where we tell it what size to create is called dynamic shared memory allocation.\n",
      "[00:48:59.000 --> 00:49:09.000]   If we don't use dynamic shared memory allocation, then we do that by not passing in the shared memory size here.\n",
      "[00:49:09.000 --> 00:49:26.000]   But instead, if we know at compile time, how big we want our tiles to be. So we can try, I've tried both 32 or 16. You can actually create an MS of TW by TW and an NS of TW by TW.\n",
      "[00:49:26.000 --> 00:49:35.000]   So you can have two separate things and because we know we're deciding at compile time, what our tile width is, then this is not dynamically created.\n",
      "[00:49:35.000 --> 00:49:43.000]   The rest of the code is the same except now we've got proper kind of two dimensional indexing, which is nice.\n",
      "[00:49:43.000 --> 00:49:59.000]   And with this one, this is faster. So we've gone down from six to five. And I think if I remember correctly, when I tried 16 tile width, it's a bit faster to it's more like four.\n",
      "[00:49:59.000 --> 00:50:08.000]   That will have that running in the background. 16.\n",
      "[00:50:08.000 --> 00:50:13.000]   Okay, let's recompile that.\n",
      "[00:50:13.000 --> 00:50:17.000]   Okay, any more questions before I move on.\n",
      "[00:50:17.000 --> 00:50:32.000]   So there's one question from Jean Mieser, he asked why the size is like TW times, TW times, two times size of floor. I think it's supposed to produce where the two comes from.\n",
      "[00:50:32.000 --> 00:50:41.000]   So we have to the shared memory, we need to be able to store the tile for M and the tile for N.\n",
      "[00:50:41.000 --> 00:50:53.000]   So each one of those is TW by TW. And so therefore we need two times TW by TW in order to have enough room for both of those two input tiles.\n",
      "[00:50:53.000 --> 00:50:58.000]   And then we use it.\n",
      "[00:50:58.000 --> 00:51:03.000]   Here, we've got a pointer to the start of M, we've got a pointer to the start of N.\n",
      "[00:51:03.000 --> 00:51:13.000]   We also saw it in the Python version, the shared memory size.\n",
      "[00:51:13.000 --> 00:51:19.000]   We passed in.\n",
      "[00:51:19.000 --> 00:51:31.000]   TW times TW times two, because we needed the MS, M's shared memory tile and N's shared memory tile.\n",
      "[00:51:31.000 --> 00:51:36.000]   Okay, thank you for the question.\n",
      "[00:51:36.000 --> 00:51:49.000]   And we did just find some documentation or some reference, why the dynamic shared memory or is this supposed to be this way that I'm a little bit surprised that it's.\n",
      "[00:51:49.000 --> 00:51:53.000]   No, it's a total mystery to me.\n",
      "[00:51:53.000 --> 00:51:57.000]   So maybe there's something wrong with my code.\n",
      "[00:51:57.000 --> 00:52:09.000]   I don't know, so like this is something I think we should try to follow up on and maybe some of our friends at Nvidia can tell me the dumb thing I did, because, you know, I'm a newbie to all this stuff so I probably did something stupid.\n",
      "[00:52:09.000 --> 00:52:23.000]   But yeah, I've looked around, I've read around, I've searched, I've asked chat GPT nothing so far has told me I found some other people who have said the same thing on the Internet, saying like always my dynamic and static having different speeds.\n",
      "[00:52:23.000 --> 00:52:27.000]   I haven't found answers to any of those either, so yeah, this one's.\n",
      "[00:52:27.000 --> 00:52:31.000]   The theory is that it definitely should not be solid.\n",
      "[00:52:31.000 --> 00:52:36.000]   They should be identical, they should create exactly the same PTX code.\n",
      "[00:52:36.000 --> 00:52:51.000]   So yeah, my guess is maybe I've made a mistake in my code somewhere, so I will, if anybody figures this out, I will update the YouTube description to say what the answer turned out to be.\n",
      "[00:52:51.000 --> 00:52:55.000]   There, Jeremy here with a message from the future.\n",
      "[00:52:55.000 --> 00:53:01.000]   I figured out why that code was going slowly.\n",
      "[00:53:01.000 --> 00:53:07.000]   And the reason is because of this tiny little bit here.\n",
      "[00:53:07.000 --> 00:53:20.000]   The problem is that when TW, the tile width, is not known at compile time, it turns out that CUDA does not know how to create an optimized piece of code.\n",
      "[00:53:20.000 --> 00:53:24.000]   For a range of tile widths.\n",
      "[00:53:24.000 --> 00:53:29.000]   So it falls back to the slowest possible version.\n",
      "[00:53:29.000 --> 00:53:40.000]   I found a somewhat better solution than just supporting one constant tile width, which is, you can skip over this if you're not interested, it's a bit more advanced.\n",
      "[00:53:40.000 --> 00:53:49.000]   But basically you can use a C++ template and you can make tile width a template parameter instead of a normal parameter.\n",
      "[00:53:49.000 --> 00:54:05.000]   When you do this, now you can only call it with a constant tile width, which is a bit ugly, but you can actually deal with that by basically supporting some fixed number of tile widths and it will compile a separate version of the kernel for each one.\n",
      "[00:54:05.000 --> 00:54:09.000]   So I've got it here doing 8 and a 16 and a 32.\n",
      "[00:54:09.000 --> 00:54:22.000]   So you could have something, so here I've just got tile width equal 16, but it's a variable to kind of show it's possible and you could replace that with some code that calculates dynamically, whether you want to make it 8 or 16 or 32, or you could do additional ones as well.\n",
      "[00:54:22.000 --> 00:54:31.000]   And then there's a lambda, this is how C++ very ugly does lambdas, looks quite different to Python lambdas, as you can see.\n",
      "[00:54:31.000 --> 00:54:42.000]   But basically this is a lambda now, which we'll take, so this is the function that we're going to call, and we'll call that function using some particular kernel.\n",
      "[00:54:42.000 --> 00:54:45.000]   This is the kernel function, kf is the kernel function.\n",
      "[00:54:45.000 --> 00:55:00.000]   Anyway, so lots of messiness there, it's pretty hideous code, and I guess this is where it gets pretty complicated when you actually want to have optimized kernels for a range of different hardware.\n",
      "[00:55:00.000 --> 00:55:13.000]   The good news is that at the moment at least any even reasonably modern Nvidia GPU supports exactly the same amount of shared memory.\n",
      "[00:55:13.000 --> 00:55:17.000]   So maybe all this dynamic stuff isn't that necessary.\n",
      "[00:55:17.000 --> 00:55:28.000]   Although having said that, I do think that you do need to change the tile width depending on the matrix or the size of the matrices that you're using.\n",
      "[00:55:28.000 --> 00:55:35.000]   So yeah, I do think this is actually reasonably complicated to make it work well in lots of different situations.\n",
      "[00:55:35.000 --> 00:55:45.000]   And I guess this is why this whole team of people at Nvidia who work on doing this in Kublass and Kootie, so we don't have to worry about it.\n",
      "[00:55:45.000 --> 00:55:53.000]   Anyway, I'm glad I got it figured out, and we'll now return you back to our scheduled programming.\n",
      "[00:55:53.000 --> 00:56:10.000]   All right, so now I've got something really exciting to show, which is that we can do everything that we've just seen in a different library called number.\n",
      "[00:56:10.000 --> 00:56:14.000]   Number is another way of writing Kootie code.\n",
      "[00:56:14.000 --> 00:56:20.000]   It's a way of writing Kootie code where you actually write the Kootie code in Python.\n",
      "[00:56:20.000 --> 00:56:30.000]   Here is the Kootie code for our call.\n",
      "[00:56:30.000 --> 00:56:38.000]   We can actually, I haven't tried this before, but we can actually see how long this takes to run.\n",
      "[00:56:38.000 --> 00:56:45.000]   Okay, that's interesting, so this one is slower still.\n",
      "[00:56:45.000 --> 00:56:53.000]   So again, maybe I'm doing something weird. This is using the dynamic shared memory approach.\n",
      "[00:56:53.000 --> 00:56:59.000]   I've got two times tile width times tile width times, I just manually put four, which is how many bytes there are in a float.\n",
      "[00:56:59.000 --> 00:57:01.000]   But still, it's running, you know, it's running at much more.\n",
      "[00:57:01.000 --> 00:57:09.000]   It's running at Kootie speeds, which is good, even if it's not the full speed we were getting from Kootie.\n",
      "[00:57:09.000 --> 00:57:15.000]   Now, why would you do this? Because, I mean, actually, if you look at the amount of code I have here,\n",
      "[00:57:15.000 --> 00:57:23.000]   it's not less code than the amount that I had here.\n",
      "[00:57:23.000 --> 00:57:29.000]   So it's not like it's not easier to write. I mean, so I've still got to use block IDX, block 10, thread IDX.\n",
      "[00:57:29.000 --> 00:57:37.000]   So all these are now available in the Kootie module, I guess.\n",
      "[00:57:37.000 --> 00:57:41.000]   And they're kind of globals available here.\n",
      "[00:57:41.000 --> 00:57:48.000]   We can create our shadow array here because if you say shadow dot array zero, this is actually quite a new thing in number.\n",
      "[00:57:48.000 --> 00:57:55.000]   It does the dynamic approach. And so you can, so when you call the kernel, rather than using triple angle brackets,\n",
      "[00:57:55.000 --> 00:58:01.000]   you use square brackets, passing in the blocks, threads per block, stream number, which we haven't learned about yet,\n",
      "[00:58:01.000 --> 00:58:06.000]   and the dynamic shared memory size. And so here is where it creates it with the dynamic shared memory size.\n",
      "[00:58:06.000 --> 00:58:10.000]   Tell it that you want them to be floats.\n",
      "[00:58:10.000 --> 00:58:17.000]   And so now we can do the exact same trick that we did before, grabbing our MS and NS, instead of underscore,\n",
      "[00:58:17.000 --> 00:58:22.000]   underscore sync threads, it's Kootie dot sync threads.\n",
      "[00:58:22.000 --> 00:58:26.000]   So in some ways I'd say like, okay, this is not necessarily a big win.\n",
      "[00:58:26.000 --> 00:58:30.000]   But there's a couple of things that do make it a big win.\n",
      "[00:58:30.000 --> 00:58:38.000]   So one I'll show you, for instance, is, I mean, we could just do something pointless, like a times one here.\n",
      "[00:58:38.000 --> 00:58:42.000]   There we go. So it should force it to recompile the kernel. Okay, run.\n",
      "[00:58:42.000 --> 00:58:46.000]   There we go, done. So it took less than a second to recompile the kernel.\n",
      "[00:58:46.000 --> 00:58:59.000]   So for some reason, which I don't fully understand, compiling number kernels, Kootie kernels is way faster than compiling\n",
      "[00:58:59.000 --> 00:59:03.000]   C and C++ Kootie kernels.\n",
      "[00:59:03.000 --> 00:59:07.000]   And I have asked an NVIDIA guy about this, and he was like, well, it's just how it is.\n",
      "[00:59:07.000 --> 00:59:13.000]   Sorry, like it doesn't seem to be an obvious way to make the C, C++ version faster.\n",
      "[00:59:13.000 --> 00:59:21.000]   So I actually think this is great for doing development is I can, you know, have actual Kootie running\n",
      "[00:59:21.000 --> 00:59:26.000]   and just change things and run it very fast. So I think that's very handy.\n",
      "[00:59:26.000 --> 00:59:31.000]   The second thing that's very handy is that I don't have to flatten my tenses.\n",
      "[00:59:31.000 --> 00:59:35.000]   M and N here are being passed in are actually M and N.\n",
      "[00:59:35.000 --> 00:59:42.000]   The only thing I've done to them is wrap them with as Kootie array, which is a take zero time.\n",
      "[00:59:42.000 --> 00:59:46.000]   It's just changing the type, basically.\n",
      "[00:59:46.000 --> 00:59:54.000]   So you don't have to flatten it. So I can actually use proper indexing notation, which is convenient.\n",
      "[00:59:54.000 --> 00:59:58.000]   So that's another nice thing. I can do things like dot shape.\n",
      "[00:59:58.000 --> 01:00:03.000]   So I don't have to pass in the H, K and W, which again is quite nice.\n",
      "[01:00:03.000 --> 01:00:09.000]   So there's some conveniences. But then I'm going to tell you the most amazingly cool thing,\n",
      "[01:00:09.000 --> 01:00:19.000]   which is the Python thread thing we created back here that kind of simulates threads\n",
      "[01:00:19.000 --> 01:00:24.000]   and simulates Kootie in Python is fully built in to number.\n",
      "[01:00:24.000 --> 01:00:29.000]   So everything that we kind of recreated from scratch here actually already exists.\n",
      "[01:00:29.000 --> 01:00:39.000]   And so in number to use it, if you Google for number Kootie simulator,\n",
      "[01:00:39.000 --> 01:00:47.000]   you'll see here that if you set the environment variable number enable Kootie Sim to one,\n",
      "[01:00:47.000 --> 01:00:51.000]   then that enables the Kootie simulator.\n",
      "[01:00:51.000 --> 01:01:01.000]   The code is executed as normal except that it's actually run on the CPU as pure Python code,\n",
      "[01:01:01.000 --> 01:01:12.000]   just like ours was. So you can, for example, set a breakpoint or print stuff directly from Python.\n",
      "[01:01:12.000 --> 01:01:17.000]   Now notice, because this is not running Kootie, it's going to be slow.\n",
      "[01:01:17.000 --> 01:01:21.000]   It's going to be exactly as slow as our manual Python version,\n",
      "[01:01:21.000 --> 01:01:25.000]   because this is just their Python manual Python version, or I think it's exactly as slow.\n",
      "[01:01:25.000 --> 01:01:29.000]   So you still want to use much smaller subsets of your data.\n",
      "[01:01:29.000 --> 01:01:33.000]   But this is a great way to actually, in my opinion,\n",
      "[01:01:33.000 --> 01:01:39.000]   to do real world Kootie development is do it in number.\n",
      "[01:01:39.000 --> 01:01:48.000]   Do it all with number enable Kootie Sim set to one with small amounts of data until everything works.\n",
      "[01:01:48.000 --> 01:01:53.000]   And then, and you have to, by the way, you have to set that environment variable\n",
      "[01:01:53.000 --> 01:01:59.000]   before you import number, right? So you would have it before you import number.\n",
      "[01:01:59.000 --> 01:02:05.000]   And if you're using a notebook, you'd have to reset the kernel, restart the kernel,\n",
      "[01:02:05.000 --> 01:02:08.000]   and then change the environment variable and then re-import number.\n",
      "[01:02:08.000 --> 01:02:14.000]   And then you can set it to zero, and now the exact same code will now be running on the GPU.\n",
      "[01:02:14.000 --> 01:02:19.000]   And then I've tested this. If you then take your code and paste it into chat GPT,\n",
      "[01:02:19.000 --> 01:02:26.000]   and say, please convert this into Kootie C code, for me at least it did it perfectly,\n",
      "[01:02:26.000 --> 01:02:28.000]   correctly, first time.\n",
      "[01:02:28.000 --> 01:02:33.000]   So I think this is a really useful way to kind of combine all the stuff we've learned\n",
      "[01:02:33.000 --> 01:02:38.000]   about from first principles, and we've done it all from scratch, and so we understand how it all works.\n",
      "[01:02:38.000 --> 01:02:45.000]   But now to implement it in practice, maybe the easiest way to do that is actually to, yeah, use number.\n",
      "[01:02:45.000 --> 01:02:51.000]   Now, of course, you don't even need to convert it to C or C++. You could just leave it in number.\n",
      "[01:02:51.000 --> 01:02:57.000]   The challenge is from a deployment point of view, you know, it might be a bit more tricky.\n",
      "[01:02:57.000 --> 01:03:05.000]   With PyTorch, if you use our load inline load Kootie approach,\n",
      "[01:03:05.000 --> 01:03:12.000]   the documentation explains how you can pre-compile that and actually provide a pip or conda installable package,\n",
      "[01:03:12.000 --> 01:03:20.000]   that people can just use right away without having any of the Kootie development toolkit installed.\n",
      "[01:03:20.000 --> 01:03:25.000]   The number that's not true, having said that if you do install number from Konda,\n",
      "[01:03:25.000 --> 01:03:29.000]   it automatically installs all this stuff you need for the toolkit, so maybe that's okay.\n",
      "[01:03:29.000 --> 01:03:34.000]   So anyway, these are some things like pros and cons to think about, you know,\n",
      "[01:03:34.000 --> 01:03:40.000]   so maybe you just just use number as is, maybe it's a little bit slower,\n",
      "[01:03:40.000 --> 01:03:47.000]   so maybe that'll be a problem, or maybe you auto convert it to Kootie C and actually use that in practice.\n",
      "[01:03:47.000 --> 01:03:54.000]   Yeah, so I think that basically covers everything.\n",
      "[01:03:54.000 --> 01:04:01.000]   Anything, any more questions or anything? Anybody wanted to add?\n",
      "[01:04:01.000 --> 01:04:13.000]   From my side, first of all, I must say, this was a super fantastic session that I learned a lot.\n",
      "[01:04:13.000 --> 01:04:18.000]   I had to expect that you would go so deep into like the math more stuff,\n",
      "[01:04:18.000 --> 01:04:23.000]   and that also I think there's show so many elements of the Kootie development.\n",
      "[01:04:23.000 --> 01:04:31.000]   So maybe talks people if they seem to have cards for the first time, or it makes it a little bit inaccessible in the first class,\n",
      "[01:04:31.000 --> 01:04:36.000]   because you just see this very busy day of class and multiply it, whatever.\n",
      "[01:04:36.000 --> 01:04:38.000]   The magic happens.\n",
      "[01:04:38.000 --> 01:04:43.000]   It was what you showed this, like the starting from this drawing,\n",
      "[01:04:43.000 --> 01:04:48.000]   or how the memory basically the structures look like and what you really need to do,\n",
      "[01:04:48.000 --> 01:04:54.000]   and the difference back to this, because normally the first approach you already get something wrong,\n",
      "[01:04:54.000 --> 01:04:59.000]   and definitely that, yeah, looking back where the memory has come from,\n",
      "[01:04:59.000 --> 01:05:06.000]   that's why I would, and because I always get some bits of info beside me.\n",
      "[01:05:06.000 --> 01:05:08.000]   Yes, marvellous, thank you.\n",
      "[01:05:08.000 --> 01:05:11.000]   Yeah, there are also a few things that stood out for me.\n",
      "[01:05:11.000 --> 01:05:18.000]   A lot of people are interested in how to ship numbers, like that's something we could certainly take a look at.\n",
      "[01:05:18.000 --> 01:05:24.000]   As far as I know, like I think numbers that have like a head of time compilation mode should make sense of easier,\n",
      "[01:05:24.000 --> 01:05:30.000]   but because it's all sort of old Python, you can ship it, you just have to eat a completely legit compilation cost.\n",
      "[01:05:30.000 --> 01:05:39.000]   Yeah, the AOT ahead of time compilation is deprecated as of what is this, February 2024.\n",
      "[01:05:39.000 --> 01:05:44.000]   People say they're going to replace it with something newer and better, but they haven't said what that is yet,\n",
      "[01:05:44.000 --> 01:05:47.000]   so that's currently an outstanding question.\n",
      "[01:05:47.000 --> 01:05:55.000]   They did say they won't remove the previous AOT approach until the new approach is working and everything,\n",
      "[01:05:55.000 --> 01:06:02.000]   so yeah, hopefully by the time people watch this video on YouTube in the future, maybe this will all be fully resolved.\n",
      "[01:06:02.000 --> 01:06:12.000]   We have a question from Tronas who wants to know if you have checked it out, compare it to QBLAST to the optimized tutor library,\n",
      "[01:06:12.000 --> 01:06:17.000]   or maybe also to the PyTorch matrix multiplication as a baseline.\n",
      "[01:06:17.000 --> 01:06:23.000]   I haven't because I'm not a CUDA expert.\n",
      "[01:06:23.000 --> 01:06:28.000]   I actually don't know how to optimize all these, you know, shared memory sizes and tiles and blah, blah, blah.\n",
      "[01:06:28.000 --> 01:06:35.000]   So, you know, I know that stuff like KuBLAST, you know, has a whole lot of, you know,\n",
      "[01:06:35.000 --> 01:06:41.000]   so actually, you know, one of the things you might have noticed is I changed my input matrix sizes from last time.\n",
      "[01:06:41.000 --> 01:06:47.000]   Last time I used the MNIST data and a kind of a pretend set of weights for MNIST,\n",
      "[01:06:47.000 --> 01:06:52.000]   and so the output was 50,000 by 10.\n",
      "[01:06:52.000 --> 01:07:04.000]   And that really kind of long narrow matrix is particularly hard to optimize because, like, with a tile width of 32, for example, most of its padding.\n",
      "[01:07:04.000 --> 01:07:12.000]   So, I actually used kind of a simpler, not your old shapes to make this a bit easier for me to think about.\n",
      "[01:07:12.000 --> 01:07:19.000]   So, I think it'll be a fun exercise as we go further to see if we can figure out,\n",
      "[01:07:19.000 --> 01:07:31.000]   for that, like, one layer MNIST model, can we create something that is close in performance to what KuBLAST or CUDA and N does?\n",
      "[01:07:31.000 --> 01:07:38.000]   Because, yeah, I think it's kind of, I found it quite tricky to think about how I would do all that automatically.\n",
      "[01:07:38.000 --> 01:07:44.000]   There are things like TVM, which, you know, maybe one day we can look at TVM together.\n",
      "[01:07:44.000 --> 01:07:57.000]   I know Thomas Feynman says he's used that. Yeah, so maybe he can even help us there, which is something which can kind of automatically optimize these and a lot more details for you.\n",
      "[01:07:57.000 --> 01:08:08.000]   And then also I know, like, sometime hopefully in the next month or so, so sometime around late February or March 2024, Mojo GPU should be becoming available, at least in a preview form.\n",
      "[01:08:08.000 --> 01:08:15.000]   And that has an auto-tune functionality, which might help us to automatically find the best parameters as well.\n",
      "[01:08:15.000 --> 01:08:23.000]   But, yeah, for now I didn't even bother checking because I'm suspected I was going to be quite embarrassed at how much further there is to go.\n",
      "[01:08:23.000 --> 01:08:35.000]   I think this is like a opportunity for the community, because you have this bag of a notebook, which can be as like everybody can fork it and play around.\n",
      "[01:08:35.000 --> 01:08:47.000]   And try different stuff, like make performance measurements, and we'll be pretty sure that somebody will make reports on things that are experimented and where you can look.\n",
      "[01:08:47.000 --> 01:08:56.000]   We were current status and we could think of closer to what state of the art basically the symmetric multiplication.\n",
      "[01:08:56.000 --> 01:09:12.000]   And I think also this session today played the foundation for other stuff like experts, special cards, technology from Tensor course, for example, from NVIDIA, which is specifically in hardware.\n",
      "[01:09:12.000 --> 01:09:18.000]   If further I have features for setting metrics modifications.\n",
      "[01:09:18.000 --> 01:09:22.000]   So, thanks for getting into the future, maybe.\n",
      "[01:09:22.000 --> 01:09:25.000]   Great.\n",
      "[01:09:25.000 --> 01:09:33.000]   So, for what it's worth, I think getting some think of us competitive is probably going to be very difficult, at least on the...\n",
      "[01:09:33.000 --> 01:09:43.000]   Like, basically the service use of things like human hundreds. I suspect that's not as true for consuming use, so that's potentially like...\n",
      "[01:09:43.000 --> 01:09:46.000]   Because it has to be less attention for the video.\n",
      "[01:09:46.000 --> 01:09:49.000]   Good point.\n",
      "[01:09:49.000 --> 01:09:55.000]   So I'm actually, Misa wants to know what is PyTorch currently using for mid-resmodication, I guess.\n",
      "[01:09:55.000 --> 01:09:59.000]   Yeah, do you know?\n",
      "[01:09:59.000 --> 01:10:03.000]   Yeah, so so so PyTorch mostly still uses, like, Kavos.\n",
      "[01:10:03.000 --> 01:10:12.000]   There is, like, an experiment with, if you use Torch Compile, there's a flag called Torch.Dottirigot-Fing.\n",
      "[01:10:12.000 --> 01:10:13.000]   Tried in Matmul.\n",
      "[01:10:13.000 --> 01:10:15.000]   I think it's off by default.\n",
      "[01:10:15.000 --> 01:10:19.000]   But, you know, that's something like we're potentially interested in exploring.\n",
      "[01:10:19.000 --> 01:10:22.000]   But I think as far as today, it's mostly still the boss.\n",
      "[01:10:22.000 --> 01:10:27.000]   The way you can tell, by the way, is if you, like, launch the PyTorch profiler,\n",
      "[01:10:27.000 --> 01:10:35.000]   there's, like, the, like, the blockchain aims a lot of specific signatures to sort of say, okay, it's using the boss, it's using test scores.\n",
      "[01:10:35.000 --> 01:10:39.000]   So, like, like, a profile trace as well would be a different version.\n",
      "[01:10:41.000 --> 01:10:48.000]   And in general, what you said, I think regarding this, the speed of compilation, for me personally, this is super important,\n",
      "[01:10:48.000 --> 01:10:51.000]   and getting, like, experimenting with things.\n",
      "[01:10:51.000 --> 01:10:56.000]   I was trying to optimize for this, and I think it's a big advantage if this is in.\n",
      "[01:10:56.000 --> 01:11:00.000]   But it's really much faster than you're currently going to succeed.\n",
      "[01:11:00.000 --> 01:11:01.000]   Yeah.\n",
      "[01:11:01.000 --> 01:11:03.000]   Like, you're always this, like, waiting.\n",
      "[01:11:03.000 --> 01:11:07.000]   For the next result, I'll tell you, is in the face as you wait.\n",
      "[01:11:07.000 --> 01:11:08.000]   Exactly.\n",
      "[01:11:08.000 --> 01:11:12.000]   Yeah, you need a quick iteration loop, so.\n",
      "[01:11:12.000 --> 01:11:20.000]   I think between the CUDA simulator and the fast number CUDA JIT, it can make life a lot better.\n",
      "[01:11:20.000 --> 01:11:29.000]   So, I think there was two more general questions regarding strategy teams.\n",
      "[01:11:29.000 --> 01:11:36.000]   Um, I think one was with, with our strategy team could be a process of, um, what is, uh, choosing?\n",
      "[01:11:36.000 --> 01:11:38.000]   What might be common as well as this?\n",
      "[01:11:38.000 --> 01:11:42.000]   Like, what were the limits basically, are for what a strategy team can do?\n",
      "[01:11:42.000 --> 01:11:45.000]   And, 'cause I think it's like, how do they think you have to try out?\n",
      "[01:11:45.000 --> 01:11:46.000]   Let's try it.\n",
      "[01:11:46.000 --> 01:11:48.000]   That'd be an interesting thing for people to experiment with, wouldn't it?\n",
      "[01:11:48.000 --> 01:11:49.000]   To see how far we can go.\n",
      "[01:11:49.000 --> 01:11:55.000]   I think in general, people tend to underestimate, maybe, what chat GPT can do.\n",
      "[01:11:55.000 --> 01:12:00.000]   So, yeah, why don't you try, try some things that maybe you suspect might be a bit too hard for it.\n",
      "[01:12:00.000 --> 01:12:02.000]   And we'll find out where the limits are.\n",
      "[01:12:02.000 --> 01:12:05.000]   So, for example, this, uh, that's quite a good shot.\n",
      "[01:12:05.000 --> 01:12:06.000]   I remember things.\n",
      "[01:12:06.000 --> 01:12:10.000]   What's this, and it's automatically by, how much did you have to?\n",
      "[01:12:10.000 --> 01:12:16.000]   I put that prompt in, well, so for when I converted number into CUDA,\n",
      "[01:12:16.000 --> 01:12:19.000]   it automatically changed the number shared memory.\n",
      "[01:12:19.000 --> 01:12:29.000]   In my prompt, I had something saying replace sync B dot wait with underscore underscore sync threads.\n",
      "[01:12:29.000 --> 01:12:32.000]   I didn't try doing it without it, maybe it would have guessed.\n",
      "[01:12:32.000 --> 01:12:39.000]   And the one, the other question regarding sort of general capabilities of GPT for code games,\n",
      "[01:12:39.000 --> 01:12:44.000]   do you think, what's your expectation for to develop us at the future with it?\n",
      "[01:12:44.000 --> 01:12:53.000]   Personally, I haven't found it at all useful for, like, anything remotely novel, you know.\n",
      "[01:12:53.000 --> 01:13:07.000]   So, I found it really useful for, like, doing, like, using, like, languages and not that familiar with or frameworks.\n",
      "[01:13:07.000 --> 01:13:12.000]   I'm not that familiar with, and it kind of gets me, you know, kind of calling some API.\n",
      "[01:13:12.000 --> 01:13:14.000]   I find it really good for that.\n",
      "[01:13:14.000 --> 01:13:22.000]   But for doing anything kind of, at all algorithm related, which is anything other than just replicating a well-known algorithm,\n",
      "[01:13:22.000 --> 01:13:24.000]   I haven't, I found it terrible.\n",
      "[01:13:24.000 --> 01:13:29.000]   So, at least the kind of work I do, which is kind of, because it's research oriented,\n",
      "[01:13:29.000 --> 01:13:32.000]   so most of what I write is pretty new.\n",
      "[01:13:32.000 --> 01:13:36.000]   I haven't found it, I haven't found it at all useful.\n",
      "[01:13:36.000 --> 01:13:42.000]   So, at least, I think my work's safe for a while. Don't know about yours.\n",
      "[01:13:42.000 --> 01:13:47.000]   Yeah, I'm sure it depends on how quickly go further to an API, but...\n",
      "[01:13:47.000 --> 01:13:53.000]   I think it's really, really play a different role in the future for us at a very good moment, that's for sure.\n",
      "[01:13:53.000 --> 01:13:57.000]   You know, I think I should be gutting them, but in how it compares to Triton, I don't know.\n",
      "[01:13:57.000 --> 01:14:00.000]   Do you have experience with Triton already?\n",
      "[01:14:00.000 --> 01:14:05.000]   Yeah, a little bit, I mean, it's, I'm not an expert, like, you know, like, you guys know a lot more than me.\n",
      "[01:14:05.000 --> 01:14:16.000]   So, I mean, Triton's different in that in Triton, you can, like, have a matrix multiplication inside, you know,\n",
      "[01:14:16.000 --> 01:14:19.000]   you can have, like, at, inside your kernel.\n",
      "[01:14:19.000 --> 01:14:27.000]   And, you know, Triton's really clever at kind of optimizing these much more sophisticated kind of things.\n",
      "[01:14:27.000 --> 01:14:40.000]   You know, so numbers a lot more basic, really. It's just a mapping of the CUDA C concepts directly to Python concepts.\n",
      "[01:14:40.000 --> 01:14:46.000]   You know, Triton is a fairly recent, literally, a PhD thesis artifact.\n",
      "[01:14:46.000 --> 01:14:50.000]   So, it's doing something a lot more clever.\n",
      "[01:14:50.000 --> 01:14:54.000]   I mean, the similarities, you know, Triton, it works with a decorator.\n",
      "[01:14:54.000 --> 01:15:00.000]   It converts, you know, Python code to GPU code.\n",
      "[01:15:00.000 --> 01:15:05.000]   I think they're good for different things, you know, because, like, Triton is somewhat limited as well.\n",
      "[01:15:05.000 --> 01:15:10.000]   Like, it's very, very smart, but it's not a mapping of the entire CUDA programming model.\n",
      "[01:15:10.000 --> 01:15:18.000]   So, for example, when I know it meta, when you guys, you know, Horace and those guys did the GPT fast thing\n",
      "[01:15:18.000 --> 01:15:23.000]   and wanted to do 4-bit discretization, you know, you found that you couldn't do it in Triton,\n",
      "[01:15:23.000 --> 01:15:29.000]   and that's because Triton doesn't have any way to express that at the moment.\n",
      "[01:15:29.000 --> 01:15:33.000]   So, yeah, I think, you know, they both have their place.\n",
      "[01:15:33.000 --> 01:15:36.000]   Do you guys feel the same way?\n",
      "[01:15:36.000 --> 01:15:41.000]   Yeah, so, like, we are going to have, like, Charles, who is the author of the GPT fast journals\n",
      "[01:15:41.000 --> 01:15:46.000]   and give us a talk in two weeks, so he's going to sort of tell us the first time that happened.\n",
      "[01:15:46.000 --> 01:15:52.000]   I think when people ask this question, the motivation is sort of, can you avoid learning CUDA?\n",
      "[01:15:52.000 --> 01:15:55.000]   I mean, CUDA is like, that's a negative.\n",
      "[01:15:55.000 --> 01:15:56.000]   That's seem to be, yeah.\n",
      "[01:15:56.000 --> 01:15:58.000]   Kind of like, learning both.\n",
      "[01:15:58.000 --> 01:16:03.000]   And, you know, Triton will just be slightly easier to ship some sort of useful kernel.\n",
      "[01:16:03.000 --> 01:16:07.000]   But, like, you're going to sort of run into the limits of the language later.\n",
      "[01:16:07.000 --> 01:16:09.000]   Yeah, and I'm not even sure it's easier.\n",
      "[01:16:09.000 --> 01:16:11.000]   I'm not even sure it's easier to use Triton.\n",
      "[01:16:11.000 --> 01:16:16.000]   Like, I feel like Triton is more complex in many ways.\n",
      "[01:16:16.000 --> 01:16:20.000]   So, like, once, you know, CUDA, then you can find the places where Triton can help.\n",
      "[01:16:20.000 --> 01:16:28.000]   I think trying to use Triton effectively, if you didn't know CUDA would be an even harder path.\n",
      "[01:16:28.000 --> 01:16:34.000]   Especially now that we've kind of figured out these ways of doing, you know,\n",
      "[01:16:34.000 --> 01:16:39.000]   iterative notebook-based CUDA development, you know, which is one of the big benefits of Triton,\n",
      "[01:16:39.000 --> 01:16:42.000]   is just it's a decorator and it's Python and stuff.\n",
      "[01:16:42.000 --> 01:16:43.000]   So, yeah.\n",
      "[01:16:43.000 --> 01:16:49.000]   So, I like driving very much, but we've always joked that maybe Ovene has a fixed version of it.\n",
      "[01:16:49.000 --> 01:16:52.000]   It's a labor, like, the con routine.\n",
      "[01:16:52.000 --> 01:16:53.000]   Yeah.\n",
      "[01:16:53.000 --> 01:16:54.000]   Yeah.\n",
      "[01:16:54.000 --> 01:17:00.000]   I can write three colors because it's higher and so often into strange things and unexpected.\n",
      "[01:17:00.000 --> 01:17:01.000]   Yeah.\n",
      "[01:17:01.000 --> 01:17:06.000]   But, also, I think that there's, like, iteration speed is similar to number.\n",
      "[01:17:06.000 --> 01:17:11.000]   That's a good positive point for Triton.\n",
      "[01:17:11.000 --> 01:17:14.000]   What's wrong?\n",
      "[01:17:14.000 --> 01:17:15.000]   OK.\n",
      "[01:17:15.000 --> 01:17:18.000]   So, I think that this was a wonderful session today.\n",
      "[01:17:18.000 --> 01:17:20.000]   Really, thank you so much, Henry.\n",
      "[01:17:20.000 --> 01:17:21.000]   Thank you.\n",
      "[01:17:21.000 --> 01:17:22.000]   Thank you.\n",
      "[01:17:22.000 --> 01:17:26.000]   Thank you so much for opportunity now to work on this.\n",
      "[01:17:26.000 --> 01:17:27.000]   Yeah.\n",
      "[01:17:27.000 --> 01:17:28.000]   Fantastic.\n",
      "[01:17:28.000 --> 01:17:31.000]   Really deep dive into measures modification.\n",
      "[01:17:31.000 --> 01:17:32.000]   Bye, everybody.\n",
      "[01:17:32.000 --> 01:17:33.000]   Thanks for joining.\n",
      "\n",
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/eUuGdh3nBGo.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/eUuGdh3nBGo.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/eUuGdh3nBGo.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/eUuGdh3nBGo.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1260.26 ms\n",
      "whisper_print_timings:     fallbacks =   3 p /   4 h\n",
      "whisper_print_timings:      mel time =  2623.71 ms\n",
      "whisper_print_timings:   sample time = 33439.42 ms / 82363 runs (    0.41 ms per run)\n",
      "whisper_print_timings:   encode time =   357.77 ms /   195 runs (    1.83 ms per run)\n",
      "whisper_print_timings:   decode time =  1400.00 ms /   798 runs (    1.75 ms per run)\n",
      "whisper_print_timings:   batchd time = 42444.65 ms / 80578 runs (    0.53 ms per run)\n",
      "whisper_print_timings:   prompt time =  9885.02 ms / 44003 runs (    0.22 ms per run)\n",
      "whisper_print_timings:    total time = 91864.87 ms\n"
     ]
    }
   ],
   "source": [
    "# transcribe using the base model, output text file (great with CUDA enabled whisper.cpp)\n",
    "try:\n",
    "    subprocess.run(['transcribe -t 12 -m ' + home_directory + '/machine_learning/whisper.cpp/models/ggml-base.en.bin -f ' \n",
    "                    + directory + extracted_audio_file + ' -otxt -ovtt -osrt -olrc'], shell=True, check=True)\n",
    "    print(\"Transcription executed successfully and saved in \" + directory)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Transcription failed with error {e.returncode}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert text file to html because send to kindle joins lines in txt file\n",
    "\n",
    "def convert_txt_to_html(txt_file_path, html_file_path):\n",
    "    with open(txt_file_path, 'r') as txt_file, open(html_file_path, 'w') as html_file:\n",
    "        html_file.write('<!DOCTYPE html>\\n<html><body>')\n",
    "        for line in txt_file:\n",
    "            html_file.write('<p>{}</p>'.format(line.strip()))\n",
    "        html_file.write('</body>\\n</html>')\n",
    "\n",
    "txt_file_path = directory + name + '.wav.lrc'  # replace with your text file path\n",
    "html_file_path = directory + name + '.html'  # replace with your HTML file path\n",
    "convert_txt_to_html(txt_file_path, html_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
