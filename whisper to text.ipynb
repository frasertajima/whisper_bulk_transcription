{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ease deployment of CPU whisper by filling all settings. Setup is outlined in https://github.com/ggerganov/whisper.cpp. I had difficulty with CUDA in Fedora Silverblue (could only make in a distrobox and the end result had artefacts or hallucinations). This ASR has the highest accuracy and lowest speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track-13.wav\n",
      "Track-13.wav-output.wav\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "\n",
    "np.set_printoptions(linewidth=50)\n",
    "\n",
    "# change to reflect your local directory and file name\n",
    "file_dir = '/var/home/fraser/machine_learning/whisper.cpp/samples/'\n",
    "# ensure the spaces are replaced with '-' (cell below will rename files for ffmpeg processing)\n",
    "audio_file = 'Track-13.wav'\n",
    "output_file = audio_file + '-output.wav'\n",
    "print(audio_file)\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all audio files with spaces in their name\n",
    "# Get a list of all audio files, .m4a, .mp3, and .wav, in the directory\n",
    "files = glob.glob(os.path.join(file_dir, '*.m4a')) + \\\n",
    "        glob.glob(os.path.join(file_dir, '*.mp3')) + \\\n",
    "        glob.glob(os.path.join(file_dir, '*.ogg')) + \\\n",
    "        glob.glob(os.path.join(file_dir, '*.wav'))\n",
    "\n",
    "# Iterate over the files \n",
    "for file in files:\n",
    "    # If the file name contains a space\n",
    "    if ' ' in file:\n",
    "        # Replace the spaces with hyphens\n",
    "        new_name = file.replace(' ', '-')\n",
    "        # Rename the file\n",
    "        os.rename(file, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, wav, from '/var/home/fraser/machine_learning/whisper.cpp/samples/Track-13.wav':\n",
      "  Duration: 00:00:44.80, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\n",
      "File '/var/home/fraser/machine_learning/whisper.cpp/samples/Track-13.wav-output.wav' already exists. Overwrite? [y/N] Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/Track-13.wav-output.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=    1400kB time=00:00:44.79 bitrate= 256.0kbits/s speed=2.13e+03x    \n",
      "video:0kB audio:1400kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.005441%\n"
     ]
    }
   ],
   "source": [
    "# convert audio_file then transcribe to text\n",
    "# overwrites existing file with same name\n",
    "try:\n",
    "    yes_command = f'echo \"y\" | '\n",
    "    subprocess.run([yes_command + 'ffmpeg' + ' -i ' + file_dir + audio_file + ' -ar 16000 -ac 1 -c:a pcm_s16le ' + file_dir + output_file], shell=True, check=True)\n",
    "    print(\"Audio coverted successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Audio convertion failed with error {e.returncode}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# transcribe using the whisper-distill model: this model is halluciating presently\\n# but once that is fixed, I can swap out the slower CPU model for the CUDA enabled model\\n# so this code is for future use\\ntry:\\n    subprocess.run([\\'transcribe -t 24 -m /var/home/fraser/machine_learning/whisper.cpp/models/ggml-large-32-2.en.bin -f \\' + file_dir + output_file + \\' -otxt\\'], shell=True, check=True)\\n    print(\"Transcription executed successfully and saved in \" + file_dir + output_file)\\nexcept subprocess.CalledProcessError as e:\\n    print(f\"Transcription failed with error {e.returncode}.\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# transcribe using the whisper-distill model: this model is halluciating presently\n",
    "# but once that is fixed, I can swap out the slower CPU model for the CUDA enabled model\n",
    "# so this code is for future use\n",
    "try:\n",
    "    subprocess.run(['transcribe -t 24 -m /var/home/fraser/machine_learning/whisper.cpp/models/ggml-large-32-2.en.bin -f ' + file_dir + output_file + ' -otxt'], shell=True, check=True)\n",
    "    print(\"Transcription executed successfully and saved in \" + file_dir + output_file)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Transcription failed with error {e.returncode}.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-model-whisper-large-q5_0.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51865\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 1280\n",
      "whisper_model_load: n_audio_head  = 20\n",
      "whisper_model_load: n_audio_layer = 32\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 1280\n",
      "whisper_model_load: n_text_head   = 20\n",
      "whisper_model_load: n_text_layer  = 32\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 8\n",
      "whisper_model_load: qntvr         = 1\n",
      "whisper_model_load: type          = 5 (large)\n",
      "whisper_model_load: adding 1608 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:      CPU total size =  1080.10 MB\n",
      "whisper_model_load: model size    = 1080.10 MB\n",
      "whisper_init_state: kv self size  =  220.20 MB\n",
      "whisper_init_state: kv cross size =  245.76 MB\n",
      "whisper_init_state: compute buffer (conv)   =   34.82 MB\n",
      "whisper_init_state: compute buffer (encode) =  926.66 MB\n",
      "whisper_init_state: compute buffer (cross)  =    9.38 MB\n",
      "whisper_init_state: compute buffer (decode) =  209.26 MB\n",
      "\n",
      "system_info: n_threads = 24 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 0 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/Track-13.wav-output.wav' (716800 samples, 44.8 sec), 24 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:10.760]   So, is it possible to insert formatting or markdown within the context of the text by\n",
      "[00:00:10.760 --> 00:00:11.960]   speaking it out?\n",
      "[00:00:11.960 --> 00:00:23.000]   So for instance, \"*bold*\" is one way of formatting text, while another one might be \"\\n\" and\n",
      "[00:00:23.000 --> 00:00:25.800]   that would indicate a new paragraph.\n",
      "[00:00:25.800 --> 00:00:33.040]   So it's an interesting experiment and there may be some ways of melding both natural speech\n",
      "[00:00:33.040 --> 00:00:44.040]   and some brief commands in order to reduce the final editing load that is required.\n",
      "\n",
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/Track-13.wav-output.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/Track-13.wav-output.wav.txt'\n",
      "\n",
      "whisper_print_timings:     load time =   577.18 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =    42.96 ms\n",
      "whisper_print_timings:   sample time =   175.33 ms /   528 runs (    0.33 ms per run)\n",
      "whisper_print_timings:   encode time = 28255.19 ms /     2 runs (14127.60 ms per run)\n",
      "whisper_print_timings:   decode time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
      "whisper_print_timings:   batchd time = 12544.39 ms /   521 runs (   24.08 ms per run)\n",
      "whisper_print_timings:   prompt time =   719.62 ms /    68 runs (   10.58 ms per run)\n",
      "whisper_print_timings:    total time = 42323.57 ms\n"
     ]
    }
   ],
   "source": [
    "# transcribe using the large quantized CPU model, output text file\n",
    "try:\n",
    "    subprocess.run(['transcribe -t 24 -m /var/home/fraser/machine_learning/whisper.cpp/models/ggml-model-whisper-large-q5_0.bin -f ' + file_dir + output_file + ' -otxt'], shell=True, check=True)\n",
    "    print(\"Transcription executed successfully and saved in \" + file_dir + output_file)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Transcription failed with error {e.returncode}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7m to transcribe a 2:24m file, but all punctuation, etc., is *perfect*. whisper-distil on CUDA takes 2.6s but has some errors. Attempts at injecting markdown formatting commands failed as the command is surrounded by ' ' for some odd reason. When this changes, it should be possible to inject formatting while dictating."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
