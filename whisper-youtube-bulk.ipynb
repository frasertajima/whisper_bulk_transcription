{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CUDA enabled whisper.cpp. Setup is outlined in https://github.com/ggerganov/whisper.cpp. YouTube extraction code from OPENVINO example notebook 227-whisper-nnfc-quantize.ipynb.\n",
    "\n",
    "Enter youtube playlist link. Downloads videos is lowest resolution, transcribes them, combines timestamped transcription into an html file for Send to Kindle. Other transcriptions are also provided such as txt, substitle text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "from pytube import YouTube\n",
    "from utils import get_audio\n",
    "from utils import prepare_srt\n",
    "from pathlib import Path\n",
    "from pytube import Playlist\n",
    "import datetime\n",
    "\n",
    "np.set_printoptions(linewidth=50)\n",
    "\n",
    "# change to reflect your local directory and file name\n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "# create a separate directory for youtube videos\n",
    "directory = home_directory + '/machine_learning/whisper.cpp/youtube/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=fjbTCPAj254\n",
      "https://www.youtube.com/watch?v=wR81RH-GI9U\n",
      "https://www.youtube.com/watch?v=VU73LRk8Zjw\n",
      "https://www.youtube.com/watch?v=lCCT9JGkLw8\n",
      "https://www.youtube.com/watch?v=739Ddvx1OIU\n",
      "https://www.youtube.com/watch?v=72QWY6ubS14\n",
      "https://www.youtube.com/watch?v=Szae6pHKr60\n",
      "https://www.youtube.com/watch?v=_Hxt8CibzIs\n",
      "https://www.youtube.com/watch?v=n3WxDCvjlAs\n",
      "https://www.youtube.com/watch?v=YEOAn9FIvyI\n",
      "https://www.youtube.com/watch?v=QwTBtKWmA0M\n"
     ]
    }
   ],
   "source": [
    "# playlist conversion to yourtube video links from deepseeker ai\n",
    "def get_video_links(playlist_url):\n",
    "    playlist = Playlist(playlist_url)\n",
    "    video_links = []\n",
    "    for video_url in playlist.video_urls:\n",
    "        video_links.append(video_url)\n",
    "    return video_links\n",
    "\n",
    "# ENTER youtube playlist here:\n",
    "playlist_url = 'https://youtube.com/playlist?list=PLYXvCE1En13epbogBmgafC_Yyyk9oQogl&feature=shared'  # replace with your playlist URL\n",
    "video_links = get_video_links(playlist_url)\n",
    "for link in video_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video https://www.youtube.com/watch?v=fjbTCPAj254 started\n",
      "fjbTCPAj254\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/fjbTCPAj254.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/fjbTCPAj254.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:22:18.49, start: 0.000000, bitrate: 287 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 187 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/fjbTCPAj254.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  154328kB time=01:22:18.48 bitrate= 256.0kbits/s speed=1.42e+03x    \n",
      "video:0kB audio:154328kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000049%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/fjbTCPAj254.wav' (79015834 samples, 4938.5 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:12.980]   So, maybe let's start. So, today we will continue talking about the lower bound for reinforcement\n",
      "[00:00:12.980 --> 00:00:18.420]   learning. So, in the last lecture, let's just first recap a bit what are we having in talking\n",
      "[00:00:18.420 --> 00:00:22.980]   about in the last lecture. In the last lecture, we talk about the general strategy to develop\n",
      "[00:00:22.980 --> 00:00:34.040]   a lower bound is we can perform this reduction, where we will reduce the algorithm, the lower\n",
      "[00:00:34.040 --> 00:00:46.640]   bounds for any randomized algorithm. And we need to construct a hard instance which is\n",
      "[00:00:46.640 --> 00:00:57.700]   specific to this algorithm. And the expected regret, this is the original target of lower\n",
      "[00:00:57.700 --> 00:01:05.960]   bounds, which trying to prove this is like small. This is lower bounded by something. And\n",
      "[00:01:05.960 --> 00:01:11.880]   we say by reduction, we can actually reduce it by some lower bound, which is to say we\n",
      "[00:01:11.880 --> 00:01:21.360]   no longer need to look at the randomized algorithm. We only need to look at deterministic algorithm.\n",
      "[00:01:21.360 --> 00:01:27.680]   And we can take expectation. And we are no longer look at the problem specific, like\n",
      "[00:01:27.680 --> 00:01:33.800]   algorithm, sufficic hard instance, but we are like taking expectation of a problem instance\n",
      "[00:01:33.800 --> 00:01:46.280]   over some distribution. And we look at the regret. So, the important thing is now we only need\n",
      "[00:01:46.280 --> 00:02:04.680]   to deal with the deterministic algorithm. And we deal with distribution over hard instance.\n",
      "[00:02:04.680 --> 00:02:10.120]   And in the last lecture, we are in the middle of trying to prove the lower bound for the\n",
      "[00:02:10.120 --> 00:02:15.720]   multi-ambended case, which is a simpler problem for reinforcement learning. So the theorem\n",
      "[00:02:15.720 --> 00:02:32.040]   looks like follows. For any algorithm for any algorithm A, there exists a multi-ambended\n",
      "[00:02:32.040 --> 00:02:51.520]   problem.\n",
      "[00:02:51.520 --> 00:03:11.640]   So that the regret of time t is lower bounded by some constant times square root of a t.\n",
      "[00:03:11.640 --> 00:03:16.120]   So this is the lower bound we are trying to prove. And we talk about some construction.\n",
      "[00:03:16.120 --> 00:03:22.840]   In the last lecture, our construction is also very simple. It is a multi-ambended problem.\n",
      "[00:03:22.840 --> 00:03:35.920]   So we have a different arms. We have arm 1, arm 2 to arm a and arm capital A. So we see\n",
      "[00:03:35.920 --> 00:03:45.120]   the average reward or mean reward for each arm or Bernoulli, the distribution or Bernoulli.\n",
      "[00:03:45.120 --> 00:03:51.480]   And we have 1/2 for the most of the arm where there is only one special arm that is Bernoulli\n",
      "[00:03:51.480 --> 00:03:58.120]   1/2 plus epsilon. And this special arm is a sample uniformly random from all possible\n",
      "[00:03:58.120 --> 00:04:19.920]   arm.\n",
      "[00:04:19.920 --> 00:04:27.320]   So this gives the construction of our distribution of hard problems, which essentially is a multi-ambended\n",
      "[00:04:27.320 --> 00:04:33.720]   problem with one special arm. And if the special arm is fixed, this is like a one multi-amb\n",
      "[00:04:33.720 --> 00:04:37.920]   problem, one hard instance. But we need to construct distribution, so basically we make\n",
      "[00:04:37.920 --> 00:04:45.840]   this special arm randomize through all possible arms. And I just want to record in the last\n",
      "[00:04:45.840 --> 00:04:51.320]   lecture, we introduce a bunch of notation. We will continue to use it. We say the expectation\n",
      "[00:04:51.320 --> 00:05:07.840]   e star is basically taking expectation over all randomness, including the random jaw of\n",
      "[00:05:07.840 --> 00:05:15.880]   special arm. So including a random jaw of special arm. And we have eA and e0. In the\n",
      "[00:05:15.880 --> 00:05:22.240]   last lecture, we define it to be eA is defined to be the expectation taken over like random\n",
      "[00:05:22.240 --> 00:05:28.280]   reward if a is a special arm. And e0 is taking expectation over reward in a special case\n",
      "[00:05:28.280 --> 00:05:34.120]   if there is no special arm. So it's like completely. Now I'm setting.\n",
      "[00:05:34.120 --> 00:05:41.800]   And we also define the notation which are r1 to t. That is a collection of all the information\n",
      "[00:05:41.800 --> 00:05:46.360]   we will receive from multi-ambended up to t round. Which is essentially the stochastic\n",
      "[00:05:46.360 --> 00:05:52.880]   reward we're going to receive in the first iteration when we put arm 1. And the stochastic\n",
      "[00:05:52.880 --> 00:05:59.200]   reward we're going to receive in the second iteration when we put the second arm. Those\n",
      "[00:05:59.200 --> 00:06:06.120]   are like the arm we put. The first time, arm we put it second time. And then until time\n",
      "[00:06:06.120 --> 00:06:18.000]   t the stochastic reward we receive when we put arm a t. Just briefly recap on the notation.\n",
      "[00:06:18.000 --> 00:06:22.040]   And the recall this is like basically the full information we receive when interacted\n",
      "[00:06:22.040 --> 00:06:26.920]   with a multi-ambended problem. Because at each iteration in the multi-ambended, we only\n",
      "[00:06:26.920 --> 00:06:33.280]   pull one single arm only observe the stochastic reward we get for that arm. So this is like\n",
      "[00:06:33.280 --> 00:06:40.440]   everything we receive from the environment up to time t. Okay. And in the very end of\n",
      "[00:06:40.440 --> 00:06:48.400]   the last lecture we have been talking about the first step where we computed the regret.\n",
      "[00:06:48.400 --> 00:06:55.760]   We say this is equal to, we have been calculating this regret and this regret is equal to epsilon\n",
      "[00:06:55.760 --> 00:07:05.720]   times t minus 1 over a summation little a from 1 to capital A. Where we're taking expectation\n",
      "[00:07:05.720 --> 00:07:13.920]   of a, taking expectation where we, when arm a is a special arm and how many times the algorithm\n",
      "[00:07:13.920 --> 00:07:27.480]   is it going to pull arm a? So in, no, yes, yes. Do we, what, sorry?\n",
      "[00:07:27.480 --> 00:07:41.040]   [inaudible]\n",
      "[00:07:41.040 --> 00:07:46.400]   R1 to t. Oh, this is a, I think, you can think this is like stochastic draw. So, so in, in\n",
      "[00:07:46.400 --> 00:07:54.440]   a first round we choose arm a1 and we get a stochastic, stochastic draw of the random\n",
      "[00:07:54.440 --> 00:07:59.160]   reward. So this is just indicating like this is like a stochastic thing we, we got in the\n",
      "[00:07:59.160 --> 00:08:07.080]   wrong one. So, it's different wrong. So even if, let's just imagine, even if I always put\n",
      "[00:08:07.080 --> 00:08:11.760]   the same arm, so my algorithm is stupid, I always put the same arm, I still get t random\n",
      "[00:08:11.760 --> 00:08:17.640]   reward, t independent random reward. So this is indicating like basically the wrong or\n",
      "[00:08:17.640 --> 00:08:29.320]   the number of episodes. We don't randomly choose each arm. So this arm is like, choose\n",
      "[00:08:29.320 --> 00:08:35.360]   them by algorithm. So algorithm, for example, algorithm can be use AB or can be Epsom greedy.\n",
      "[00:08:35.360 --> 00:08:40.280]   So they will look at the previous feedback, which like algorithm will in the first iteration\n",
      "[00:08:40.280 --> 00:08:46.160]   pick a1 and got the reward one. And then algorithm will process the feedback and choose arm, choose\n",
      "[00:08:46.160 --> 00:08:52.000]   the second arm. And then you get a second feedback. So you can think this is, can be\n",
      "[00:08:52.000 --> 00:08:57.840]   a reward sequence you observe from UCBL wisdom or from Epsom greedy algorithm or for any\n",
      "[00:08:57.840 --> 00:08:58.840]   algorithm.\n",
      "[00:08:58.840 --> 00:09:02.960]   Okay, so arm is chosen by the algorithm.\n",
      "[00:09:02.960 --> 00:09:06.640]   And your arm is chosen by the algorithm. Yes. So this is what I'm, what I'm saying is\n",
      "[00:09:06.640 --> 00:09:12.320]   like for whatever algorithm, this is like completely history, like you observe from the\n",
      "[00:09:12.320 --> 00:09:20.080]   environment. That's a very good question. Okay, so we'll actually immediately use the\n",
      "[00:09:20.080 --> 00:09:26.360]   fact. So we said, when we look at the expression of the regret, the only thing kind of nontrivial\n",
      "[00:09:26.360 --> 00:09:34.560]   is this expectation. That is under the scenario where arm A is a special arm, how many times\n",
      "[00:09:34.560 --> 00:09:42.320]   this algorithm is going to pull arm A. So we say we will actually kind of bounding this\n",
      "[00:09:42.320 --> 00:09:51.320]   term by comparing to E zero, by comparing to the, like the case, vanilla case where\n",
      "[00:09:51.320 --> 00:10:04.960]   there is no special arm. So we look at this difference, E A and A, minus E zero and A.\n",
      "[00:10:04.960 --> 00:10:11.200]   So whole idea is basically when epsilon is very small, then whether A is a special arm\n",
      "[00:10:11.200 --> 00:10:15.080]   or whether zero special arm, like it's a, it's a, whether there's no special arm, this\n",
      "[00:10:15.080 --> 00:10:21.560]   is like very difficult to distinguish. So we expect for any algorithm, like you won't\n",
      "[00:10:21.560 --> 00:10:29.060]   be able to spend a significant amount of time pulling an A. So we can actually do the\n",
      "[00:10:29.060 --> 00:10:38.560]   expression that this difference is actually equal to with summation over all the possible\n",
      "[00:10:38.560 --> 00:10:49.040]   histories we're going to observe for the, for the algorithm. And because algorithm is\n",
      "[00:10:49.040 --> 00:11:00.040]   fixed, so whenever I fix the history from R one to T, then we will completely decide\n",
      "[00:11:00.040 --> 00:11:10.080]   it how many times I'm going to pull arm A. So we say the number of times for pulling\n",
      "[00:11:10.080 --> 00:11:30.120]   arm A, this is for fixed algorithm, the number of times so pulling arm A is completely decided\n",
      "[00:11:30.120 --> 00:11:52.400]   and this is a deterministic algorithm.\n",
      "[00:11:52.400 --> 00:12:07.520]   By the history. Okay, so basically by looking at the history and now because we have a fixed\n",
      "[00:12:07.520 --> 00:12:11.840]   decision-making algorithm, so we will always able to read around the history how many times\n",
      "[00:12:11.840 --> 00:12:18.640]   we have pull arm A. And once we know this, then the remaining thing we just need to look\n",
      "[00:12:18.640 --> 00:12:26.440]   at the probability of observing this history. So the difference between under the case where\n",
      "[00:12:26.440 --> 00:12:30.720]   A is a special arm, what is the probability I'm going to observe this kind of history\n",
      "[00:12:30.720 --> 00:12:53.840]   R one to R T and subtracted by P zero R one to R T.\n",
      "[00:12:53.840 --> 00:13:07.120]   Okay. And remember this R one to R T is just a T dimensional scalar, a T dimensional vector\n",
      "[00:13:07.120 --> 00:13:32.560]   which is coordinated as a scalar.\n",
      "[00:13:32.560 --> 00:13:49.720]   So basically if the algorithm is fixed by looking at just the reward, we kind of know\n",
      "[00:13:49.720 --> 00:13:53.720]   what algorithm we're going to pull next and this is going to be the reward of the next\n",
      "[00:13:53.720 --> 00:14:00.400]   arm we pull and we can basically recover the entire history of how this algorithm is pulling\n",
      "[00:14:00.400 --> 00:14:21.560]   different arms and what is the reward they're going to receive.\n",
      "[00:14:21.560 --> 00:14:29.960]   So if no questions, I'm going to do another bound. Another thing is now we have this expression.\n",
      "[00:14:29.960 --> 00:14:36.160]   We can basically just upper bound this N A that is whatever the trajectories, number\n",
      "[00:14:36.160 --> 00:14:42.880]   of time we're going to pull arm A by T. So I'm just upper bounding this N A by T and\n",
      "[00:14:42.880 --> 00:14:54.480]   then write the remaining thing R one to T and we say the upper bound which is P A of R one\n",
      "[00:14:54.480 --> 00:15:08.440]   to T subtract P zero R one to T. And for those of you who learn like the probability,\n",
      "[00:15:08.440 --> 00:15:13.160]   you will notice this is essentially the T V distance, total variation distance between\n",
      "[00:15:13.160 --> 00:15:25.360]   this probability and this probability. So it's two times T times the T V distance between\n",
      "[00:15:25.360 --> 00:15:43.520]   P zero, P A and P zero.\n",
      "[00:15:43.520 --> 00:15:49.720]   T V is essentially just defining as one half of this summation of the absolute difference\n",
      "[00:15:49.720 --> 00:15:59.960]   on each map.\n",
      "[00:15:59.960 --> 00:16:02.960]   Yes.\n",
      "[00:16:02.960 --> 00:16:11.000]   I'm pretty confident so what's the reason so why do we choose special arm and its problem\n",
      "[00:16:11.000 --> 00:16:25.000]   so why why what special arm so we want to make this problem hard so we're essentially\n",
      "[00:16:25.000 --> 00:16:31.960]   saying essentially we want to prove some regret but regret lower bound that is we're saying\n",
      "[00:16:31.960 --> 00:16:38.320]   for any algorithm, we need to suffer from some regret. And if we don't have any special\n",
      "[00:16:38.320 --> 00:16:44.600]   arm like every arm is one half then we just we can just put any arm and we won't suffer\n",
      "[00:16:44.600 --> 00:16:49.800]   any regret. And only because there is one special arm that is greater than the remaining\n",
      "[00:16:49.800 --> 00:16:55.400]   arm so that best strategy is always putting this special arm but because I don't really\n",
      "[00:16:55.400 --> 00:16:59.880]   know which arm is special arm this epsilon is very small and also it's a random arm that\n",
      "[00:16:59.880 --> 00:17:05.480]   the special arm is hiding among all arms. So that's why it's very difficult for me to\n",
      "[00:17:05.480 --> 00:17:10.520]   figure out which is which one is the best arm and if I don't put a special arm and put\n",
      "[00:17:10.520 --> 00:17:17.040]   some other arm I will suffer regret epsilon per round. So I'm just going to say this is\n",
      "[00:17:17.040 --> 00:17:23.040]   actually hard and that I will suffer epsilon for a very long time until I figure out this\n",
      "[00:17:23.040 --> 00:17:24.040]   arm.\n",
      "[00:17:24.040 --> 00:17:30.000]   Also we don't know which arms the best arm and we just don't know there is a special\n",
      "[00:17:30.000 --> 00:17:36.080]   arm and trying to find some other arm will go. The algorithm is trying to find out what\n",
      "[00:17:36.080 --> 00:17:44.840]   is the special arm. You can see any multi arm advantage but implicitly they should figure\n",
      "[00:17:44.840 --> 00:18:06.040]   out what is the special arm like which one is the best arm. Any questions about this\n",
      "[00:18:06.040 --> 00:18:21.840]   part or if you confuse this one this one this one yeah this is what this is just come from\n",
      "[00:18:21.840 --> 00:18:40.520]   the definition here. So if you are happy with this now we do the next step. So next step\n",
      "[00:18:40.520 --> 00:18:53.400]   we will just directly involve some inequality from the probability theory it's called Pinskerz\n",
      "[00:18:53.400 --> 00:19:02.520]   inequality. So the important thing is this P and P0 is a probability over R1 to R2 which\n",
      "[00:19:02.520 --> 00:19:07.480]   is a sequence and we hope we can do some decomposition and a TV distance is not very\n",
      "[00:19:07.480 --> 00:19:15.560]   easy to decomposition well KL and KL divergence is much easier to do the factorization. So\n",
      "[00:19:15.560 --> 00:19:23.080]   for Pinskerz inequality we know for any two probability P and Q we can actually say the\n",
      "[00:19:23.080 --> 00:19:34.760]   TV distance between P and Q is upper bounded by square root one half of the KL divergence\n",
      "[00:19:34.760 --> 00:20:01.160]   between Q and Q. So that means this is less or equal to square root of less or equal to\n",
      "[00:20:01.160 --> 00:20:21.680]   T times square root of 2 KL P0 over PA. So now we only need to look at this KL and KL divergence.\n",
      "[00:20:21.680 --> 00:20:38.760]   So we continue with the KL divergence. First by definition of KL divergence KL is again\n",
      "[00:20:38.760 --> 00:20:50.280]   a summation of all the possible R1 to T sequence and the definition of KL divergence is P0 R1\n",
      "[00:20:50.280 --> 00:21:14.160]   to T times the log of P0 R1 to T over PA R1 to T, yes, this is basically the general definition.\n",
      "[00:21:14.160 --> 00:21:29.200]   This is like just a summation of Px log Px over Qx, summation over for Ox. This is like\n",
      "[00:21:29.200 --> 00:21:45.440]   the definition of KL and which we just replace x by R1 to T here. So now we will use some\n",
      "[00:21:45.440 --> 00:22:07.160]   structure for it. The first step we don't do anything for our side but inside for each\n",
      "[00:22:07.160 --> 00:22:14.600]   probability of R1 to T, P0 R1 to T and PA R1 to T, we kind of we factorize it. We say\n",
      "[00:22:14.600 --> 00:22:21.880]   this probability of joint probability of everything is equal to the product of a condition probability.\n",
      "[00:22:21.880 --> 00:22:31.560]   So this is equal to product over T, T from R1 to T. The probability under this zero instance\n",
      "[00:22:31.560 --> 00:22:42.980]   of the observing Rt condition we observe previous R1 to T minus 1. This is like a very\n",
      "[00:22:42.980 --> 00:22:48.200]   standard probability equality and for the denominator is also saying it's a product\n",
      "[00:22:48.200 --> 00:23:02.900]   of R1 to T and PA Rt given R1 to T, T minus 1.\n",
      "[00:23:02.900 --> 00:23:05.140]   (clicking)\n",
      "[00:23:05.140 --> 00:23:07.380]   (clicking)\n",
      "[00:23:07.380 --> 00:23:35.380]   (silence)\n",
      "[00:23:35.380 --> 00:23:37.380]   And for the next step,\n",
      "[00:23:37.380 --> 00:23:41.380]   we notice we can move the product within the log\n",
      "[00:23:41.380 --> 00:23:43.380]   outside to make it a summation.\n",
      "[00:23:43.380 --> 00:23:46.380]   Okay.\n",
      "[00:23:46.380 --> 00:23:50.380]   So the next step is just equal to summation over little t\n",
      "[00:23:50.380 --> 00:23:54.380]   from 1 to capital T,\n",
      "[00:23:54.380 --> 00:24:03.380]   and the summation of R1 to t,\n",
      "[00:24:03.380 --> 00:24:12.380]   P0, R1 to t, log.\n",
      "[00:24:12.380 --> 00:24:19.380]   P0, R little t conditioned on the previous thing,\n",
      "[00:24:19.380 --> 00:24:23.380]   divided by Pa,\n",
      "[00:24:23.380 --> 00:24:32.380]   R little t given by the previous thing.\n",
      "[00:24:32.380 --> 00:24:36.380]   So all I did is just make the product outside the log,\n",
      "[00:24:36.380 --> 00:24:45.380]   so that becomes the summation.\n",
      "[00:24:45.380 --> 00:24:48.380]   So another thing we notice is we can essentially\n",
      "[00:24:48.380 --> 00:24:51.380]   divide the summation to be two parts.\n",
      "[00:24:51.380 --> 00:24:58.380]   One is the first summation from t plus 1 to t,\n",
      "[00:24:58.380 --> 00:25:02.380]   and then I summation from R1 to Rt.\n",
      "[00:25:02.380 --> 00:25:06.380]   Okay. So I can essentially summation over all possible,\n",
      "[00:25:06.380 --> 00:25:11.380]   like in different order.\n",
      "[00:25:11.380 --> 00:25:14.380]   And we notice the remaining part actually does not\n",
      "[00:25:14.380 --> 00:25:19.380]   depend on this t plus 1 to capital T.\n",
      "[00:25:19.380 --> 00:25:26.380]   So we can essentially also decompose this to be P0,\n",
      "[00:25:26.380 --> 00:25:31.380]   Rt plus 1 to capital T,\n",
      "[00:25:31.380 --> 00:25:35.380]   conditioned on R1 to t,\n",
      "[00:25:35.380 --> 00:25:41.380]   times probability of R1 to t.\n",
      "[00:25:41.380 --> 00:25:44.380]   And we say the summation we decompose into two parts,\n",
      "[00:25:44.380 --> 00:25:49.380]   that is with summation from R1 to t,\n",
      "[00:25:49.380 --> 00:25:55.380]   and the summation from Rt plus 1 to capital T.\n",
      "[00:25:55.380 --> 00:26:02.380]   And we notice this part actually sum to 1.\n",
      "[00:26:02.380 --> 00:26:08.380]   So we can like simplify this expression,\n",
      "[00:26:08.380 --> 00:26:14.380]   which is equal to summation little t from 1 to capital T,\n",
      "[00:26:14.380 --> 00:26:19.380]   and summation R from 1 to little t,\n",
      "[00:26:19.380 --> 00:26:28.380]   P0 R from 1 to little t, log of the probability Rt,\n",
      "[00:26:28.380 --> 00:26:33.380]   given R1 to t minus 1.\n",
      "[00:26:33.380 --> 00:26:40.380]   And Pa Rt from 1 to t minus 1.\n",
      "[00:26:40.380 --> 00:26:50.380]   And Pb Rt from 1 to t minus 2.\n",
      "[00:26:50.380 --> 00:26:55.380]   And Pb Rt from 1 to t minus 2.\n",
      "[00:26:55.380 --> 00:27:01.380]   And Pb Rt from 1 to t minus 2.\n",
      "[00:27:01.380 --> 00:27:06.380]   And Pb Rt from 1 to t minus 2.\n",
      "[00:27:06.380 --> 00:27:11.380]   And Pb Rt from 1 to t minus 2.\n",
      "[00:27:11.380 --> 00:27:16.380]   And Pb Rt from 1 to t minus 2.\n",
      "[00:27:16.380 --> 00:27:21.380]   And Pb Rt from 1 to t minus 2.\n",
      "[00:27:21.380 --> 00:27:25.380]   And Pb Rt from 1 to t minus 2.\n",
      "[00:27:25.380 --> 00:27:28.380]   And another thing we notice from this fact is,\n",
      "[00:27:28.380 --> 00:27:33.380]   because we say R1 to t minus 1 is the complete history\n",
      "[00:27:33.380 --> 00:27:37.380]   we observed from the environment for this algorithm.\n",
      "[00:27:37.380 --> 00:27:41.380]   So that means for fixed algorithm at the action,\n",
      "[00:27:41.380 --> 00:27:50.380]   the algorithm choose at time t is completely determined\n",
      "[00:27:50.380 --> 00:28:00.380]   by the history that R1 to t minus 1.\n",
      "[00:28:00.380 --> 00:28:05.380]   Once we fix the history, we know what this algorithm is going to make\n",
      "[00:28:05.380 --> 00:28:08.380]   for what action it's going to take at time t.\n",
      "[00:28:08.380 --> 00:28:15.380]   And also we know if somehow at is not equal to A,\n",
      "[00:28:15.380 --> 00:28:17.380]   is not equal to the special arm.\n",
      "[00:28:17.380 --> 00:28:25.380]   This is like special arm.\n",
      "[00:28:25.380 --> 00:28:36.380]   Then we will have P0 Rt given R1 to t minus 1.\n",
      "[00:28:36.380 --> 00:28:45.380]   It's equal to Pa Rt R1 to t minus 1.\n",
      "[00:28:45.380 --> 00:28:49.380]   The reason is essentially when we condition our previous history,\n",
      "[00:28:49.380 --> 00:28:51.380]   we kind of determine what is the arm.\n",
      "[00:28:51.380 --> 00:28:54.380]   And when arm is not equal to A, we know this is not a special arm.\n",
      "[00:28:54.380 --> 00:28:57.380]   If it's a non-special arm in the vanilla case P0,\n",
      "[00:28:57.380 --> 00:29:00.380]   and in the case A is special arm,\n",
      "[00:29:00.380 --> 00:29:03.380]   it doesn't really matter because I'm putting a non-special arm.\n",
      "[00:29:03.380 --> 00:29:07.380]   And the reward is always going to be sampled from Bernoulli distribution\n",
      "[00:29:07.380 --> 00:29:09.380]   with 1/2 as a parameter.\n",
      "[00:29:09.380 --> 00:29:12.380]   So in that case, this probability is exactly equal\n",
      "[00:29:12.380 --> 00:29:14.380]   because I'm putting a non-special arm.\n",
      "[00:29:14.380 --> 00:29:19.380]   And in this case, we know this thing will cancel\n",
      "[00:29:19.380 --> 00:29:21.380]   because they're equal to equal to 1.\n",
      "[00:29:21.380 --> 00:29:25.380]   So that means when we look at this expression,\n",
      "[00:29:25.380 --> 00:29:32.380]   we only need to count those history summation from 1 to capital T.\n",
      "[00:29:32.380 --> 00:29:38.380]   We only need to count summation over those history R1 to little t\n",
      "[00:29:38.380 --> 00:29:45.380]   so that a t is precisely equal to A.\n",
      "[00:29:45.380 --> 00:29:48.380]   So we only need to summation over those history\n",
      "[00:29:48.380 --> 00:29:53.380]   that my algorithm at the time t will pick the special arm.\n",
      "[00:29:53.380 --> 00:29:57.380]   Otherwise, this log term will just be zero,\n",
      "[00:29:57.380 --> 00:30:02.380]   and we will need to count those terms.\n",
      "[00:30:02.380 --> 00:30:07.380]   And this is P0 R1 to T.\n",
      "[00:30:07.380 --> 00:30:17.380]   Log P0 Rt given R1 to T.\n",
      "[00:30:17.380 --> 00:30:22.380]   And Pa is actually given R2 to T.\n",
      "[00:30:22.380 --> 00:30:32.380]   [ Pause ]\n",
      "[00:30:32.380 --> 00:30:42.380]   [ Pause ]\n",
      "[00:30:42.380 --> 00:30:52.380]   [ Pause ]\n",
      "[00:30:52.380 --> 00:31:04.380]   [ Pause ]\n",
      "[00:31:04.380 --> 00:31:05.380]   Okay, yes.\n",
      "[00:31:05.380 --> 00:31:08.380]   [ Inaudible ]\n",
      "[00:31:08.380 --> 00:31:09.380]   Why is it equal?\n",
      "[00:31:09.380 --> 00:31:12.380]   Why are those two are equal?\n",
      "[00:31:12.380 --> 00:31:16.380]   Oh, because if a t is not a special arm,\n",
      "[00:31:16.380 --> 00:31:19.380]   then everything is Bernoulli one half, right?\n",
      "[00:31:19.380 --> 00:31:23.380]   This is the probability of this and this are also Bernoulli one half.\n",
      "[00:31:23.380 --> 00:31:27.380]   Yeah, both are Bernoulli one half.\n",
      "[00:31:27.380 --> 00:31:30.380]   Yeah, I think when you take Rt equal to one, Rt equal to zero,\n",
      "[00:31:30.380 --> 00:31:33.380]   both are equal to one half, that's true.\n",
      "[00:31:33.380 --> 00:31:47.380]   [ Inaudible ]\n",
      "[00:31:47.380 --> 00:31:51.380]   So next step is more or less quite straightforward.\n",
      "[00:31:51.380 --> 00:31:54.380]   We copy the first thing, T from one to capital T.\n",
      "[00:31:54.380 --> 00:31:59.380]   And we again decompose this summation to be like the first\n",
      "[00:31:59.380 --> 00:32:03.380]   summation over Rt from one to T minus one.\n",
      "[00:32:03.380 --> 00:32:09.380]   And we notice the selection where this condition where A t is equal to A\n",
      "[00:32:09.380 --> 00:32:13.380]   is already completely determined by history up to T minus one.\n",
      "[00:32:13.380 --> 00:32:16.380]   Okay, so that we summation over all the possible history,\n",
      "[00:32:16.380 --> 00:32:19.380]   so that A t is equal to A.\n",
      "[00:32:19.380 --> 00:32:25.380]   And then we can pull this P zero outside R1 to T minus one.\n",
      "[00:32:25.380 --> 00:32:28.380]   So we notice this is T and this is T minus one.\n",
      "[00:32:28.380 --> 00:32:31.380]   So we also need to take a summation over the last step.\n",
      "[00:32:31.380 --> 00:32:35.380]   That is we need to summation over Rt.\n",
      "[00:32:35.380 --> 00:32:39.380]   And because Rt is a Bernoulli distribution, so we know Rt\n",
      "[00:32:39.380 --> 00:32:43.380]   essentially only has two value, that is zero or one.\n",
      "[00:32:43.380 --> 00:32:50.380]   And we look at the P zero Rt given R one to T minus one.\n",
      "[00:32:50.380 --> 00:32:58.380]   And the log P zero Rt given R one to T minus one.\n",
      "[00:32:58.380 --> 00:33:08.380]   And the P A Rt from one to T minus one.\n",
      "[00:33:08.380 --> 00:33:19.380]   So although we write a very complicated like conditional\n",
      "[00:33:19.380 --> 00:33:22.380]   dependency on this R one to T minus one.\n",
      "[00:33:22.380 --> 00:33:26.380]   But the most important thing is it already tells you I'm putting a special arm.\n",
      "[00:33:26.380 --> 00:33:31.380]   So in that case under P zero, this is like a Bernoulli one-half,\n",
      "[00:33:31.380 --> 00:33:33.380]   where under P A, because I'm putting a special arm,\n",
      "[00:33:33.380 --> 00:33:36.380]   this is like Bernoulli one-half plus epsilon.\n",
      "[00:33:36.380 --> 00:33:40.380]   So essentially this is a KL divergence between Bernoulli one-half\n",
      "[00:33:40.380 --> 00:33:43.380]   and Bernoulli one-half plus epsilon.\n",
      "[00:33:43.380 --> 00:33:49.380]   So the next stop is basically just heat from zero to capital T.\n",
      "[00:33:49.380 --> 00:33:57.380]   And we can essentially first group this thing together.\n",
      "[00:33:57.380 --> 00:34:01.380]   This is just equal to probability.\n",
      "[00:34:01.380 --> 00:34:07.380]   Under zero we take A T is equal to A.\n",
      "[00:34:07.380 --> 00:34:19.380]   And whatever remaining here is actually equal to the KL between Bernoulli one-half\n",
      "[00:34:19.380 --> 00:34:30.380]   versus like Bernoulli one-half plus epsilon.\n",
      "[00:34:30.380 --> 00:34:34.380]   And you can plug in the value and with the definition of KL divergence,\n",
      "[00:34:34.380 --> 00:34:39.380]   you can compute what is this KL.\n",
      "[00:34:39.380 --> 00:34:44.380]   And eventually this is equal to.\n",
      "[00:34:44.380 --> 00:34:48.380]   So this part, the summation T from one to capital T and P zero,\n",
      "[00:34:48.380 --> 00:34:55.380]   A T equal to A is actually equal to expectation under the zero case.\n",
      "[00:34:55.380 --> 00:34:59.380]   What is the number of time we're going to pull arm A.\n",
      "[00:34:59.380 --> 00:35:09.380]   And the KL divergence is equal to one-half log one over one minus four epsilon squared.\n",
      "[00:35:09.380 --> 00:35:14.380]   I'm just doing the calculation for you, but you can go back to check the computation.\n",
      "[00:35:14.380 --> 00:35:20.380]   This is exactly the KL divergence and this is what we have here.\n",
      "[00:35:20.380 --> 00:35:25.380]   Oops.\n",
      "[00:35:25.380 --> 00:35:48.380]   Oops.\n",
      "[00:35:48.380 --> 00:35:59.380]   How do we just erase this as well?\n",
      "[00:35:59.380 --> 00:36:04.380]   Yes.\n",
      "[00:36:04.380 --> 00:36:24.380]   [ Inaudible ]\n",
      "[00:36:24.380 --> 00:36:27.380]   Yeah, T zero is exactly the same definition.\n",
      "[00:36:27.380 --> 00:36:37.380]   T zero is the probability under the case that there's no special arm.\n",
      "[00:36:37.380 --> 00:36:39.380]   Yeah, P is a special arm.\n",
      "[00:36:39.380 --> 00:36:46.380]   And this is a probability because we actually pull the special arm.\n",
      "[00:36:46.380 --> 00:36:53.380]   This is the like condition on the history so that I will always pull special arm A at the wrong one T.\n",
      "[00:36:53.380 --> 00:36:58.380]   So I kind of select those history so in that case like because I'm putting a special arm.\n",
      "[00:36:58.380 --> 00:37:03.380]   So that's why the PA is a one-half plus epsilon in that case.\n",
      "[00:37:03.380 --> 00:37:09.380]   [ Inaudible ]\n",
      "[00:37:09.380 --> 00:37:10.380]   Yeah.\n",
      "[00:37:10.380 --> 00:37:25.380]   The PA means A is special arm, the arm A is special arm.\n",
      "[00:37:25.380 --> 00:37:30.380]   [ Inaudible ]\n",
      "[00:37:30.380 --> 00:37:39.380]   Okay.\n",
      "[00:37:39.380 --> 00:37:44.380]   So we just recall how we get this like where we use this KL divergence.\n",
      "[00:37:44.380 --> 00:37:47.380]   Like we essentially bound the KL divergence by this.\n",
      "[00:37:47.380 --> 00:37:54.380]   So recall we use the KL divergence because originally we look at this difference.\n",
      "[00:37:54.380 --> 00:38:00.380]   EA and A subtract E zero and A.\n",
      "[00:38:00.380 --> 00:38:17.380]   And we say this is upper bounded by T times square root of two KL of P zero over PA.\n",
      "[00:38:17.380 --> 00:38:25.380]   So the other thing we can note we notice is we also have this summation of A from one to capital A.\n",
      "[00:38:25.380 --> 00:38:34.380]   That is the summation over all action we have E zero and A is equal to T.\n",
      "[00:38:34.380 --> 00:38:41.380]   This is basically saying the expected number of times we're going to pull arm A under the same environment.\n",
      "[00:38:41.380 --> 00:38:45.380]   That is no arm and special arm is always equal to T.\n",
      "[00:38:45.380 --> 00:38:52.380]   That is eventually algorithm in the T rounds can only pull T times in total.\n",
      "[00:38:52.380 --> 00:38:57.380]   So the number of times we pull each arm's sum together is always equal to T.\n",
      "[00:38:57.380 --> 00:39:07.380]   This is for any algorithm.\n",
      "[00:39:07.380 --> 00:39:11.380]   So this is including some cheating algorithm which we just always put some fixed arm.\n",
      "[00:39:11.380 --> 00:39:17.380]   In that case, the number number is like heavily favored for some respect for some arms.\n",
      "[00:39:17.380 --> 00:39:21.380]   But the other arm essentially I never put them.\n",
      "[00:39:21.380 --> 00:39:25.380]   So summation is also equal to T.\n",
      "[00:39:25.380 --> 00:39:32.380]   This indicates we can essentially summation the above equation for all A.\n",
      "[00:39:32.380 --> 00:39:37.380]   So what we have is summation over A from one to capital A.\n",
      "[00:39:37.380 --> 00:39:44.380]   EA and A.\n",
      "[00:39:44.380 --> 00:39:56.380]   It's less or equal to the summation of A from one to capital A E zero and A\n",
      "[00:39:56.380 --> 00:40:03.380]   plus T times summation over A again.\n",
      "[00:40:03.380 --> 00:40:13.380]   So we have a capital A, square root of two KL P zero over PA.\n",
      "[00:40:13.380 --> 00:40:19.380]   So we have a capital A.\n",
      "[00:40:19.380 --> 00:40:24.380]   So we have a capital A.\n",
      "[00:40:24.380 --> 00:40:33.380]   So we have a capital A.\n",
      "[00:40:33.380 --> 00:40:51.380]   And finally, I know we noticed the first term is just equal to T, we already said.\n",
      "[00:40:51.380 --> 00:40:55.380]   And the second term we applied the previous bound we developed.\n",
      "[00:40:55.380 --> 00:41:15.380]   And this is equal to T times summation A from one to capital A, square root of E, E zero\n",
      "[00:41:15.380 --> 00:41:31.380]   and A log one over one minus four epsilon squared.\n",
      "[00:41:31.380 --> 00:41:38.380]   So right hand side we still have this E zero and A here and we want to get rid of that.\n",
      "[00:41:38.380 --> 00:41:44.380]   We'll get this by closer towards.\n",
      "[00:41:44.380 --> 00:42:06.380]   Essentially we can say the first T and then T times the square root.\n",
      "[00:42:06.380 --> 00:42:12.380]   We can think this is a summation of a square root of this E zero and A times one.\n",
      "[00:42:12.380 --> 00:42:20.380]   So by applying kush was we move the summation inside and we have the summation A from one to capital A.\n",
      "[00:42:20.380 --> 00:42:28.380]   This is E zero and A.\n",
      "[00:42:28.380 --> 00:42:33.380]   Times summation A from one to capital A one.\n",
      "[00:42:33.380 --> 00:42:42.380]   And eventually have the log one over one minus four epsilon squared.\n",
      "[00:42:42.380 --> 00:43:02.380]   So eventually this is just equal to T plus T times square root of A T log one over one minus four epsilon squared.\n",
      "[00:43:02.380 --> 00:43:30.380]   [silence]\n",
      "[00:43:30.380 --> 00:43:35.380]   So we're very close to the final answer.\n",
      "[00:43:35.380 --> 00:43:44.380]   So once we get this NA bound we now can look at our regret.\n",
      "[00:43:44.380 --> 00:43:53.380]   Still recall that regret is we already calculated is equal to epsilon times T subtracted by one over A.\n",
      "[00:43:53.380 --> 00:44:03.380]   Summation A from one to capital A, E, A and A.\n",
      "[00:44:03.380 --> 00:44:10.380]   And now we directly apply the bound.\n",
      "[00:44:10.380 --> 00:44:13.380]   So we can move the T outside.\n",
      "[00:44:13.380 --> 00:44:23.380]   We know this is greater or equal to epsilon T times one subtracted by one over A.\n",
      "[00:44:23.380 --> 00:44:37.380]   Because we also have a T here for this bound and subtracted by square root of T over A log one over one minus four epsilon squared.\n",
      "[00:44:37.380 --> 00:45:00.380]   [silence]\n",
      "[00:45:00.380 --> 00:45:02.380]   So we're already there.\n",
      "[00:45:02.380 --> 00:45:08.380]   We say this is more or less roughly if we look at other, we essentially have two terms.\n",
      "[00:45:08.380 --> 00:45:13.380]   This one over A usually is like much smaller, it's like even smaller than one half.\n",
      "[00:45:13.380 --> 00:45:18.380]   So when we will compare one over A to one, it doesn't really matter, it's just some constant.\n",
      "[00:45:18.380 --> 00:45:26.380]   So the first term is actually epsilon T, roughly order of epsilon T up to some constant.\n",
      "[00:45:26.380 --> 00:45:38.380]   And then the important thing is the second term. So the whole hope is that we can use the second term to contribute to the big portion of the one minus one over A and to make everything simple.\n",
      "[00:45:38.380 --> 00:45:47.380]   So for the second term, we notice this is actually something extremely close to one.\n",
      "[00:45:47.380 --> 00:45:53.380]   This is extremely close to one. And the log, when not extremely close to one, we can take it out.\n",
      "[00:45:53.380 --> 00:46:00.380]   So this is roughly something like epsilon squared, some constant times epsilon squared.\n",
      "[00:46:00.380 --> 00:46:14.380]   And up to some constant where epsilon is very small, so we can put it outside and it becomes something epsilon times T over A, square root of T over A.\n",
      "[00:46:14.380 --> 00:46:25.380]   So up to some constants, this is what do we have for two terms. The first term is this two term together, because this one doesn't matter much.\n",
      "[00:46:25.380 --> 00:46:33.380]   And the second term is this term.\n",
      "[00:46:33.380 --> 00:46:45.380]   Because this is a one, right, like one over is less than one half.\n",
      "[00:46:45.380 --> 00:46:50.380]   So in the end, we just pick epsilon to maximize this lower bound.\n",
      "[00:46:50.380 --> 00:47:15.380]   In which case essentially epsilon, we need to pick epsilon to be some C times squared of AT for small epsilon, for small C.\n",
      "[00:47:15.380 --> 00:47:24.380]   And in that case, we can put some small C prime.\n",
      "[00:47:24.380 --> 00:47:32.380]   In that case, we can see this again becomes some constant, like epsilon squared of A over T and times squared of T over A, and some constant.\n",
      "[00:47:32.380 --> 00:47:39.380]   So as long as we pick C prime to be something small, then this is something constant but positive.\n",
      "[00:47:39.380 --> 00:47:57.380]   Well, the word remains is an epsilon times T, which is become square root of AT. So this finished proof.\n",
      "[00:47:57.380 --> 00:48:07.380]   So the probability calculation is a little bit involved, but the whole idea is we look at this difference between the,\n",
      "[00:48:07.380 --> 00:48:13.380]   like, vanilla case where there's no special arm versus we have a special arm.\n",
      "[00:48:13.380 --> 00:48:16.380]   And we essentially do an upper bound on this KL divergence.\n",
      "[00:48:16.380 --> 00:48:20.380]   We say this probability, two probability distribution are very close.\n",
      "[00:48:20.380 --> 00:48:25.380]   So that because we choose epsilon to be very small.\n",
      "[00:48:25.380 --> 00:48:36.380]   So that essentially because probability is very close, then there's not much information is leaking, even in this case where we do have some special arm.\n",
      "[00:48:36.380 --> 00:48:41.380]   Do have some signal. So that is why it's difficult for any algorithm.\n",
      "[00:48:41.380 --> 00:48:48.380]   Yes.\n",
      "[00:48:48.380 --> 00:48:56.380]   We already said this is like a square, this is squared of A over T. So that means this thing is constant, right?\n",
      "[00:48:56.380 --> 00:48:59.380]   When you plug it in here, there's a small constant.\n",
      "[00:48:59.380 --> 00:49:07.380]   We can, we can choose a prime to be some small, small constant. So that this is one minus some small constant. This is again some small constant.\n",
      "[00:49:07.380 --> 00:49:13.380]   And so the whatever remains is outside where epsilon T is just a square root of AT.\n",
      "[00:49:13.380 --> 00:49:27.380]   [silence]\n",
      "[00:49:27.380 --> 00:49:33.380]   [silence]\n",
      "[00:49:33.380 --> 00:49:38.380]   [silence]\n",
      "[00:49:38.380 --> 00:49:44.380]   [silence]\n",
      "[00:49:44.380 --> 00:49:51.380]   [silence]\n",
      "[00:49:51.380 --> 00:49:58.380]   [silence]\n",
      "[00:49:58.380 --> 00:50:08.380]   [silence]\n",
      "[00:50:08.380 --> 00:50:30.380]   [silence]\n",
      "[00:50:30.380 --> 00:50:40.380]   [silence]\n",
      "[00:50:40.380 --> 00:50:45.380]   So any questions about this one?\n",
      "[00:50:45.380 --> 00:50:46.380]   Yes.\n",
      "[00:50:46.380 --> 00:50:51.380]   [inaudible]\n",
      "[00:50:51.380 --> 00:50:59.380]   Oh, it's again like a quadratic thing, right? This is like a linear term. This is a quadratic term.\n",
      "[00:50:59.380 --> 00:51:15.380]   So I think you can make it write it out, epsilon T, subtract C times epsilon squared T over A, something like that.\n",
      "[00:51:15.380 --> 00:51:28.380]   And for the quadratic thing, I think the optimal research is choosing to be over A, right?\n",
      "[00:51:28.380 --> 00:51:44.380]   I think optimal lines just choose them as, like the coefficients which, which often lines just this, roughly some constant of A, which you can just do this square out.\n",
      "[00:51:44.380 --> 00:52:04.380]   [silence]\n",
      "[00:52:04.380 --> 00:52:09.380]   Is that sort of telling you what the hardest type of a problem you can make is?\n",
      "[00:52:09.380 --> 00:52:14.380]   Yeah, it's essentially like what is the right scaling of epsilon? Like what is the right scaling of the noise?\n",
      "[00:52:14.380 --> 00:52:24.380]   Make it very difficult for the problem to tell, for any algorithm to tell if it can only interact with the problem with T iterations.\n",
      "[00:52:24.380 --> 00:52:28.380]   Like you should basically scale it as a square root of A over T.\n",
      "[00:52:28.380 --> 00:52:38.380]   [silence]\n",
      "[00:52:38.380 --> 00:52:45.380]   So finally, I think we will also mention that with similar techniques.\n",
      "[00:52:45.380 --> 00:52:49.380]   So this is a regret bound.\n",
      "[00:52:49.380 --> 00:52:56.380]   And we can also essentially ask if we don't care about regret, we only care about the pack. That is, some will come at you to identify the best arm.\n",
      "[00:52:56.380 --> 00:53:06.380]   What is the lower bound? We can essentially use a similar proof. So we're similar techniques.\n",
      "[00:53:06.380 --> 00:53:19.380]   We can also prove a lower bound.\n",
      "[00:53:19.380 --> 00:53:48.380]   And some will come at you to identify the best arm.\n",
      "[00:53:48.380 --> 00:54:02.380]   And this looks like follows. Basically we say for any algorithm A\n",
      "[00:54:02.380 --> 00:54:18.380]   that can adaptively pull\n",
      "[00:54:18.380 --> 00:54:42.380]   T times an output a possibly random\n",
      "[00:54:42.380 --> 00:54:52.380]   Psi. So this is like restricting the protocol for algorithm A. This is not restricting the power of algorithm.\n",
      "[00:54:52.380 --> 00:55:00.380]   But basically it says algorithm A can only pull arm for T times and need to output a single arm because we output a possibly random single arm.\n",
      "[00:55:00.380 --> 00:55:15.380]   So we're doing like this best arm identification. And then there exists\n",
      "[00:55:15.380 --> 00:55:30.380]   a multi arm bending problem.\n",
      "[00:55:30.380 --> 00:55:40.380]   So that we will have i* which is a reward of the optimal arm\n",
      "[00:55:40.380 --> 00:55:54.380]   subtracted by expectation over internal randomness in the algorithm A Psi. Essentially it's the expected reward that we're going to receive by pulling the arm output it by the algorithm.\n",
      "[00:55:54.380 --> 00:56:08.380]   We're saying this is a lower bound it by C times square root A over T.\n",
      "[00:56:08.380 --> 00:56:16.380]   So this is a theorem. This essentially implies\n",
      "[00:56:16.380 --> 00:56:23.380]   if we want to achieve, if we want to achieve make this epsilon then that means we need a sample.\n",
      "[00:56:23.380 --> 00:56:31.380]   I'm going to say that T is greater equal to some constant times A over epsilon square.\n",
      "[00:56:31.380 --> 00:56:39.380]   So we have to use this number of samples to identify the epsilon optimal arm.\n",
      "[00:56:39.380 --> 00:56:51.380]   Essentially we kind of think this is something similar to the regrab lower bound. This is like a similar episode lower bound.\n",
      "[00:56:51.380 --> 00:56:59.380]   I think in the class we kind of mentioned regret is a harder than sample complexity because regrab needs to balance the exploration versus exploitation.\n",
      "[00:56:59.380 --> 00:57:04.380]   Well in some ways you don't need to, you don't care about the exploitation.\n",
      "[00:57:04.380 --> 00:57:07.380]   You can just do pure exploration eventually find the optimal arm.\n",
      "[00:57:07.380 --> 00:57:13.380]   Well the lower bound is like on the reverse order. Lower bound is basically we want to prove some problem is hard.\n",
      "[00:57:13.380 --> 00:57:17.380]   And we say that somehow it is easier problem. Well regratus is a harder problem.\n",
      "[00:57:17.380 --> 00:57:25.380]   So that means in the lower bound proving regratus lower bound is easier than proving the sample capacity lower bound.\n",
      "[00:57:25.380 --> 00:57:29.380]   So that means the previous regratus lower bound would not imply this lower bound.\n",
      "[00:57:29.380 --> 00:57:33.380]   Well this lower bound somehow can imply the previous regratus lower bound.\n",
      "[00:57:33.380 --> 00:57:38.380]   And so how to prove this we will actually leave it as a homework.\n",
      "[00:57:38.380 --> 00:57:41.380]   So essentially the argument is roughly very similar.\n",
      "[00:57:41.380 --> 00:57:46.380]   It's a very good practice. You can just leverage a lot of KL computation we have already done.\n",
      "[00:57:46.380 --> 00:57:56.380]   But make a slightly trick on the outside argument. Like we don't, we not look at the regratus star, we look at some slightly different thing.\n",
      "[00:57:56.380 --> 00:58:01.380]   [silence]\n",
      "[00:58:01.380 --> 00:58:06.380]   [silence]\n",
      "[00:58:06.380 --> 00:58:11.380]   [silence]\n",
      "[00:58:11.380 --> 00:58:16.380]   [silence]\n",
      "[00:58:16.380 --> 00:58:21.380]   [silence]\n",
      "[00:58:21.380 --> 00:58:30.380]   [silence]\n",
      "[00:58:30.380 --> 00:58:36.380]   [silence]\n",
      "[00:58:36.380 --> 00:58:44.380]   Okay, so finally after we're talking about the lower bound for the multi-am bandit, now we will move to the mark population process.\n",
      "[00:58:44.380 --> 00:58:49.380]   [silence]\n",
      "[00:58:49.380 --> 00:58:53.380]   The reason we talk about lower bound for multi-am bandit because it's a special case.\n",
      "[00:58:53.380 --> 00:58:58.380]   It's an easier case for multi-am bandit for a mark population process.\n",
      "[00:58:58.380 --> 00:59:02.380]   So now we will actually extend it how we extend it to the MDP.\n",
      "[00:59:02.380 --> 00:59:12.380]   [silence]\n",
      "[00:59:12.380 --> 00:59:26.380]   If you still remember that in the MDP literature, we look at a bunch of algorithm, like UCBVI, crafting, UCBVI Princeton.\n",
      "[00:59:26.380 --> 00:59:31.380]   [silence]\n",
      "[00:59:31.380 --> 00:59:46.380]   And in terms of regret, the UCBVI is roughly the order of HQSAT, while the Princeton is...\n",
      "[00:59:46.380 --> 00:59:51.380]   [silence]\n",
      "[00:59:51.380 --> 00:59:59.380]   So essentially this is a lower bound that we're trying to prove. At that time we say UCBVI Princeton is an optimal.\n",
      "[00:59:59.380 --> 01:00:11.380]   So we say the lower bound is square root of HSAG.\n",
      "[01:00:11.380 --> 01:00:17.380]   So up to log factor, it will exactly match the UCBVI.\n",
      "[01:00:17.380 --> 01:00:23.380]   So this lower bound is slightly more involved, and we will probably not talk about it in this class.\n",
      "[01:00:23.380 --> 01:00:30.380]   And we'll talk about alternative, which kind of captures some of the essential idea of how we do the extension.\n",
      "[01:00:30.380 --> 01:00:36.380]   So this type of result is so-called in a non-stationary setting.\n",
      "[01:00:36.380 --> 01:00:40.380]   [silence]\n",
      "[01:00:40.380 --> 01:00:48.380]   Non-stationary setting means P1 may not be equal to P2, not necessarily equal to P2, and not necessarily equal to PT.\n",
      "[01:00:48.380 --> 01:00:53.380]   So they can completely different. That is the transition at each step is different.\n",
      "[01:00:53.380 --> 01:01:09.380]   So we can look at a simpler case, which is stationary case, where P1 need to be equal to P2, need to be equal to PT.\n",
      "[01:01:09.380 --> 01:01:18.380]   So it turns out in the stationary case, because the transition is easier, and we can essentially shave off everything by a square root of H factor.\n",
      "[01:01:18.380 --> 01:01:27.380]   So we can have some modified algorithm, achieve this H square SAT, and this OTUDA, HSAG.\n",
      "[01:01:27.380 --> 01:01:34.380]   And the lower bound is also a square root of HSAG.\n",
      "[01:01:34.380 --> 01:01:37.380]   So we'll talk about this lower bound.\n",
      "[01:01:37.380 --> 01:01:41.380]   This is going to be the lower bound we talk about.\n",
      "[01:01:41.380 --> 01:02:04.380]   [silence]\n",
      "[01:02:04.380 --> 01:02:14.380]   So the theorem, we're going to do this for any algorithm A.\n",
      "[01:02:14.380 --> 01:02:21.380]   [silence]\n",
      "[01:02:21.380 --> 01:02:50.380]   There exists A and DP, with initialization, as 1 to K, K from little 1 to K,\n",
      "[01:02:50.380 --> 01:03:02.380]   so that the expectation over A regret.\n",
      "[01:03:02.380 --> 01:03:08.380]   [silence]\n",
      "[01:03:08.380 --> 01:03:14.380]   That is defined as the summation K from little 1 to K.\n",
      "[01:03:14.380 --> 01:03:29.380]   The value, optimal value at S1K, subtracted the value of the algorithm taken at the original K at S1K.\n",
      "[01:03:29.380 --> 01:03:47.380]   This is lower bounded by C times square root of HSAT.\n",
      "[01:03:47.380 --> 01:03:55.380]   So everything is very standard, like for any algorithm, and there exists MDP, so this is standard of lower bound, the kind of statement.\n",
      "[01:03:55.380 --> 01:04:02.380]   The only thing like slightly different from what we have been previously talking about is this initialization.\n",
      "[01:04:02.380 --> 01:04:15.380]   We're saying there exists some initialization, and we remember in upper bound for simplicity, throughout this class, we talk about for some fixed, for some fixed initialization.\n",
      "[01:04:15.380 --> 01:04:20.380]   So it turns out upper bound also works for a adversarial initialization.\n",
      "[01:04:20.380 --> 01:04:29.380]   That's actually a very easy adaptation. I haven't talked in the class, but you can easily essentially use the same proof to say upper bound.\n",
      "[01:04:29.380 --> 01:04:37.380]   Works for a adversarial initialization.\n",
      "[01:04:37.380 --> 01:04:43.380]   [silence]\n",
      "[01:04:43.380 --> 01:04:52.380]   That is, suppose I have a diversory which just tells me what is the initial state at each episode.\n",
      "[01:04:52.380 --> 01:05:01.380]   So to make our life slightly easier, we essentially say the lower bound, we also state the lower bound for adversarial initialization.\n",
      "[01:05:01.380 --> 01:05:27.380]   [silence]\n",
      "[01:05:27.380 --> 01:05:32.380]   As I said, this is just to make our lower bound is a bit easier to prove.\n",
      "[01:05:32.380 --> 01:05:38.380]   I think for fixed initialization, the lower bound construction needs to be a little bit more complicated.\n",
      "[01:05:38.380 --> 01:05:44.380]   I think the reason we can't do this is because in the upper bound we can actually allow k-different initializations.\n",
      "[01:05:44.380 --> 01:05:48.380]   So lower bound as long as we match the upper bound is fine.\n",
      "[01:05:48.380 --> 01:05:54.380]   [silence]\n",
      "[01:05:54.380 --> 01:05:59.380]   Because for lower bound, the harder the problem, the easier we prove the lower bound.\n",
      "[01:05:59.380 --> 01:06:02.380]   Lower bound is trying to say the problem is harder.\n",
      "[01:06:02.380 --> 01:06:05.380]   [silence]\n",
      "[01:06:05.380 --> 01:06:09.380]   You can just pick whatever the initialization, adversarial.\n",
      "[01:06:09.380 --> 01:06:17.380]   [silence]\n",
      "[01:06:17.380 --> 01:06:25.380]   Okay.\n",
      "[01:06:25.380 --> 01:06:35.380]   So with a multi-ambonator already did all the hard work to proving this specialized set up lower bound is actually very straightforward now.\n",
      "[01:06:35.380 --> 01:06:41.380]   So let's again consider the following distribution of hard problem.\n",
      "[01:06:41.380 --> 01:06:52.380]   [silence]\n",
      "[01:06:52.380 --> 01:06:56.380]   Yeah, this is stationary. Sorry, this is stationary.\n",
      "[01:06:56.380 --> 01:07:04.380]   [silence]\n",
      "[01:07:04.380 --> 01:07:09.380]   [silence]\n",
      "[01:07:09.380 --> 01:07:16.380]   We start with the case where we have three states and then it's very easy to extend to as states.\n",
      "[01:07:16.380 --> 01:07:24.380]   Let's first consider three states.\n",
      "[01:07:24.380 --> 01:07:28.380]   So construction is also very straightforward. So this is a three state MDP.\n",
      "[01:07:28.380 --> 01:07:36.380]   We have a zero state, one state, and two states.\n",
      "[01:07:36.380 --> 01:07:46.380]   In zero states, I have one special arm that will transition to state one with probability one-half plus epsilon.\n",
      "[01:07:46.380 --> 01:08:02.380]   And all other action will transition to state one with probability one-half.\n",
      "[01:08:02.380 --> 01:08:10.380]   And similarly, starting from state zero, transition to state two, if I take the special action at a star,\n",
      "[01:08:10.380 --> 01:08:14.380]   we will have one-half minus epsilon probability to transition to the next state,\n",
      "[01:08:14.380 --> 01:08:24.380]   where on the other hand, if we take any other actions, we will do one-half.\n",
      "[01:08:24.380 --> 01:08:30.380]   Every word associated in this transition, no reward here.\n",
      "[01:08:30.380 --> 01:08:43.380]   [silence]\n",
      "[01:08:43.380 --> 01:08:49.380]   So the part, once we go to the state one and the state two, that's very straightforward.\n",
      "[01:08:49.380 --> 01:08:54.380]   The state one is an absorbing state, so whatever action you do, you always go back to state one.\n",
      "[01:08:54.380 --> 01:09:07.380]   And state two is, again, it's also absorbing. So whatever the action you take, you also go back there.\n",
      "[01:09:07.380 --> 01:09:16.380]   And the only difference is when you go to state one, every time you get a reward one.\n",
      "[01:09:16.380 --> 01:09:21.380]   And when you go to state two, all times you get a reward equal to zero.\n",
      "[01:09:21.380 --> 01:09:28.380]   [silence]\n",
      "[01:09:28.380 --> 01:09:34.380]   And this is stationary because this is a three-state MDP, where the transition is basically, I give you here,\n",
      "[01:09:34.380 --> 01:09:42.380]   so this transition is the same for R-steps.\n",
      "[01:09:42.380 --> 01:09:55.380]   [silence]\n",
      "[01:09:55.380 --> 01:10:01.380]   And any questions?\n",
      "[01:10:01.380 --> 01:10:05.380]   [inaudible]\n",
      "[01:10:05.380 --> 01:10:11.380]   Hard means this is the MDP, we can prove lower bound. Essentially, it will confuse algorithm.\n",
      "[01:10:11.380 --> 01:10:19.380]   Like essentially, we can prove this lower bound.\n",
      "[01:10:19.380 --> 01:10:26.380]   Or essentially, this is like a construction of MDP, we can achieve this lower bound, you can see.\n",
      "[01:10:26.380 --> 01:10:31.380]   And now, we need to look at the distribution of hard MDP. This is like a single MDP.\n",
      "[01:10:31.380 --> 01:10:43.380]   So distribution is, again, very simple. I just sample a star. The special arm is also uniformly from all actions.\n",
      "[01:10:43.380 --> 01:10:53.380]   [silence]\n",
      "[01:10:53.380 --> 01:11:01.380]   [silence]\n",
      "[01:11:01.380 --> 01:11:14.380]   [silence]\n",
      "[01:11:14.380 --> 01:11:20.380]   So because I can choose whatever the initial states is, like the Viceros picking initial states.\n",
      "[01:11:20.380 --> 01:11:27.380]   So in this case, I will just make initial states to be zero.\n",
      "[01:11:27.380 --> 01:11:32.380]   And let's see what happens in the first step.\n",
      "[01:11:32.380 --> 01:11:37.380]   So what happens in the first step is if we pick the special arm A,\n",
      "[01:11:37.380 --> 01:11:43.380]   [silence]\n",
      "[01:11:43.380 --> 01:11:45.380]   the transition,\n",
      "[01:11:45.380 --> 01:11:49.380]   [silence]\n",
      "[01:11:49.380 --> 01:11:58.380]   We're going to say, so we're going to transition to state one with probability one-half plus epsilon.\n",
      "[01:11:58.380 --> 01:12:01.380]   And state zero.\n",
      "[01:12:01.380 --> 01:12:08.380]   And state two with probability one-half minus epsilon.\n",
      "[01:12:08.380 --> 01:12:15.380]   And the cumulative reward we're going to get is actually just H minus one in a state one case.\n",
      "[01:12:15.380 --> 01:12:22.380]   So for the other remaining steps, I'm going to just circle around state one.\n",
      "[01:12:22.380 --> 01:12:32.380]   And it's a absorbing state, so I will never go out. This is like cumulative reward.\n",
      "[01:12:32.380 --> 01:12:38.380]   [silence]\n",
      "[01:12:38.380 --> 01:12:44.380]   And this is zero.\n",
      "[01:12:44.380 --> 01:12:51.380]   And if we choose all other arms,\n",
      "[01:12:51.380 --> 01:13:00.380]   we also again have the problem where state one, state two,\n",
      "[01:13:00.380 --> 01:13:15.380]   one-half, one-half, and H minus one zero.\n",
      "[01:13:15.380 --> 01:13:31.380]   [silence]\n",
      "[01:13:31.380 --> 01:13:34.380]   So for those of you who kind of already observed a pattern,\n",
      "[01:13:34.380 --> 01:13:39.380]   this is precisely what we have for the hard instance for the multi-ambentant,\n",
      "[01:13:39.380 --> 01:13:44.380]   with the only difference that in a multi-ambentant, the special arm is like zero one.\n",
      "[01:13:44.380 --> 01:13:46.380]   Bernoulli one-half plus epsilon.\n",
      "[01:13:46.380 --> 01:13:51.380]   Now, instead of Bernoulli, like we get the reward that is H minus one instead of one.\n",
      "[01:13:51.380 --> 01:14:11.380]   So just all the reward is scaling up by H minus one.\n",
      "[01:14:11.380 --> 01:14:16.380]   So to get the same hard instance as MAB,\n",
      "[01:14:16.380 --> 01:14:39.380]   except the reward of H minus one factor.\n",
      "[01:14:39.380 --> 01:14:50.380]   In that sense, we can immediately prove the regret,\n",
      "[01:14:50.380 --> 01:14:56.380]   which is again directly affected by the scaling of reward is also scaled up by H minus one factor.\n",
      "[01:14:56.380 --> 01:15:03.380]   So we know in the multi-ambentant case, we know the regret is some consonant times a K.\n",
      "[01:15:03.380 --> 01:15:08.380]   T is like the multi-ambentant. In this case, like the number of episodes, which is K,\n",
      "[01:15:08.380 --> 01:15:13.380]   and times the scaling, which is H minus one.\n",
      "[01:15:13.380 --> 01:15:27.380]   And this is equal to C prime times H, H, because we observe one of the H inside.\n",
      "[01:15:27.380 --> 01:15:54.380]   Like we make T is equal to K. And essentially prove the case for the number of states is small.\n",
      "[01:15:54.380 --> 01:15:59.380]   And finally, for multiple state case, it's also relatively straightforward.\n",
      "[01:15:59.380 --> 01:16:17.380]   That we just, for as state, we kind of construct S over three.\n",
      "[01:16:17.380 --> 01:16:22.380]   How are we able to do it if it's possible in initial states?\n",
      "[01:16:22.380 --> 01:16:26.380]   Because I do not necessarily choose initial states.\n",
      "[01:16:26.380 --> 01:16:32.380]   So I just fix the initial state to be zero in that case.\n",
      "[01:16:32.380 --> 01:16:40.380]   Are we the other ones have lower regret?\n",
      "[01:16:40.380 --> 01:16:45.380]   Everyone has no regret, right, because you cannot do it.\n",
      "[01:16:45.380 --> 01:16:54.380]   So for S states, we can essentially construct S over three copies of higher instance.\n",
      "[01:16:54.380 --> 01:17:08.380]   Of the higher instance, we just previously constructed with different special arm.\n",
      "[01:17:08.380 --> 01:17:26.380]   Different means like those special arm are independently drawn from uniform distribution, like independent.\n",
      "[01:17:26.380 --> 01:17:33.380]   So if S is not multiple of three, you can just ignore the last one or two states.\n",
      "[01:17:33.380 --> 01:17:38.380]   So fine, like just make it trivial and you don't incur any regret there.\n",
      "[01:17:38.380 --> 01:17:48.380]   So let's say if you have S over three harder instance, that essentially we have something like this.\n",
      "[01:17:48.380 --> 01:17:59.380]   Like four and seven like this.\n",
      "[01:17:59.380 --> 01:18:13.380]   So what we will do is we will initialize at those states\n",
      "[01:18:13.380 --> 01:18:32.380]   and we call it each branch, precisely k over S over three times.\n",
      "[01:18:32.380 --> 01:18:42.380]   Essentially, we will just uniformly put our episodes into different branches.\n",
      "[01:18:42.380 --> 01:18:46.380]   So we know each branch we occur the same regret.\n",
      "[01:18:46.380 --> 01:18:55.380]   So we can easily compute the regret of k.\n",
      "[01:18:55.380 --> 01:19:00.380]   It's greater equal to I have S over three branches.\n",
      "[01:19:00.380 --> 01:19:11.380]   And for each branch, I am incurring the regret that is H, A, and because at each regret, it should branch.\n",
      "[01:19:11.380 --> 01:19:25.380]   I no longer pull it for T steps. I only pull it for T over S over three steps.\n",
      "[01:19:25.380 --> 01:19:35.380]   And you can prove this is roughly something double prime of the square root of H, S, A, T.\n",
      "[01:19:35.380 --> 01:19:42.380]   And I finish the proof.\n",
      "[01:19:42.380 --> 01:19:45.380]   So it seems the MDP construction is really straightforward.\n",
      "[01:19:45.380 --> 01:19:48.380]   It's just basically a banded problem.\n",
      "[01:19:48.380 --> 01:19:49.380]   I think this is a reason.\n",
      "[01:19:49.380 --> 01:19:54.380]   This is because we kind of essentially dealing with the easier case of the stationary setting.\n",
      "[01:19:54.380 --> 01:19:57.380]   The non-station setting is a little bit more complicated than that.\n",
      "[01:19:57.380 --> 01:20:00.380]   We can no longer just construct a banded case.\n",
      "[01:20:00.380 --> 01:20:12.380]   We need to construct something slightly non-trivial than that one.\n",
      "[01:20:12.380 --> 01:20:25.380]   Regardless, the summation of each round, the summation of every round.\n",
      "[01:20:25.380 --> 01:20:28.380]   But because each branch is completely independent.\n",
      "[01:20:28.380 --> 01:20:41.380]   So you can essentially think of regret as a summation of the regret incur in each branch.\n",
      "[01:20:41.380 --> 01:20:45.380]   I think the whole idea is you can use the intuitive idea.\n",
      "[01:20:45.380 --> 01:20:48.380]   We talk about it in the last lecture.\n",
      "[01:20:48.380 --> 01:20:53.380]   And this lecture, the KL computation in the first part of the lecture,\n",
      "[01:20:53.380 --> 01:21:00.380]   is basically for rigorous information theoretical argument on how we proved that kind of lower bounds.\n",
      "[01:21:00.380 --> 01:21:05.380]   But essentially, we developed this lower bound to match the upper bound to say the upper bound\n",
      "[01:21:05.380 --> 01:21:12.380]   that we developed is sharp.\n",
      "[01:21:12.380 --> 01:21:18.380]   Starting from the next lecture, we will talk about the very last part of the tabular MDP\n",
      "[01:21:18.380 --> 01:21:24.380]   that is the offline scenario where we no longer have online access or simulator.\n",
      "[01:21:24.380 --> 01:21:28.380]   Instead, we have some other people collect a bunch of data for me.\n",
      "[01:21:28.380 --> 01:21:32.380]   I'm going to reason what is optimal policy based on those offline data,\n",
      "[01:21:32.380 --> 01:21:36.380]   which is another setting, which is also frequently used in practice.\n",
      "[01:21:36.380 --> 01:21:41.380]   And then we will go to the more advanced function approximation,\n",
      "[01:21:41.380 --> 01:21:45.380]   like in the large space scenario where we no longer have this finite number of states,\n",
      "[01:21:45.380 --> 01:21:50.380]   and we'll talk about how we can handle those kind of settings.\n",
      "[01:21:50.380 --> 01:21:55.380]   Question, so we made a hard problem that has this regret,\n",
      "[01:21:55.380 --> 01:22:00.380]   and then we could make a significantly harder problem that has a higher regret lower bound\n",
      "[01:22:00.380 --> 01:22:02.380]   because we know that there exists an upper bound?\n",
      "[01:22:02.380 --> 01:22:03.380]   Yes, exactly.\n",
      "[01:22:03.380 --> 01:22:05.380]   That's a good problem.\n",
      "[01:22:05.380 --> 01:22:06.380]   Thank you.\n",
      "[01:22:06.380 --> 01:22:11.380]   Okay, so we'll see you in the week after.\n",
      "[01:22:11.380 --> 01:22:13.380]   Also, happy spring break.\n",
      "[01:22:13.380 --> 01:22:23.380]   [BLANK_AUDIO]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/fjbTCPAj254.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/fjbTCPAj254.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/fjbTCPAj254.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/fjbTCPAj254.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1272.82 ms\n",
      "whisper_print_timings:     fallbacks =  11 p /   9 h\n",
      "whisper_print_timings:      mel time =  2806.47 ms\n",
      "whisper_print_timings:   sample time = 24221.79 ms / 58524 runs (    0.41 ms per run)\n",
      "whisper_print_timings:   encode time =   393.25 ms /   221 runs (    1.78 ms per run)\n",
      "whisper_print_timings:   decode time =  1467.49 ms /   858 runs (    1.71 ms per run)\n",
      "whisper_print_timings:   batchd time = 30023.77 ms / 56566 runs (    0.53 ms per run)\n",
      "whisper_print_timings:   prompt time = 11133.20 ms / 50067 runs (    0.22 ms per run)\n",
      "whisper_print_timings:    total time = 71810.72 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=wR81RH-GI9U started\n",
      "wR81RH-GI9U\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/wR81RH-GI9U.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/wR81RH-GI9U.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:17:49.77, start: 0.000000, bitrate: 250 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 151 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/wR81RH-GI9U.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  145930kB time=01:17:49.76 bitrate= 256.0kbits/s speed=1.27e+03x    \n",
      "video:0kB audio:145930kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000052%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/wR81RH-GI9U.wav' (74716241 samples, 4669.8 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.120]   [MUSIC PLAYING]\n",
      "[00:00:03.120 --> 00:00:06.120]   So I think we have been talking about online reinforcement\n",
      "[00:00:06.120 --> 00:00:08.560]   learning and exploration versus exploitation\n",
      "[00:00:08.560 --> 00:00:11.080]   trade-off over the past week.\n",
      "[00:00:11.080 --> 00:00:14.520]   So let's just wrap up this.\n",
      "[00:00:14.520 --> 00:00:18.560]   So we have been talking about online reinforcement learning.\n",
      "[00:00:18.560 --> 00:00:22.400]   Essentially, we look at a lot of regret, time complexity,\n",
      "[00:00:22.400 --> 00:00:23.880]   and space complexity.\n",
      "[00:00:23.880 --> 00:00:28.680]   So let's consider like a run algorithm for T iteration,\n",
      "[00:00:28.680 --> 00:00:30.280]   run algorithm for T episodes.\n",
      "[00:00:30.280 --> 00:00:41.440]   So we're asking what kind of guarantees we can provide.\n",
      "[00:00:41.440 --> 00:00:45.280]   So there are a lot of metrics we're looking at.\n",
      "[00:00:45.280 --> 00:00:48.480]   First, we're looking at what kind of different algorithm.\n",
      "[00:00:48.480 --> 00:00:52.800]   And then we look at the regret of algorithm.\n",
      "[00:00:52.800 --> 00:00:55.680]   And we can look at the time complexity of the algorithm,\n",
      "[00:00:55.680 --> 00:01:00.320]   and then also the space complexity of the algorithm.\n",
      "[00:01:00.320 --> 00:01:01.480]   OK.\n",
      "[00:01:01.480 --> 00:01:06.120]   So historically, there are a lot of earlier algorithm,\n",
      "[00:01:06.120 --> 00:01:09.000]   where I would say the earliest algorithm that\n",
      "[00:01:09.000 --> 00:01:15.360]   achieved like sublinear regret is so-called e-cube or R-max.\n",
      "[00:01:15.360 --> 00:01:18.760]   Those algorithm we haven't introduced in the class,\n",
      "[00:01:18.760 --> 00:01:20.640]   but basically the regret they can achieve\n",
      "[00:01:20.640 --> 00:01:27.120]   is something polynomial in terms of HSA, and times T\n",
      "[00:01:27.120 --> 00:01:30.880]   to the 1 minus R5, where R5 is not very good.\n",
      "[00:01:30.880 --> 00:01:38.240]   And later, there are some other algorithm called UCRL.\n",
      "[00:01:38.240 --> 00:01:42.800]   This algorithm is different from algorithm we introduced\n",
      "[00:01:42.800 --> 00:01:45.920]   in the class, where the algorithm we introduced in the class\n",
      "[00:01:45.920 --> 00:01:48.320]   is like a VIUCB, where we put up confidence bound\n",
      "[00:01:48.320 --> 00:01:49.600]   on the value function.\n",
      "[00:01:49.600 --> 00:01:59.440]   This put a confidence bound on the transition on transition.\n",
      "[00:01:59.440 --> 00:02:14.000]   And it will require at least a square root of s, square h,\n",
      "[00:02:14.000 --> 00:02:14.960]   regret.\n",
      "[00:02:14.960 --> 00:02:17.000]   Regrett is at least this.\n",
      "[00:02:17.000 --> 00:02:18.840]   So more importantly, this will actually\n",
      "[00:02:18.840 --> 00:02:21.040]   pay something s square dependence,\n",
      "[00:02:21.040 --> 00:02:23.440]   which is the reason is they put a confidence bound\n",
      "[00:02:23.440 --> 00:02:24.520]   on the transition.\n",
      "[00:02:24.520 --> 00:02:28.240]   We kind of emphasize this very important difference\n",
      "[00:02:28.240 --> 00:02:30.720]   in the similar setting, where our somehow\n",
      "[00:02:30.720 --> 00:02:34.880]   is at least a skill with SA instead of s square.\n",
      "[00:02:34.880 --> 00:02:36.440]   Well, in a regret, you're basically\n",
      "[00:02:36.440 --> 00:02:37.920]   putting everything in a square root.\n",
      "[00:02:37.920 --> 00:02:41.280]   But basically, this UCRL is because it's not doing very\n",
      "[00:02:41.280 --> 00:02:45.640]   tight, so it will still pay s square dependency.\n",
      "[00:02:45.640 --> 00:02:48.400]   So what do we have introduced in this lecture\n",
      "[00:02:48.400 --> 00:02:52.960]   is the first algorithm is the UCRL.\n",
      "[00:02:52.960 --> 00:02:55.720]   And specifically, the algorithm we introduced in the class\n",
      "[00:02:55.720 --> 00:03:01.000]   is a half-ding version, half-ding version of bonus.\n",
      "[00:03:07.480 --> 00:03:13.740]   The regret we received is 0 tilde, square root of h cube\n",
      "[00:03:13.740 --> 00:03:14.240]   sat.\n",
      "[00:03:14.240 --> 00:03:22.480]   And the time complexity we will use\n",
      "[00:03:22.480 --> 00:03:29.360]   is we can easily count that in every episode per episode,\n",
      "[00:03:29.360 --> 00:03:31.640]   we're going to basically doing an entire path\n",
      "[00:03:31.640 --> 00:03:32.920]   of value iteration.\n",
      "[00:03:32.920 --> 00:03:36.480]   So that will take some hs square a time.\n",
      "[00:03:36.480 --> 00:03:40.880]   So the total time would be t s square a.\n",
      "[00:03:40.880 --> 00:03:43.000]   We will actually explain this later.\n",
      "[00:03:43.000 --> 00:03:46.560]   But basically, in every episode, we're\n",
      "[00:03:46.560 --> 00:03:50.440]   doing a full path of value iteration.\n",
      "[00:03:50.440 --> 00:03:52.320]   And a space complexity is something\n",
      "[00:03:52.320 --> 00:03:56.440]   like a hs square to store entire transition.\n",
      "[00:03:56.440 --> 00:04:01.480]   And in the class, we also mentioned\n",
      "[00:04:01.480 --> 00:04:10.320]   there's a Winston version, version of bonus, which\n",
      "[00:04:10.320 --> 00:04:13.680]   can achieve slightly sharper regret guarantee,\n",
      "[00:04:13.680 --> 00:04:17.920]   which is 0 tilde h square, I say.\n",
      "[00:04:17.920 --> 00:04:27.200]   And it will be the same time times area and space complexity.\n",
      "[00:04:27.200 --> 00:04:29.440]   So the benefit of using Winston bonus,\n",
      "[00:04:29.440 --> 00:04:33.280]   as we said, in last lecture, the sharper bonus you use,\n",
      "[00:04:33.280 --> 00:04:36.680]   the better regret you will have.\n",
      "[00:04:36.680 --> 00:04:38.920]   And only complications, so the Winston\n",
      "[00:04:38.920 --> 00:04:40.400]   requires some variance estimation,\n",
      "[00:04:40.400 --> 00:04:43.800]   so the algorithm is a little bit more complicated.\n",
      "[00:04:43.800 --> 00:04:46.220]   And in the last lecture, we also introduced the Q-learning\n",
      "[00:04:46.220 --> 00:04:56.800]   UCB and the regret guarantee for Q-learning\n",
      "[00:04:56.800 --> 00:05:04.160]   with UCB is 0 tilde, square root of h to the fourth SAT,\n",
      "[00:05:04.160 --> 00:05:08.360]   slightly worse than the UCBVI.\n",
      "[00:05:08.360 --> 00:05:10.480]   But the major benefit of Q-learning\n",
      "[00:05:10.480 --> 00:05:14.400]   is in every episode, we're just looking at a one-tris-acture.\n",
      "[00:05:14.400 --> 00:05:17.160]   And we're only doing an update on that tris-acture.\n",
      "[00:05:17.160 --> 00:05:20.400]   So the time is 0 tilde t.\n",
      "[00:05:20.400 --> 00:05:21.400]   Everything is 0 tilde.\n",
      "[00:05:21.400 --> 00:05:23.320]   Probably take some log factor.\n",
      "[00:05:26.640 --> 00:05:29.200]   This is not.\n",
      "[00:05:29.200 --> 00:05:31.280]   And a space complexity, because we no longer\n",
      "[00:05:31.280 --> 00:05:33.760]   need to store the transition, we only store the value.\n",
      "[00:05:33.760 --> 00:05:35.920]   So it's a HSA.\n",
      "[00:05:35.920 --> 00:05:37.360]   So we save an S factor.\n",
      "[00:05:37.360 --> 00:05:40.560]   This is like model-free algorithm.\n",
      "[00:05:40.560 --> 00:05:42.880]   And finally, we can also have the Princeton version.\n",
      "[00:05:42.880 --> 00:05:55.320]   And that will give you a slightly better regret\n",
      "[00:05:55.320 --> 00:05:56.720]   in terms of H dependency.\n",
      "[00:05:56.720 --> 00:06:03.680]   So we have been giving a lot of upper bounds.\n",
      "[00:06:03.680 --> 00:06:05.840]   And the natural question you would ask\n",
      "[00:06:05.840 --> 00:06:10.880]   is whether those are optimal or whether we can develop some\n",
      "[00:06:10.880 --> 00:06:12.880]   better algorithm, for example, like UCB\n",
      "[00:06:12.880 --> 00:06:15.920]   with some other bonus, like a much sharper bonus.\n",
      "[00:06:15.920 --> 00:06:19.720]   Can we actually do a lot better than those guarantees?\n",
      "[00:06:19.720 --> 00:06:21.840]   So it turns out we have been talking a lot\n",
      "[00:06:21.840 --> 00:06:25.960]   about a lower bound, like multiple times.\n",
      "[00:06:25.960 --> 00:06:28.960]   So this is, again, information theoretical lower bounds.\n",
      "[00:06:28.960 --> 00:06:34.800]   You can actually prove the best you can do is actually just\n",
      "[00:06:34.800 --> 00:06:36.960]   a square root of H square SAT.\n",
      "[00:06:36.960 --> 00:06:42.960]   So no algorithm can beat this.\n",
      "[00:06:50.160 --> 00:07:02.320]   So if you're remarks, essentially explain the table.\n",
      "[00:07:02.320 --> 00:07:12.280]   First is UCBVI needs to do a full path\n",
      "[00:07:12.280 --> 00:07:23.600]   of value iteration for episode.\n",
      "[00:07:23.600 --> 00:07:31.960]   This requires like essentially the time complexity\n",
      "[00:07:31.960 --> 00:07:34.680]   to do the value iteration is you have H steps.\n",
      "[00:07:34.680 --> 00:07:37.280]   And for steps, you are doing like a matrix vector\n",
      "[00:07:37.280 --> 00:07:41.560]   multiplication, so it takes H S square a time,\n",
      "[00:07:41.560 --> 00:07:44.240]   or like a flop compute.\n",
      "[00:07:44.240 --> 00:07:49.320]   So this leads to this per episode.\n",
      "[00:07:49.320 --> 00:07:51.200]   This leads to a total amount of time\n",
      "[00:07:51.200 --> 00:07:59.320]   is T square and T S.\n",
      "[00:07:59.320 --> 00:08:04.400]   OK, so that's why we put a T S square a here.\n",
      "[00:08:04.400 --> 00:08:07.160]   And why this is like a lot worse than Q learning,\n",
      "[00:08:07.160 --> 00:08:09.400]   where essentially in every episode,\n",
      "[00:08:09.400 --> 00:08:13.080]   you're only doing only looking at trajectories\n",
      "[00:08:13.080 --> 00:08:14.960]   and doing computation based on trajectories.\n",
      "[00:08:14.960 --> 00:08:27.520]   Second one, we already stated this in the last lecture\n",
      "[00:08:27.520 --> 00:08:42.600]   that a Brinston bonus require variance\n",
      "[00:08:42.600 --> 00:08:45.840]   as made, so it's more difficult to compute.\n",
      "[00:08:45.840 --> 00:09:07.640]   But I give Schauper grad, and which\n",
      "[00:09:07.640 --> 00:09:10.040]   will leads to a Schauper's sample chemistry as well.\n",
      "[00:09:10.040 --> 00:09:22.880]  , and finally, we also noticed\n",
      "[00:09:22.880 --> 00:09:28.240]   in terms of regrets, the UCBVI with Brinston bonuses\n",
      "[00:09:28.240 --> 00:09:55.080]   already the optimal.\n",
      "[00:09:55.080 --> 00:10:02.600]   Near, we call it near optimal request, where this near optimal\n",
      "[00:10:02.600 --> 00:10:04.680]   means up to some log vector.\n",
      "[00:10:04.680 --> 00:10:16.760]   We'll see today the lower bound.\n",
      "[00:10:16.760 --> 00:10:18.720]   We typically do some expectation,\n",
      "[00:10:18.720 --> 00:10:19.920]   and we don't have the log vector.\n",
      "[00:10:19.920 --> 00:10:21.800]   But the upper bound, we do the high probability,\n",
      "[00:10:21.800 --> 00:10:22.760]   we have the log vector.\n",
      "[00:10:22.760 --> 00:10:25.600]   So eventually, if we don't really care about the log\n",
      "[00:10:25.600 --> 00:10:28.840]   vector, and this UCBVI with Brinston bonus,\n",
      "[00:10:28.840 --> 00:10:30.200]   already match the lower bounds.\n",
      "[00:10:30.200 --> 00:10:46.440]   Any questions on this summary of online reinforcement\n",
      "[00:10:46.440 --> 00:10:48.000]   learning in the tabular setting so far?\n",
      "[00:10:48.000 --> 00:11:03.960]   OK, so if no questions, I guess\n",
      "[00:11:03.960 --> 00:11:09.640]   we will move to the next chapter of this lecture.\n",
      "[00:11:09.640 --> 00:11:11.480]   We will mostly talk about lower bounds.\n",
      "[00:11:11.480 --> 00:11:22.320]   So far, we have been mentioning this lower bounds\n",
      "[00:11:22.320 --> 00:11:24.880]   for multiple times in this class.\n",
      "[00:11:24.880 --> 00:11:27.760]   We only very vaguely say this is some information theoretical\n",
      "[00:11:27.760 --> 00:11:30.200]   lower bound due to some statistical noise.\n",
      "[00:11:30.200 --> 00:11:34.640]   And but we never give a precise definition or something.\n",
      "[00:11:34.640 --> 00:11:38.920]   So we will dig more into what type of lower bounds we want,\n",
      "[00:11:38.920 --> 00:11:41.720]   and how we're going to achieve some algorithm independent\n",
      "[00:11:41.720 --> 00:11:44.240]   lower bounds.\n",
      "[00:11:44.240 --> 00:11:46.520]   So for the start, for simplicity,\n",
      "[00:11:46.520 --> 00:11:50.760]   we will first focus on the multi-arm banded case\n",
      "[00:11:50.760 --> 00:11:53.920]   for this lecture.\n",
      "[00:11:53.920 --> 00:11:55.920]   In the next lecture, we will talk about how\n",
      "[00:11:55.920 --> 00:11:58.240]   to generalize it to a Markov decision process\n",
      "[00:11:58.240 --> 00:11:59.640]   and reinforcement learning.\n",
      "[00:11:59.640 --> 00:12:02.480]   But a lot of key ideas already containing\n",
      "[00:12:02.480 --> 00:12:10.080]   establishing this lower bounds for multi-arm banded.\n",
      "[00:12:10.080 --> 00:12:12.480]   So recall, what kind of lower bounds\n",
      "[00:12:12.480 --> 00:12:15.720]   we want to achieve in multi-arm banded.\n",
      "[00:12:15.720 --> 00:12:17.240]   Whenever we talk about lower bounds,\n",
      "[00:12:17.240 --> 00:12:20.520]   we usually want to say lower bounds will give some guarantees\n",
      "[00:12:20.520 --> 00:12:23.200]   to say some algorithm is already optimal.\n",
      "[00:12:23.200 --> 00:12:26.240]   So I think the first thing is that we look\n",
      "[00:12:26.240 --> 00:12:31.080]   at what kind of upper bounds we want to match,\n",
      "[00:12:31.080 --> 00:12:33.600]   or we want to prove guarantees that this upper bound is already\n",
      "[00:12:33.600 --> 00:12:34.600]   tight.\n",
      "[00:12:34.600 --> 00:12:42.320]   So just to recall, in the multi-arm banded,\n",
      "[00:12:42.320 --> 00:12:46.680]   we have following regret guarantee for the UCB algorithm.\n",
      "[00:12:46.680 --> 00:13:09.440]   So theorem is the following that, with probability,\n",
      "[00:13:09.440 --> 00:13:16.440]   at least 1 minus p, the regret of a UCB algorithm.\n",
      "[00:13:16.440 --> 00:13:36.640]   It is at the most following that regret t,\n",
      "[00:13:36.640 --> 00:13:43.840]   summation from 1 to a capital T,\n",
      "[00:13:43.840 --> 00:13:47.960]   i*, which is the optimal reward you can achieve,\n",
      "[00:13:47.960 --> 00:13:50.960]   achieved by putting an optimal arm, subject to it,\n",
      "[00:13:50.960 --> 00:13:53.560]   the reward you're going to achieve by putting your arm.\n",
      "[00:13:53.560 --> 00:13:55.480]   We say this is definition of the regret,\n",
      "[00:13:55.480 --> 00:13:58.040]   and the UCB algorithm will guarantee\n",
      "[00:13:58.040 --> 00:14:00.560]   the regret will scale with upper bounded\n",
      "[00:14:00.560 --> 00:14:06.640]   by some constants times square root of a t Yoda,\n",
      "[00:14:06.640 --> 00:14:09.280]   where Yoda is this log factor.\n",
      "[00:14:09.280 --> 00:14:21.080]   So in the previous lecture, we have introduced this regret\n",
      "[00:14:21.080 --> 00:14:21.600]   guarantee.\n",
      "[00:14:21.600 --> 00:14:30.600]   So the reason we introduced multi-arm banded\n",
      "[00:14:30.600 --> 00:14:32.240]   like UCB algorithm, one of the reasons\n",
      "[00:14:32.240 --> 00:14:35.040]   like this field has been studied for a very long time.\n",
      "[00:14:35.040 --> 00:14:37.560]   It turns out this algorithm is like the optimal algorithm.\n",
      "[00:14:37.560 --> 00:14:39.840]   This is like the best algorithm you can achieve.\n",
      "[00:14:39.840 --> 00:14:41.760]   So today, we'll actually prove a lower bound,\n",
      "[00:14:41.760 --> 00:14:45.440]   saying no algorithm can achieve better than this.\n",
      "[00:14:45.440 --> 00:14:47.640]   So let's first think what kind of statement\n",
      "[00:14:47.640 --> 00:14:50.640]   will basically say this fact, like no algorithm\n",
      "[00:14:50.640 --> 00:14:53.360]   is better than UCB in terms of achieving this regret.\n",
      "[00:14:53.360 --> 00:15:05.800]   So the theorem we're going to prove today\n",
      "[00:15:05.800 --> 00:15:07.600]   is something like following.\n",
      "[00:15:07.600 --> 00:15:10.400]   First, we will say no algorithm better than UCB,\n",
      "[00:15:10.400 --> 00:15:13.880]   so we do need to say for any algorithm.\n",
      "[00:15:13.880 --> 00:15:17.200]   We don't need to restrict to a certain class of algorithm\n",
      "[00:15:17.200 --> 00:15:19.560]   like doing this operation, doing that algorithm.\n",
      "[00:15:19.560 --> 00:15:22.240]   We just want to have the strongest statement that\n",
      "[00:15:22.240 --> 00:15:23.600]   is for any algorithm.\n",
      "[00:15:31.200 --> 00:15:38.060]   Algorithm A, which can be even random algorithm.\n",
      "[00:15:38.060 --> 00:15:44.480]   So we want to say even the random algorithm cannot beat\n",
      "[00:15:44.480 --> 00:15:45.240]   how are UCB.\n",
      "[00:15:45.240 --> 00:15:54.000]   And this is an algorithm part.\n",
      "[00:15:54.000 --> 00:15:58.600]   But on the other hand, we have to say something\n",
      "[00:15:58.600 --> 00:16:03.040]   about a problem, because for any algorithm,\n",
      "[00:16:03.040 --> 00:16:08.280]   if the problem is easy, then we won't have any lower bounds\n",
      "[00:16:08.280 --> 00:16:09.200]   to say the regret.\n",
      "[00:16:09.200 --> 00:16:21.740]   So we'll actually say there exists a multi-\n",
      "[00:16:21.740 --> 00:16:33.980]   algorithm. So this problem can be very specific to algorithm\n",
      "[00:16:33.980 --> 00:16:39.140]   A, which is the hardest multi-armed problem regarding\n",
      "[00:16:39.140 --> 00:16:42.220]   to respect to algorithm A. So different algorithm\n",
      "[00:16:42.220 --> 00:16:45.220]   may encounter difficulty in different multi-armed\n",
      "[00:16:45.220 --> 00:16:45.980]   independent problems.\n",
      "[00:16:45.980 --> 00:16:48.060]   But we just want to find a multi-armed problem.\n",
      "[00:16:48.060 --> 00:16:52.500]   That is the hardest for this particular algorithm A.\n",
      "[00:16:52.500 --> 00:16:59.260]   So that we have the following that the expectation\n",
      "[00:16:59.260 --> 00:17:02.860]   in terms of algorithm A, because we\n",
      "[00:17:02.860 --> 00:17:04.460]   might be random algorithm.\n",
      "[00:17:04.460 --> 00:17:14.620]   So this is taking an expectation over the internal\n",
      "[00:17:14.620 --> 00:17:37.980]   randomness of algorithm A.\n",
      "[00:17:37.980 --> 00:17:44.820]   We see the expected request of T round is at least some constant\n",
      "[00:17:44.820 --> 00:17:47.100]   c times square root of A t.\n",
      "[00:17:47.100 --> 00:17:58.980]   So we know there is no log vector,\n",
      "[00:17:58.980 --> 00:18:00.740]   but we will do expected request.\n",
      "[00:18:00.740 --> 00:18:12.460]   So the algorithm is different than the A and the square root\n",
      "[00:18:12.460 --> 00:18:13.460]   root.\n",
      "[00:18:13.460 --> 00:18:13.980]   Yes.\n",
      "[00:18:13.980 --> 00:18:17.060]   The question is, this algorithm A is different from this A.\n",
      "[00:18:17.060 --> 00:18:18.500]   This A is the number of actions.\n",
      "[00:18:28.780 --> 00:18:31.060]   So we'll just curliate for the algorithm\n",
      "[00:18:31.060 --> 00:18:33.980]   and this standard A to be the number of actions.\n",
      "[00:18:33.980 --> 00:18:53.860]   So except to slightly mismatch where in the lower bound,\n",
      "[00:18:53.860 --> 00:18:55.860]   we do expectation where upper bound,\n",
      "[00:18:55.860 --> 00:18:58.340]   we kind of prove the high probability results.\n",
      "[00:18:58.340 --> 00:19:00.380]   So typically, this mismatch will be resolved\n",
      "[00:19:00.380 --> 00:19:03.020]   if we say for upper bound, we can also do some expected\n",
      "[00:19:03.020 --> 00:19:04.940]   regret and turns out for UCB, you can also\n",
      "[00:19:04.940 --> 00:19:08.460]   do expected regret, we'll omit these details.\n",
      "[00:19:08.460 --> 00:19:11.340]   But other than this slightly mismatch,\n",
      "[00:19:11.340 --> 00:19:15.660]   basically, other stuff are matched with each other.\n",
      "[00:19:15.660 --> 00:19:24.380]   So we'll say in a high level, what we are saying\n",
      "[00:19:24.380 --> 00:19:33.380]   is upper bound or theorem 1 is more\n",
      "[00:19:33.380 --> 00:19:37.620]   or less saying there exists some algorithm, which\n",
      "[00:19:37.620 --> 00:19:40.780]   should we kind of explicit construct, which is a UCB algorithm,\n",
      "[00:19:40.780 --> 00:19:49.540]   so that for any problem, we have the regret less than blah, blah,\n",
      "[00:19:49.540 --> 00:19:56.500]   blah, and in terms of lower bounds,\n",
      "[00:19:56.500 --> 00:20:00.220]   we're more or less doing the opposite of it.\n",
      "[00:20:00.220 --> 00:20:05.020]   What we are saying is for any algorithm,\n",
      "[00:20:05.020 --> 00:20:08.540]   there exists some problem.\n",
      "[00:20:08.540 --> 00:20:13.060]   So the regret is greater than blah, blah, blah.\n",
      "[00:20:13.060 --> 00:20:16.660]   And if those two are matched, then we say this algorithm\n",
      "[00:20:16.660 --> 00:20:18.860]   is the minimax optimal algorithm.\n",
      "[00:20:18.860 --> 00:20:45.220]   So we can observe a few things from the type of statement.\n",
      "[00:20:45.220 --> 00:20:47.180]   The first thing is, in terms of lower bound,\n",
      "[00:20:47.180 --> 00:20:50.140]   we need to talk about there exists some problem.\n",
      "[00:20:50.140 --> 00:20:52.820]   So a lot of techniques of proving lower bound\n",
      "[00:20:52.820 --> 00:20:56.820]   is basically based on constructing a hard instance.\n",
      "[00:20:56.820 --> 00:21:11.020]   So those are the most important part of the lower bound.\n",
      "[00:21:11.020 --> 00:21:13.220]   So different from upper bounds, which\n",
      "[00:21:13.220 --> 00:21:16.260]   is the most important part is in the algorithm design.\n",
      "[00:21:16.260 --> 00:21:18.260]   Well, the lower bound most important part\n",
      "[00:21:18.260 --> 00:21:20.740]   is in the construction of the hard instance.\n",
      "[00:21:20.740 --> 00:21:25.580]   And also, the reason is the second point\n",
      "[00:21:25.580 --> 00:21:26.740]   that we want to mention.\n",
      "[00:21:26.740 --> 00:21:30.100]   This is like algorithm independent lower bounds.\n",
      "[00:21:30.100 --> 00:21:39.700]   I just want to emphasize this point again.\n",
      "[00:21:39.700 --> 00:21:41.180]   The lower bound that we're trying to prove\n",
      "[00:21:41.180 --> 00:21:44.780]   is not saying a UCBVI cannot achieve blah, blah, blah,\n",
      "[00:21:44.780 --> 00:21:46.540]   or Q-learning cannot achieve blah.\n",
      "[00:21:46.540 --> 00:21:50.180]   We're saying no algorithm can't achieve this.\n",
      "[00:21:50.180 --> 00:21:53.300]   So this is like the strongest type of lower bound\n",
      "[00:21:53.300 --> 00:21:56.460]   you can imagine, and we're using the information theoretical\n",
      "[00:21:56.460 --> 00:21:56.980]   limit.\n",
      "[00:21:56.980 --> 00:22:02.380]   This is due to some information theoretical limit.\n",
      "[00:22:02.380 --> 00:22:04.180]   [NON-ENGLISH SPEECH]\n",
      "[00:22:04.180 --> 00:22:14.740]   Or statistical limit.\n",
      "[00:22:14.740 --> 00:22:25.740]   Yes.\n",
      "[00:22:25.740 --> 00:22:29.740]   What's the word in the handle of how?\n",
      "[00:22:29.740 --> 00:22:31.700]   What?\n",
      "[00:22:31.700 --> 00:22:35.020]   Oh, sorry, construct hard instance.\n",
      "[00:22:35.020 --> 00:22:38.060]   Hard instance means like for specific algorithm,\n",
      "[00:22:38.060 --> 00:22:41.660]   you basically want to say this algorithm,\n",
      "[00:22:41.660 --> 00:22:45.140]   there exists a multi-ambentant so that the regret is high.\n",
      "[00:22:45.140 --> 00:22:47.620]   So that means like for specific algorithm,\n",
      "[00:22:47.620 --> 00:22:49.940]   you will have a problem, which is like very difficult\n",
      "[00:22:49.940 --> 00:22:52.020]   for the algorithm, so that algorithm will suffer\n",
      "[00:22:52.020 --> 00:22:53.420]   very high regret.\n",
      "[00:22:53.420 --> 00:22:55.580]   So hard instance means like you will construct\n",
      "[00:22:55.580 --> 00:22:57.820]   some hard multi-ambentant problem,\n",
      "[00:22:57.820 --> 00:23:01.080]   which is hard for the algorithm you're talking about.\n",
      "[00:23:01.080 --> 00:23:07.020]   So for the lower bound is saying for any algorithm,\n",
      "[00:23:07.020 --> 00:23:08.900]   you will be able to find some problem, which\n",
      "[00:23:08.900 --> 00:23:12.300]   is very hard for algorithm, so that the regret on this problem\n",
      "[00:23:12.300 --> 00:23:15.860]   is like very large.\n",
      "[00:23:15.860 --> 00:23:19.820]   So what's the meaning of hard in this context?\n",
      "[00:23:19.820 --> 00:23:22.420]   Hard means like regret is high.\n",
      "[00:23:22.420 --> 00:23:25.420]   This is basically what I mean.\n",
      "[00:23:25.420 --> 00:23:26.420]   Yes.\n",
      "[00:23:27.420 --> 00:23:29.420]   OK.\n",
      "[00:23:29.420 --> 00:23:33.420]   So do we construct a set of hard instances\n",
      "[00:23:33.420 --> 00:23:36.420]   and those that are existing?\n",
      "[00:23:36.420 --> 00:23:38.420]   But I always think once, one instance has to change\n",
      "[00:23:38.420 --> 00:23:39.420]   the order.\n",
      "[00:23:39.420 --> 00:23:40.420]   Yes.\n",
      "[00:23:40.420 --> 00:23:42.420]   We will talk more about like how we're going\n",
      "[00:23:42.420 --> 00:23:44.420]   to construct the hard instance.\n",
      "[00:23:44.420 --> 00:23:46.420]   I think this is a very good point.\n",
      "[00:23:46.420 --> 00:23:49.420]   I think typically and later we'll talk about the techniques,\n",
      "[00:23:49.420 --> 00:23:51.420]   because right now on the basic formula,\n",
      "[00:23:51.420 --> 00:23:54.420]   it seems like for any algorithm, we need to construct\n",
      "[00:23:54.420 --> 00:23:58.220]   a hard instance, which is hard for that algorithm.\n",
      "[00:23:58.220 --> 00:24:00.020]   Then which is not very feasible, because there\n",
      "[00:24:00.020 --> 00:24:02.580]   are infinite many algorithm, and we cannot just construct\n",
      "[00:24:02.580 --> 00:24:03.500]   infinite many problems.\n",
      "[00:24:03.500 --> 00:24:05.340]   So we'll talk about the techniques,\n",
      "[00:24:05.340 --> 00:24:08.100]   how we're going to construct some hard instance\n",
      "[00:24:08.100 --> 00:24:09.900]   generically for a lot of our algorithm.\n",
      "[00:24:09.900 --> 00:24:32.900]   OK.\n",
      "[00:24:32.900 --> 00:24:36.900]   So before we talk about the formal techniques of how to\n",
      "[00:24:36.900 --> 00:24:38.900]   construct hard instance, which can be a little bit technical,\n",
      "[00:24:38.900 --> 00:24:41.900]   I first want to intuitively argue,\n",
      "[00:24:41.900 --> 00:24:46.900]   why intuitively convince your Y square root T makes sense.\n",
      "[00:24:46.900 --> 00:24:49.900]   So let's first do some entry level that is,\n",
      "[00:24:49.900 --> 00:24:53.900]   do some warm up, which we will say intuitive arguments.\n",
      "[00:24:53.900 --> 00:25:01.900]   Y square root T makes sense.\n",
      "[00:25:07.900 --> 00:25:12.900]   Why this is a fundamental limit where we cannot actually\n",
      "[00:25:12.900 --> 00:25:15.900]   bypass this limit.\n",
      "[00:25:15.900 --> 00:25:20.900]   So the hard instance we can consider in this kind of\n",
      "[00:25:20.900 --> 00:25:22.900]   problem is also quite simple.\n",
      "[00:25:22.900 --> 00:25:25.900]   This is intuitive argument, so this is not a form argument.\n",
      "[00:25:25.900 --> 00:25:27.900]   So let's consider two-arm bandits.\n",
      "[00:25:27.900 --> 00:25:47.900]   Where we have arm one and arm two.\n",
      "[00:25:47.900 --> 00:25:51.900]   So both arm, the random reward is drawn according to some\n",
      "[00:25:51.900 --> 00:25:52.900]   Bernoulli distribution.\n",
      "[00:25:53.900 --> 00:26:00.900]   Well, the first arm is draw with a Bernoulli distribution,\n",
      "[00:26:00.900 --> 00:26:03.900]   that reward is like a 1/2 plus epsilon.\n",
      "[00:26:03.900 --> 00:26:07.900]   Well, the second arm is the reward is completely drawn\n",
      "[00:26:07.900 --> 00:26:11.900]   from Bernoulli 1/2.\n",
      "[00:26:11.900 --> 00:26:19.900]   So we consider the case\n",
      "[00:26:19.900 --> 00:26:21.900]   epsilon is extremely small.\n",
      "[00:26:21.900 --> 00:26:25.900]   So you can see if you have a Bernoulli which is 1/2\n",
      "[00:26:25.900 --> 00:26:28.900]   versus 1/2 plus something extremely small.\n",
      "[00:26:28.900 --> 00:26:30.900]   Typically, by just looking at some samples,\n",
      "[00:26:30.900 --> 00:26:33.900]   it's very difficult to tell which arm is better,\n",
      "[00:26:33.900 --> 00:26:38.900]   which arm is 1/2 plus epsilon 1.\n",
      "[00:26:44.900 --> 00:27:12.900]   So the key thing is hard to tell from samples.\n",
      "[00:27:12.900 --> 00:27:26.900]   So some brief computation on how many samples we actually need\n",
      "[00:27:26.900 --> 00:27:32.900]   to tell the difference between arm one and arm two.\n",
      "[00:27:32.900 --> 00:27:37.900]   So because we are considered a case where epsilon is extremely small,\n",
      "[00:27:37.900 --> 00:27:40.900]   where we can essentially talk about some asymptotic regime,\n",
      "[00:27:40.900 --> 00:27:42.900]   where we again, in the very beginning,\n",
      "[00:27:42.900 --> 00:27:46.900]   we say we have something similar to concentration,\n",
      "[00:27:46.900 --> 00:27:48.900]   where concentration only providing upper bounds,\n",
      "[00:27:48.900 --> 00:27:50.900]   but the central limit theorem,\n",
      "[00:27:50.900 --> 00:27:55.900]   basically providing the entire behavior.\n",
      "[00:27:55.900 --> 00:28:05.900]   So by CLT.\n",
      "[00:28:05.900 --> 00:28:09.900]   By CLT, essentially, we can look at the behavior\n",
      "[00:28:09.900 --> 00:28:16.900]   of R1 bar, which is the empirical average of pulling first arm.\n",
      "[00:28:16.900 --> 00:28:33.900]   That is, I pull first arm for a lot of n times,\n",
      "[00:28:33.900 --> 00:28:35.900]   and then I compute the empirical average.\n",
      "[00:28:35.900 --> 00:28:38.900]   That is summation of all the reward and divided by n.\n",
      "[00:28:38.900 --> 00:28:45.900]   And we know our arm bar is roughly will be equal to 1/2 plus epsilon,\n",
      "[00:28:45.900 --> 00:28:51.900]   which is the expectation, and plus a Gaussian noise.\n",
      "[00:28:51.900 --> 00:28:54.900]   This is like a bicentual limit theorem,\n",
      "[00:28:54.900 --> 00:28:57.900]   where the standard deviation, the mean is 0,\n",
      "[00:28:57.900 --> 00:29:00.900]   and the standard deviation is 1 over 4n.\n",
      "[00:29:05.900 --> 00:29:13.900]   Well, the R2 bar, if we also observe n samples from the second arm,\n",
      "[00:29:13.900 --> 00:29:20.900]   its expectation is 1/2, and plus Gaussian distribution\n",
      "[00:29:20.900 --> 00:29:24.900]   with 0, 1 over 4n.\n",
      "[00:29:24.900 --> 00:29:27.900]   This is assuming we are always estimating the empirical average,\n",
      "[00:29:27.900 --> 00:29:33.900]   and the use this as my criterion to say which arm is better.\n",
      "[00:29:33.900 --> 00:30:02.900]   [ Pause ]\n",
      "[00:30:02.900 --> 00:30:06.900]   So, assuming we're using an algorithm which look at the empirical average,\n",
      "[00:30:06.900 --> 00:30:09.900]   and we say like we wish the empirical average is higher,\n",
      "[00:30:09.900 --> 00:30:13.900]   and then we just will pick that arm as my optimal arm,\n",
      "[00:30:13.900 --> 00:30:15.900]   or something like that.\n",
      "[00:30:15.900 --> 00:30:31.900]   So, we know when standard deviation\n",
      "[00:30:31.900 --> 00:30:37.900]   of this random Gaussian, that is 1 over 2 squared of n,\n",
      "[00:30:37.900 --> 00:30:43.900]   is actually roughly equal to epsilon, or epsilon constant.\n",
      "[00:30:43.900 --> 00:30:46.900]   That is, this is like our signal level that is epsilon,\n",
      "[00:30:46.900 --> 00:30:48.900]   and this is our noise.\n",
      "[00:30:48.900 --> 00:30:52.900]   So, when noise is roughly up to the signal level,\n",
      "[00:30:52.900 --> 00:31:09.900]   then we essentially know we have constant probability\n",
      "[00:31:09.900 --> 00:31:15.900]   that R1 bar is less than equal to R2 bar.\n",
      "[00:31:15.900 --> 00:31:19.900]   So, that is when this standard deviation is like basically,\n",
      "[00:31:19.900 --> 00:31:23.900]   I'm a same order or even larger than this signal.\n",
      "[00:31:23.900 --> 00:31:27.900]   Then, although the average, the meaning of arm 1 is higher than arm 2,\n",
      "[00:31:27.900 --> 00:31:31.900]   we still have like constant probability if we just look at empirical average.\n",
      "[00:31:31.900 --> 00:31:36.900]   This one will be smaller than R1 bar will be smaller than R2 bar.\n",
      "[00:31:36.900 --> 00:31:41.900]   Okay.\n",
      "[00:31:41.900 --> 00:31:43.900]   So, intuitively, in this standard scenario,\n",
      "[00:31:43.900 --> 00:31:52.900]   we have a constant probability you will actually identify the raw arm to be the optimal arm.\n",
      "[00:31:52.900 --> 00:32:05.900]   So, basically, we can tell which arm is better with high probability.\n",
      "[00:32:05.900 --> 00:32:15.900]   That is, with constant probability, we'll kind of make our own.\n",
      "[00:32:15.900 --> 00:32:22.900]   So, this essentially indicates that therefore,\n",
      "[00:32:22.900 --> 00:32:30.900]   we just pick epsilon equal to like 1 over square root of n,\n",
      "[00:32:30.900 --> 00:32:35.900]   which is where this n is, like, n is just t or, okay,\n",
      "[00:32:35.900 --> 00:32:41.900]   let's just say 1 over square t.\n",
      "[00:32:41.900 --> 00:32:48.900]   Probably up to constant factor of 2 because we kind of pull an arm on like both,\n",
      "[00:32:48.900 --> 00:32:53.900]   n times on both arm.\n",
      "[00:32:53.900 --> 00:32:58.900]   So, we say if we pick epsilon is equal to roughly something like that,\n",
      "[00:32:58.900 --> 00:33:04.900]   1 over square root of t.\n",
      "[00:33:04.900 --> 00:33:26.900]   Then we say through the entire interaction\n",
      "[00:33:26.900 --> 00:33:33.900]   with multi-ambentance for t-round,\n",
      "[00:33:33.900 --> 00:33:59.900]   so, we're not sure which arm is better.\n",
      "[00:33:59.900 --> 00:34:05.900]   We're not sure which arm is better.\n",
      "[00:34:25.900 --> 00:34:50.900]   Therefore, we're more or less doing some random guessing.\n",
      "[00:34:50.900 --> 00:34:54.900]   In this case, because we're really not sure which arm is better,\n",
      "[00:34:54.900 --> 00:34:57.900]   like whatever arm I pulled here with some constant probability,\n",
      "[00:34:57.900 --> 00:34:59.900]   it's like a wrong arm.\n",
      "[00:34:59.900 --> 00:35:07.900]   So, in that case, the expected regret would be roughly something like t,\n",
      "[00:35:07.900 --> 00:35:09.900]   t is the number of iterations,\n",
      "[00:35:09.900 --> 00:35:16.900]   times each round we're going to suffer probably like other epsilon at room.\n",
      "[00:35:16.900 --> 00:35:20.900]   Okay, because we're really not sure which arm is good.\n",
      "[00:35:20.900 --> 00:35:22.900]   So, once we pull the wrong arm,\n",
      "[00:35:22.900 --> 00:35:24.900]   we actually suffer some epsilon regret.\n",
      "[00:35:24.900 --> 00:35:34.900]   So, in that case, the total regret is something like square root of t.\n",
      "[00:35:34.900 --> 00:35:39.900]   So, this is like a bit of intuitive argument why this square root t kind of makes sense,\n",
      "[00:35:39.900 --> 00:35:41.900]   like why this is something happening,\n",
      "[00:35:41.900 --> 00:35:48.900]   why you should not expect something like t to the one-third or something even faster than that.\n",
      "[00:35:48.900 --> 00:36:05.900]   Like we said, when standard deviation is roughly the same order,\n",
      "[00:36:05.900 --> 00:36:12.900]   signal then with constant probability r1 bar is less than r2 bar.\n",
      "[00:36:12.900 --> 00:36:17.900]   That means like a probability like if you just do some random draw from arm 1, arm 2,\n",
      "[00:36:17.900 --> 00:36:19.900]   like you put arm 1 for n times,\n",
      "[00:36:19.900 --> 00:36:23.900]   pull arm 2 for n times, then with one-third probability, arm 1,\n",
      "[00:36:23.900 --> 00:36:30.900]   although it has higher mean, will actually have a lower, like, imperial average.\n",
      "[00:36:30.900 --> 00:36:33.900]   So, if you just look at imperial average, you cannot really tell which arm is better,\n",
      "[00:36:33.900 --> 00:36:35.900]   because it can be wrong.\n",
      "[00:36:35.900 --> 00:36:37.900]   The order can be wrong.\n",
      "[00:36:37.900 --> 00:36:42.900]   Yes?\n",
      "[00:36:42.900 --> 00:36:46.900]   >> Sorry, where does that t omega epsilon come from again?\n",
      "[00:36:46.900 --> 00:36:53.900]   >> Oh, you still remember that regret is computed as a summation of t from 1 to capital T.\n",
      "[00:36:53.900 --> 00:36:59.900]   There are i star subtract to the i t, right?\n",
      "[00:36:59.900 --> 00:37:01.900]   This is like the definition.\n",
      "[00:37:01.900 --> 00:37:03.900]   Like basically you look at the optimal arm,\n",
      "[00:37:03.900 --> 00:37:10.900]   the optimal reward you can achieve for t rounds and subtracted by expected reward,\n",
      "[00:37:10.900 --> 00:37:12.900]   you receive by putting your arm.\n",
      "[00:37:12.900 --> 00:37:19.900]   So, what I'm seeing is essentially because you are not very sure which arm is correct.\n",
      "[00:37:19.900 --> 00:37:23.900]   So, on average, in average, you're going to print some epsilon arrow.\n",
      "[00:37:23.900 --> 00:37:25.900]   Like this is like a bit informal argument.\n",
      "[00:37:25.900 --> 00:37:29.900]   >> Like in that expression, I forget.\n",
      "[00:37:29.900 --> 00:37:32.900]   Those expected values of the equal work reach on, like,\n",
      "[00:37:32.900 --> 00:37:36.900]   so the exact e 1/2 was epsilon and 1/2 are those also random variables.\n",
      "[00:37:36.900 --> 00:37:41.900]   >> I think this is a bit informal argument.\n",
      "[00:37:41.900 --> 00:37:44.900]   So, we'll talk about a formal one later, but here we're just saying,\n",
      "[00:37:44.900 --> 00:37:48.900]   I think the reasoning is informal because you cannot make it very rigorous.\n",
      "[00:37:48.900 --> 00:37:54.900]   But basically what I'm saying is once you are not very sure about which arm is the optimal arm,\n",
      "[00:37:54.900 --> 00:37:58.900]   then you're like with one heart probability or certain probability,\n",
      "[00:37:58.900 --> 00:38:00.900]   like you may make it wrong.\n",
      "[00:38:00.900 --> 00:38:05.900]   So, that's why you suffer omega epsilon type regret, like, pre-tuation.\n",
      "[00:38:05.900 --> 00:38:07.900]   Yes?\n",
      "[00:38:07.900 --> 00:38:12.900]   >> Is it possible for regret to have a tail keeper?\n",
      "[00:38:12.900 --> 00:38:13.900]   Is that possible?\n",
      "[00:38:13.900 --> 00:38:20.900]   >> Regret, for this definition, we cannot because whatever we have,\n",
      "[00:38:20.900 --> 00:38:23.900]   the front is like I star.\n",
      "[00:38:23.900 --> 00:38:25.900]   I star is like the optimal.\n",
      "[00:38:25.900 --> 00:38:30.900]   So, I think the regret definition we have here is for stochastic multi-arm bandage\n",
      "[00:38:30.900 --> 00:38:34.900]   where you can actually define an optimal arm and whatever your achieve will\n",
      "[00:38:34.900 --> 00:38:37.900]   will not be better than optimal arm.\n",
      "[00:38:37.900 --> 00:38:41.900]   But there are some other scenario where if you kind of could,\n",
      "[00:38:41.900 --> 00:38:43.900]   you have some adversarial reward.\n",
      "[00:38:43.900 --> 00:38:44.900]   So, this is like a different setting.\n",
      "[00:38:44.900 --> 00:38:47.900]   If you have some adversarial opponent and you're beating with like a\n",
      "[00:38:47.900 --> 00:38:50.900]   best fixed arm in hindsight, in that kind of scenario,\n",
      "[00:38:50.900 --> 00:38:52.900]   regret can be negative.\n",
      "[00:38:52.900 --> 00:38:55.900]   But basically in this class, it's a positive.\n",
      "[00:38:55.900 --> 00:38:56.900]   >> Okay.\n",
      "[00:38:56.900 --> 00:39:02.900]   Then in this context, the optimal arm is like the maximum.\n",
      "[00:39:02.900 --> 00:39:03.900]   >> Yeah.\n",
      "[00:39:03.900 --> 00:39:07.900]   In this case, it's just putting on one.\n",
      "[00:39:07.900 --> 00:39:10.900]   On one has a higher expected reward.\n",
      "[00:39:10.900 --> 00:39:11.900]   >> Okay.\n",
      "[00:39:11.900 --> 00:39:13.900]   >> For every key?\n",
      "[00:39:13.900 --> 00:39:14.900]   >> Yeah.\n",
      "[00:39:14.900 --> 00:39:15.900]   For every key.\n",
      "[00:39:15.900 --> 00:39:22.900]   >> Okay.\n",
      "[00:39:22.900 --> 00:39:29.900]   >> Okay.\n",
      "[00:39:29.900 --> 00:39:30.900]   So, it's very good.\n",
      "[00:39:30.900 --> 00:39:32.900]   You ask a lot of good questions.\n",
      "[00:39:32.900 --> 00:39:36.900]   I guess the reason you have a lot of questions is also because,\n",
      "[00:39:36.900 --> 00:39:38.900]   as I said, this is like an informal argument.\n",
      "[00:39:38.900 --> 00:39:43.900]   So, we will actually talk about two critical parts that is missing\n",
      "[00:39:43.900 --> 00:39:45.900]   this argument.\n",
      "[00:39:45.900 --> 00:39:48.900]   Like, I think this gives a very intuitive understanding because\n",
      "[00:39:48.900 --> 00:39:53.900]   of COT and like why the score T kind of like makes sense.\n",
      "[00:39:53.900 --> 00:39:55.900]   It serves as a sanity check.\n",
      "[00:39:55.900 --> 00:39:58.900]   But on the hand, there are some problems.\n",
      "[00:39:58.900 --> 00:40:02.900]   The first one is this is like a purely, okay.\n",
      "[00:40:02.900 --> 00:40:05.900]   So, problem of this like intuitive argument.\n",
      "[00:40:05.900 --> 00:40:10.900]   The first one, this is like a purely asymptotic.\n",
      "[00:40:10.900 --> 00:40:15.900]   Okay.\n",
      "[00:40:15.900 --> 00:40:24.900]   The reason is like we're using a CLT.\n",
      "[00:40:24.900 --> 00:40:33.900]   CLT requires an go to infinity.\n",
      "[00:40:33.900 --> 00:40:37.900]   So, this does not give any like meaningful result when T is\n",
      "[00:40:37.900 --> 00:40:44.900]   finite or like in that case it's not very satisfactory.\n",
      "[00:40:44.900 --> 00:40:52.900]   So, the second problem of this argument is this argument\n",
      "[00:40:52.900 --> 00:41:15.900]   to somehow assume using the algorithm\n",
      "[00:41:15.900 --> 00:41:30.900]   that is going to check empirical average\n",
      "[00:41:30.900 --> 00:41:36.900]   check and compare empirical average.\n",
      "[00:41:36.900 --> 00:41:42.900]   Well, this is also somehow like a very unsatisfactory assumption.\n",
      "[00:41:42.900 --> 00:41:46.900]   Like for example, the real algorithm may not look at the empirical average.\n",
      "[00:41:46.900 --> 00:41:53.900]   Just look at some other like wild stuff or like maybe I just always guess some number.\n",
      "[00:41:53.900 --> 00:41:59.900]   So, in that kind of scenario like it doesn't really like rule out some like a very wild\n",
      "[00:41:59.900 --> 00:42:02.900]   or maybe some potentially very smart algorithm.\n",
      "[00:42:02.900 --> 00:42:12.900]   So, what do we want as a truly algorithm independent?\n",
      "[00:42:12.900 --> 00:42:41.900]   [ Writing on Board ]\n",
      "[00:42:41.900 --> 00:43:09.900]   [ Writing on Board ]\n",
      "[00:43:09.900 --> 00:43:14.900]   So, I hope this warm up example gives you like enough motivation to now formally consider\n",
      "[00:43:14.900 --> 00:43:18.900]   like how we're going to prove some lower bounds like that.\n",
      "[00:43:18.900 --> 00:43:22.900]   Like it helps you like getting some intuition why square T is like crack order.\n",
      "[00:43:22.900 --> 00:43:28.900]   But we also need a more rigorous mathematical tools to prove some lower bounds like that.\n",
      "[00:43:28.900 --> 00:43:33.900]   So, we kind of like already noticed the formal argument of this is like the key\n",
      "[00:43:33.900 --> 00:43:41.900]   of a loon lower bound is always going to be creating hard problems.\n",
      "[00:43:41.900 --> 00:43:55.900]   [ Writing on Board ]\n",
      "[00:43:55.900 --> 00:43:58.900]   So, the first strategy of creating hard problems is like this.\n",
      "[00:43:58.900 --> 00:44:01.900]   We just create a fixed hard problem.\n",
      "[00:44:01.900 --> 00:44:09.900]   We'll actually notice this is like a very bad strategy.\n",
      "[00:44:09.900 --> 00:44:16.900]   [ Writing on Board ]\n",
      "[00:44:16.900 --> 00:44:18.900]   This is always an incorrect way.\n",
      "[00:44:18.900 --> 00:44:24.900]   The reason is if I create some problem like this I would just have like a cheating algorithm\n",
      "[00:44:24.900 --> 00:44:29.900]   which always output arm one or like always pull arm one like blindly.\n",
      "[00:44:29.900 --> 00:44:33.900]   This algorithm doesn't look at any mean or at all but it's just always pull arm one.\n",
      "[00:44:33.900 --> 00:44:38.900]   And because I have a fixed hard, like fixed hard instance.\n",
      "[00:44:38.900 --> 00:44:41.900]   So, there is always one arm that is the best arm.\n",
      "[00:44:41.900 --> 00:44:48.900]   So, for that problem like if I just always put a best arm and that will always give you like a zero regret\n",
      "[00:44:48.900 --> 00:44:50.900]   that cannot be hard for that algorithm.\n",
      "[00:44:50.900 --> 00:44:52.900]   So, this is not very good.\n",
      "[00:44:52.900 --> 00:44:57.900]   I think the reason not very good of doing like fixed problem is because in the lower bounds\n",
      "[00:44:57.900 --> 00:45:06.900]   we're actually saying for any algorithm there exists some hard instance.\n",
      "[00:45:06.900 --> 00:45:12.900]   There exists some hard problem which kind of when the algorithm operates on this hard problem\n",
      "[00:45:12.900 --> 00:45:14.900]   you actually suffer a lot of regret.\n",
      "[00:45:14.900 --> 00:45:18.900]   So, this existence of hard problem actually comes after the algorithm.\n",
      "[00:45:18.900 --> 00:45:31.900]   We already said we may want to create algorithm specific hard instance.\n",
      "[00:45:31.900 --> 00:45:42.900]   So, this is a second approach which is like basically what we stated in the lower bound\n",
      "[00:45:42.900 --> 00:45:46.900]   statement and that is like the most extreme case you can do.\n",
      "[00:45:46.900 --> 00:46:04.900]   But we already said this also like a very challenging in math.\n",
      "[00:46:04.900 --> 00:46:07.900]   Like we know there are like tons of different algorithms.\n",
      "[00:46:07.900 --> 00:46:13.900]   So, like usually it's very difficult to say like what a specific algorithm and describe\n",
      "[00:46:13.900 --> 00:46:15.900]   the algorithm and do the hard instance for that.\n",
      "[00:46:15.900 --> 00:46:26.900]   So, typically we will do like a third way which kind of will be more expressive than just a fixed\n",
      "[00:46:26.900 --> 00:46:27.900]   hard problem.\n",
      "[00:46:27.900 --> 00:46:34.900]   But also we want to do the difficulty of constructing some algorithm specific hard instance which\n",
      "[00:46:34.900 --> 00:46:35.900]   is like too complicated.\n",
      "[00:46:35.900 --> 00:47:02.900]   So, what we will do is we will usually consider a distribution over a hard problem.\n",
      "[00:47:02.900 --> 00:47:06.900]   So, not just consider one hard problem, we will consider like multiple hard problems.\n",
      "[00:47:06.900 --> 00:47:12.900]   And then the true hard instance we constructed is basically taking an average or taking some\n",
      "[00:47:12.900 --> 00:47:15.900]   distribution over those specific hard instances.\n",
      "[00:47:15.900 --> 00:47:21.900]   So, we are seeing on average this set of problem is like very difficult for our algorithm.\n",
      "[00:47:21.900 --> 00:47:49.900]   So, we will actually do this approach.\n",
      "[00:47:49.900 --> 00:47:56.900]   So, another thing that is difficult to handle is we actually have the random algorithm.\n",
      "[00:47:56.900 --> 00:48:03.900]   It's very flexible.\n",
      "[00:48:03.900 --> 00:48:15.900]   So, this random algorithm can do some arbitrary stuff which is like very difficult to describe\n",
      "[00:48:15.900 --> 00:48:16.900]   and usually to bound.\n",
      "[00:48:16.900 --> 00:48:23.900]   So, it turns out when we add such a formal argument because we kind of lose some power\n",
      "[00:48:23.900 --> 00:48:28.900]   when we no longer do this algorithm specific hard instance, now we are doing a distribution\n",
      "[00:48:28.900 --> 00:48:29.900]   over a hard problem.\n",
      "[00:48:29.900 --> 00:48:35.900]   When we like sacrifice some power of here, we can actually no longer need to look at the\n",
      "[00:48:35.900 --> 00:48:36.900]   random algorithm.\n",
      "[00:48:36.900 --> 00:48:41.900]   We basically can do some reduction argument.\n",
      "[00:48:41.900 --> 00:49:07.900]   So, we say it's sufficient to look at deterministic algorithm on distribution of hard instances.\n",
      "[00:49:07.900 --> 00:49:10.900]   So, we will actually do this reduction argument right now.\n",
      "[00:49:10.900 --> 00:49:15.900]   So, the reduction will basically reduce the randomised free phrase like reduce the power.\n",
      "[00:49:15.900 --> 00:49:19.900]   So, we kind of lose some power when we no longer construct an algorithm specific hard instance.\n",
      "[00:49:19.900 --> 00:49:22.900]   Now, we are doing just a distribution over a hard problem.\n",
      "[00:49:22.900 --> 00:49:27.900]   But when doing that, we can actually for free no longer need to consider the randomised algorithm.\n",
      "[00:49:27.900 --> 00:49:29.900]   We only need to consider deterministic algorithm.\n",
      "[00:49:29.900 --> 00:49:33.900]   So, the formal argument is basically looking like follows.\n",
      "[00:49:33.900 --> 00:49:37.900]   This is basically a very standard way to do the lower bound.\n",
      "[00:49:37.900 --> 00:49:41.900]   So, we start with the lower bound we are trying to prove.\n",
      "[00:49:41.900 --> 00:49:46.900]   So, the lower bound we are trying to prove is we look at minimise over all the possible\n",
      "[00:49:46.900 --> 00:49:47.900]   algorithm.\n",
      "[00:49:47.900 --> 00:49:57.900]   Let's say in some random randomised algorithm.\n",
      "[00:49:57.900 --> 00:50:00.900]   This algorithm A is a randomised algorithm.\n",
      "[00:50:00.900 --> 00:50:22.900]   And we will take a max over, let's call this p, p is the problem, like the multi-ambented problem.\n",
      "[00:50:22.900 --> 00:50:28.900]   And we look at this expected regret.\n",
      "[00:50:28.900 --> 00:50:32.900]   Let's use the regret in terms of regret actually depends on p.\n",
      "[00:50:32.900 --> 00:50:37.900]   So, regret actually depends on what kind of multi-ambented problem you are looking at.\n",
      "[00:50:37.900 --> 00:50:42.900]   And also depends on the algorithm and the total number of iterations you have.\n",
      "[00:50:42.900 --> 00:50:46.900]   So, this is the quantity we are looking at for the lower bounds.\n",
      "[00:50:46.900 --> 00:50:51.900]   That is for any algorithm, we can construct the hardest multi-ambented problem.\n",
      "[00:50:51.900 --> 00:50:56.900]   So, we look at expected regret.\n",
      "[00:50:56.900 --> 00:51:01.900]   So, the first step we already mentioned that formally we will replace this algorithm\n",
      "[00:51:01.900 --> 00:51:06.900]   specific hard instance that is we are taking max of problem after we choosing the algorithm\n",
      "[00:51:06.900 --> 00:51:09.900]   to be some distribution of hard instance.\n",
      "[00:51:09.900 --> 00:51:16.900]   So, we say this is greater or equal to, again, taking the minimum over algorithm in randomised\n",
      "[00:51:16.900 --> 00:51:21.900]   algorithm.\n",
      "[00:51:21.900 --> 00:51:27.900]   But now, instead of doing maximization over the problem, we are taking problem according\n",
      "[00:51:27.900 --> 00:51:31.900]   to some fixed distribution, we will choose later.\n",
      "[00:51:31.900 --> 00:51:33.900]   It will be the hard distribution.\n",
      "[00:51:33.900 --> 00:51:42.900]   And then we just copy the remaining thing.\n",
      "[00:51:42.900 --> 00:51:52.900]   So, this is essentially doing the first step we are talking about.\n",
      "[00:51:52.900 --> 00:51:57.900]   We kind of, we lose some power by no longer considered algorithm specific hard instance.\n",
      "[00:51:57.900 --> 00:52:03.900]   We are now considered a distribution over a hard problem.\n",
      "[00:52:03.900 --> 00:52:16.900]   Now, we will do the second step that will reduce the randomised algorithm to deterministic algorithm.\n",
      "[00:52:16.900 --> 00:52:23.900]   So, first benefit we observed by doing this expectation now instead of doing the max is\n",
      "[00:52:23.900 --> 00:52:29.900]   we can actually change the expectation order because it doesn't matter if we are doing\n",
      "[00:52:29.900 --> 00:52:35.900]   like expectation, we can first take expectation of internal randomness in algorithm A and then\n",
      "[00:52:35.900 --> 00:52:41.900]   take expectation over the problem P. So, this is precisely equal to, we are doing minimization\n",
      "[00:52:41.900 --> 00:52:47.900]   of algorithm A, random algorithm.\n",
      "[00:52:47.900 --> 00:52:55.900]   And we first take over internal randomness inside algorithm A and then we take expectation\n",
      "[00:52:55.900 --> 00:53:09.900]   over this problem P sample from distribution D, the regret.\n",
      "[00:53:15.900 --> 00:53:42.900]   This is the distribution of hard problems.\n",
      "[00:53:42.900 --> 00:53:51.900]   So, the final step involve a bit of argument of the randomised algorithm.\n",
      "[00:53:51.900 --> 00:53:54.900]   So, what is the randomised algorithm?\n",
      "[00:53:54.900 --> 00:54:09.900]   Randomised algorithm essentially is, you can view this like a collection of deterministic\n",
      "[00:54:09.900 --> 00:54:18.900]   algorithms. So, basically you can view randomised algorithm that is A\n",
      "[00:54:18.900 --> 00:54:43.900]   to take some input of some internal randomness, internal random coin.\n",
      "[00:54:43.900 --> 00:54:47.900]   So, we can think this random algorithm can be always similar to the following way that\n",
      "[00:54:47.900 --> 00:54:55.900]   this, I have an algorithm and this algorithm depends on some internal randomness random bits.\n",
      "[00:54:55.900 --> 00:54:59.900]   So, if you're given some, like this random bits, you already sample this random bits,\n",
      "[00:54:59.900 --> 00:55:02.900]   this random bits can be like infinite lengths.\n",
      "[00:55:02.900 --> 00:55:06.900]   If you already sample this random bits, then the algorithm just becomes deterministic.\n",
      "[00:55:06.900 --> 00:55:11.900]   And the algorithm is random because the random bits is random.\n",
      "[00:55:11.900 --> 00:55:16.900]   So, in that case, if I'm minimizing over some random algorithm, then I'm taking expectation\n",
      "[00:55:16.900 --> 00:55:30.900]   over random algorithm. Essentially, I'm doing minimisation of some algorithm of random bits.\n",
      "[00:55:30.900 --> 00:55:45.900]   And then I'm taking expectation over R according to some distribution, some dt tutor or something like that.\n",
      "[00:55:45.900 --> 00:55:54.900]   And whatever the remaining thing here.\n",
      "[00:55:54.900 --> 00:56:02.900]   We know the minimiser is always achieved at one particular R because if you're doing R\n",
      "[00:56:02.900 --> 00:56:07.900]   according to some distribution and you're doing the minimiser over the random action,\n",
      "[00:56:07.900 --> 00:56:13.900]   so a random algorithm, so the minimiser is actually achieved at one particular R.\n",
      "[00:56:13.900 --> 00:56:18.900]   So, that is basically saying the minimiser is achieved at a deterministic algorithm.\n",
      "[00:56:18.900 --> 00:56:24.900]   So, without a lot of generality, say this is actually equal to minimiser over algorithm\n",
      "[00:56:24.900 --> 00:56:49.900]   and a deterministic algorithm.\n",
      "[00:56:49.900 --> 00:56:56.900]   Any questions about this? Yes.\n",
      "[00:56:56.900 --> 00:57:00.900]   So, you could take a random algorithm and then you set the R that becomes deterministic.\n",
      "[00:57:00.900 --> 00:57:07.900]   I was wondering, should that not be a greater than or equal to, instead of an equal or?\n",
      "[00:57:07.900 --> 00:57:11.900]   I feel like the minimity of the application should be greater than the overall.\n",
      "[00:57:11.900 --> 00:57:17.900]   Overall, it's greater or equal, but now the question is like now because this is like an expectation\n",
      "[00:57:17.900 --> 00:57:22.900]   like we say we're doing the minimiser of R, so it's like greater or equal.\n",
      "[00:57:22.900 --> 00:57:28.900]   But because on the outside, you're actually minimizing over random algorithm.\n",
      "[00:57:28.900 --> 00:57:33.900]   So, if it's greater or equal, then that means your random algorithm is not the best algorithm.\n",
      "[00:57:33.900 --> 00:57:38.900]   So, the best random algorithm basically will just focus on this particular R.\n",
      "[00:57:38.900 --> 00:57:46.900]   So, that's why it's exactly equal.\n",
      "[00:57:46.900 --> 00:57:52.900]   I think what your set is correct in the sense.\n",
      "[00:57:52.900 --> 00:58:09.900]   Let's just be a bit more detailed.\n",
      "[00:58:09.900 --> 00:58:23.900]   So, we're saying this is like greater or equal to minimum of algorithm A, random algorithm A,\n",
      "[00:58:23.900 --> 00:58:44.900]   and then we also do the minimum of R of this everything and this is equal to minimum of deterministic algorithm.\n",
      "[00:58:44.900 --> 00:58:54.900]   This is in general a crash argument, but basically what I'm saying is because you're also doing a minimization over the random algorithm,\n",
      "[00:58:54.900 --> 00:59:03.900]   so basically the best algorithm you will do is basically focusing on one particular R that will actually give you a better algorithm.\n",
      "[00:59:03.900 --> 00:59:11.900]   In terms of if you have an algorithm that will spend some time on some worse R, then that's not an optimal algorithm.\n",
      "[00:59:11.900 --> 00:59:17.900]   So, in terms of the minimizing over the random algorithm, this is actually equal.\n",
      "[00:59:17.900 --> 00:59:22.900]   This is actually equal instead of greater or equal, but if you're not happy, you're just a greater equal.\n",
      "[00:59:22.900 --> 00:59:26.900]   That's also fine.\n",
      "[00:59:26.900 --> 00:59:37.900]   So, it's just a copy this thing.\n",
      "[00:59:37.900 --> 00:59:41.900]   Yes?\n",
      "[00:59:41.900 --> 00:59:44.900]   Why is P here?\n",
      "[00:59:44.900 --> 00:59:51.900]   P is the hard instance, is the MAB problem. Is it a multi-armed binary problem?\n",
      "[00:59:51.900 --> 00:59:57.900]   Okay, those are hard instance over distribution.\n",
      "[00:59:57.900 --> 01:00:09.900]   No, you're saying that the deterministic algorithm that just could probably be one of the maximum R or whatever is the minimum random algorithm.\n",
      "[01:00:09.900 --> 01:00:26.900]   Right, right, exactly.\n",
      "[01:00:26.900 --> 01:00:33.900]   Do we define regret? Do we consider the expectations that is of three words, right?\n",
      "[01:00:33.900 --> 01:00:34.900]   Yes.\n",
      "[01:00:34.900 --> 01:00:37.900]   Then how do we count the shape of the distribution?\n",
      "[01:00:37.900 --> 01:00:40.900]   How do we count what?\n",
      "[01:00:40.900 --> 01:00:44.900]   The shape of the distribution. Do we care about the shape of the distribution?\n",
      "[01:00:44.900 --> 01:00:51.900]   The shape of the distribution, you're talking about the reward distribution.\n",
      "[01:00:51.900 --> 01:00:58.900]   I think we don't care about the reward distribution, but when we talk about this multi-armed binary problem, it's not just the reward distribution.\n",
      "[01:00:58.900 --> 01:01:04.900]   It can also be like which arm has a higher mean and what is the value of those mean? Those are different.\n",
      "[01:01:04.900 --> 01:01:11.900]   For example, I can have one problem that the first arm, the mean is like one, second arm is a mean is zero.\n",
      "[01:01:11.900 --> 01:01:16.900]   Then I have another multi-armed binary problem. The first arm mean is zero, the second arm mean is one.\n",
      "[01:01:16.900 --> 01:01:20.900]   We don't care about details of those pieces.\n",
      "[01:01:20.900 --> 01:01:27.900]   We can include that in this multi-armed binary problem, but it turns out in the proof that it doesn't matter much.\n",
      "[01:01:27.900 --> 01:01:33.900]   What we are caring about is the macro complete picture.\n",
      "[01:01:33.900 --> 01:01:41.900]   How we set up the mean and how we set up which arm is the best arm, that kind of stuff.\n",
      "[01:01:41.900 --> 01:02:03.900]   Basically, the reduction says now we can, in conclusion,\n",
      "[01:02:03.900 --> 01:02:25.900]   what we need to do is we construct a distribution of problem that is hard\n",
      "[01:02:25.900 --> 01:02:36.900]   for the deterministic algorithm.\n",
      "[01:02:36.900 --> 01:02:45.900]   This is what we achieved by this reduction. Basically, we start from the very beginning from the definition of the lower bounds we have.\n",
      "[01:02:45.900 --> 01:02:54.900]   Now, we only need to look at the deterministic algorithm and we are looking at the distribution over hard instance.\n",
      "[01:02:54.900 --> 01:03:12.900]   Once we know that, we will now be able to construct the hard instance.\n",
      "[01:03:12.900 --> 01:03:27.900]   Our construction is basically very similar to this tool-bended problem. It's just a slightly trick of this problem.\n",
      "[01:03:27.900 --> 01:03:48.900]   Our construction is that we have a different arm than a lot of tool-arm only.\n",
      "[01:03:48.900 --> 01:03:57.900]   Again, the reward of each arm is a Bernoulli distribution.\n",
      "[01:03:57.900 --> 01:04:10.900]   Every arm is one-half, only one particular arm A.\n",
      "[01:04:10.900 --> 01:04:21.900]   It's like a particular special arm A is one-half plus epsilon, where everyone else is like one-half.\n",
      "[01:04:21.900 --> 01:04:34.900]   This is like a special arm.\n",
      "[01:04:34.900 --> 01:04:43.900]   We already mentioned that we cannot construct one particular hard instance because there will be a cheating algorithm that always blindly put arm A.\n",
      "[01:04:43.900 --> 01:04:48.900]   That will essentially be this problem. This problem will not be hard for this algorithm.\n",
      "[01:04:48.900 --> 01:04:52.900]   We need to build a distribution over hard instance.\n",
      "[01:04:52.900 --> 01:04:56.900]   The idea to build a distribution over hard instance is very simple.\n",
      "[01:04:56.900 --> 01:05:03.900]   Essentially, we will sample the special arm.\n",
      "[01:05:03.900 --> 01:05:21.900]   Every problem will only have one special arm, but we will sample this special arm uniformly\n",
      "[01:05:21.900 --> 01:05:43.900]   from A arm.\n",
      "[01:05:43.900 --> 01:05:50.900]   What we essentially mean is that we are doing a uniform distribution.\n",
      "[01:05:50.900 --> 01:06:10.900]   The hard distribution is a uniform distribution over MAB with different special arm.\n",
      "[01:06:10.900 --> 01:06:31.900]   We have such problems, we have such problems.\n",
      "[01:06:31.900 --> 01:06:40.900]   I have a multi-ambentative problem with the first arm as my special arm, then that is one problem, and then I have a second arm as my special arm that I have a second problem.\n",
      "[01:06:40.900 --> 01:06:55.900]   In total, I have A is such a problem, and eventually this distribution is just a uniform over all such problems.\n",
      "[01:06:55.900 --> 01:07:02.900]   In doing this way, essentially we hide a special arm because this special arm is uniformly sampled from all different possible arms.\n",
      "[01:07:02.900 --> 01:07:05.900]   I actually have no idea which arm it is.\n",
      "[01:07:05.900 --> 01:07:21.900]   I can no longer have some cheating algorithm just blindly put some arm in expectation that kind of cheating algorithm will actually perform very bad.\n",
      "[01:07:21.900 --> 01:07:29.900]   Any problem about this construction? Construction is quite simple.\n",
      "[01:07:29.900 --> 01:07:39.900]   Before we talk a little bit more about how we are going to use this construction to prove the formal theorem, let's introduce a bit of notation.\n",
      "[01:07:39.900 --> 01:08:01.900]   We say expectation star. We use this notation as we take an expectation with respect to the random draw of special arm.\n",
      "[01:08:01.900 --> 01:08:09.900]   Essentially this is an expectation over some matter problem. We are taking an expectation over some of this special arm which one it is.\n",
      "[01:08:09.900 --> 01:08:14.900]   Then we will use this A notation and E0 notation.\n",
      "[01:08:14.900 --> 01:08:40.900]   So A notation is an expectation regarding to the reward if arm A is the special arm.\n",
      "[01:08:40.900 --> 01:08:47.900]   If arm A is special arm then the reward sequence is probably look like.\n",
      "[01:08:47.900 --> 01:08:50.900]   I am taking an expectation over that.\n",
      "[01:08:50.900 --> 01:09:00.900]   Finally, we will do E0 as a reference with respect to, again, also an expectation regarding to all the reward sequence.\n",
      "[01:09:00.900 --> 01:09:14.900]   If there is no special arm,\n",
      "[01:09:14.900 --> 01:09:24.900]   that is every arm has a 1/2 as a mean reward.\n",
      "[01:09:24.900 --> 01:09:50.900]   We will use a short notation. We call R1 to T.\n",
      "[01:09:50.900 --> 01:09:57.900]   R1 to T is actually a collection of all the important information up to T's round.\n",
      "[01:09:57.900 --> 01:10:01.900]   Basically the word whatever everything environment provides you.\n",
      "[01:10:01.900 --> 01:10:08.900]   That is the stochastic reward you receive at first round by pulling arm A1.\n",
      "[01:10:08.900 --> 01:10:32.900]   This is like the, let's say Rk Ak until this is the stochastic reward.\n",
      "[01:10:32.900 --> 01:11:01.900]   Receive at the T's round for arm At which is the arm, the arm learner pull at the T's round.\n",
      "[01:11:01.900 --> 01:11:26.900]   Essentially this is the entire feedback environment that is the learner will pull A1, A2, and A2, and then they got a stochastic reward that is R1, R2, and R2.\n",
      "[01:11:26.900 --> 01:11:33.900]   So in shortage we will just denote everything as R1 to R2.\n",
      "[01:11:33.900 --> 01:11:53.900]   When we write this notation of R1 to R2, we really mean this R1, A1, RTA, they are taking on different arms.\n",
      "[01:11:53.900 --> 01:12:08.900]   So given this, we can now basically start by looking at how we are going to formally prove the theorem 2.\n",
      "[01:12:08.900 --> 01:12:16.900]   So we look at E* of regret.\n",
      "[01:12:16.900 --> 01:12:22.900]   First we also need to take an expectation over the meta problem.\n",
      "[01:12:22.900 --> 01:12:29.900]   We even need to take a random draw of special arm, then in each special arm we need to take an expectation over the reward we receive.\n",
      "[01:12:29.900 --> 01:12:40.900]   And this is equal to 1 over A summation A in 1 to A, EA.\n",
      "[01:12:40.900 --> 01:12:58.900]   So essentially we are saying E* is equal to this 1 over A because we essentially define this E* to be an average distribution of a multi-ambulator with one special arm.\n",
      "[01:12:58.900 --> 01:13:05.900]   Then we can expand the regret in terms of first the total reward are going to achieve it by the optimal arm.\n",
      "[01:13:05.900 --> 01:13:16.900]   So no matter which problem in sense is the optimal arm, the mean reward of optimal arm is always 1/2 plus epsilon.\n",
      "[01:13:16.900 --> 01:13:27.900]   So this is a total value so we can achieve it by putting the optimal arm.\n",
      "[01:13:27.900 --> 01:13:50.900]   And the value of asian is going to receive is the following.\n",
      "[01:13:50.900 --> 01:13:55.900]   Subtracted by summation T.\n",
      "[01:13:55.900 --> 01:14:00.900]   So the asian is going to receive is always first we have 1/2.\n",
      "[01:14:00.900 --> 01:14:05.900]   And then sometimes we are going to achieve epsilon, sometimes we will not achieve epsilon by the learner.\n",
      "[01:14:05.900 --> 01:14:14.900]   So the only times that we will achieve epsilon is we actually put a right arm where we have an indicator function.\n",
      "[01:14:14.900 --> 01:14:32.900]   But if the A*t, the arm we put at a time t is actually equal to A which in this case is the optimal arm.\n",
      "[01:14:32.900 --> 01:14:44.900]   So we can do some simplification over this formula that we have 1/A, summation A from 1 to capital A, E/A.\n",
      "[01:14:44.900 --> 01:14:50.900]   What do we have inside? First we have like t/2, t/2 cancel out.\n",
      "[01:14:50.900 --> 01:14:56.900]   So t/2 is gone. What do we have is we are always going to times epsilon.\n",
      "[01:14:56.900 --> 01:15:01.900]   And the first term times epsilon is t.\n",
      "[01:15:01.900 --> 01:15:07.900]   And the second term times epsilon is the summation over t where indicator A*t is equal to A.\n",
      "[01:15:07.900 --> 01:15:16.900]   And that will be equal to NA where NA is the number of times we put action A is poor.\n",
      "[01:15:16.900 --> 01:15:45.900]   So, finally we can do some rearrangement because we know the first term epsilon t actually doesn't really depends on this A average over A.\n",
      "[01:15:45.900 --> 01:15:48.900]   It doesn't really matter. And this thing only affects the second term.\n",
      "[01:15:48.900 --> 01:16:13.900]   So we can put inside and we actually equal this equal to epsilon times t subtracted by 1/A, summation A from 1 to capital A, E/A, NA.\n",
      "[01:16:13.900 --> 01:16:42.900]   So, we know the key quantity now in order to lower bound the regret, we need to look at this eA and A.\n",
      "[01:16:42.900 --> 01:16:55.900]   That is if this arm A is a special arm, how many times I am going to pull this A arm.\n",
      "[01:16:55.900 --> 01:17:09.900]   So, we are running out of time. In the next lecture our key idea is we will actually compare this with e0 and A.\n",
      "[01:17:09.900 --> 01:17:14.900]   That is in the case of no special arm, how many times I am going to pull arm A.\n",
      "[01:17:14.900 --> 01:17:20.900]   So, because this epsilon, like the whole idea is again, this epsilon is chosen to be very, very small.\n",
      "[01:17:20.900 --> 01:17:29.900]   So, if we only observe like t rounds of reward, it's very difficult to distinguish this problem versus the zero problem.\n",
      "[01:17:29.900 --> 01:17:41.900]   So, in that case, we will actually say those two are pretty close. And when these two are pretty close, then basically we will estimate some lower bounds.\n",
      "[01:17:41.900 --> 01:17:46.900]   Let's end of this lecture. See you on Thursday.\n",
      "[01:17:46.900 --> 01:17:49.940]   [MUSIC PLAYING]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/wR81RH-GI9U.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/wR81RH-GI9U.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/wR81RH-GI9U.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/wR81RH-GI9U.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1257.68 ms\n",
      "whisper_print_timings:     fallbacks =   2 p /   0 h\n",
      "whisper_print_timings:      mel time =  2643.00 ms\n",
      "whisper_print_timings:   sample time = 20907.39 ms / 54067 runs (    0.39 ms per run)\n",
      "whisper_print_timings:   encode time =   339.27 ms /   191 runs (    1.78 ms per run)\n",
      "whisper_print_timings:   decode time =   891.51 ms /   485 runs (    1.84 ms per run)\n",
      "whisper_print_timings:   batchd time = 27601.03 ms / 52619 runs (    0.52 ms per run)\n",
      "whisper_print_timings:   prompt time =  9677.86 ms / 42987 runs (    0.23 ms per run)\n",
      "whisper_print_timings:    total time = 63750.24 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=VU73LRk8Zjw started\n",
      "VU73LRk8Zjw\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/VU73LRk8Zjw.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/VU73LRk8Zjw.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:18:44.77, start: 0.000000, bitrate: 236 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 136 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/VU73LRk8Zjw.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  147649kB time=01:18:44.77 bitrate= 256.0kbits/s speed=1.4e+03x     \n",
      "video:0kB audio:147649kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000052%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/VU73LRk8Zjw.wav' (75596371 samples, 4724.8 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.080]   [MUSIC PLAYING]\n",
      "[00:00:03.080 --> 00:00:06.080]   In the last lecture, we talk about exploration\n",
      "[00:00:06.080 --> 00:00:07.120]   in Mount Yum Bandit.\n",
      "[00:00:07.120 --> 00:00:17.960]   And specifically, we talk about two alwism.\n",
      "[00:00:17.960 --> 00:00:21.040]   One is epsilon greedy or epsilon first alwism.\n",
      "[00:00:21.040 --> 00:00:28.160]   The other one is the upper confidence bound.\n",
      "[00:00:28.160 --> 00:00:30.400]   We're seeing the epsilon greedy algorithm,\n",
      "[00:00:30.400 --> 00:00:34.400]   which just naively try random action\n",
      "[00:00:34.400 --> 00:00:35.960]   at epsilon fraction of the time.\n",
      "[00:00:35.960 --> 00:00:39.280]   Well, upper confidence bound combines both exploration\n",
      "[00:00:39.280 --> 00:00:42.560]   and exploitation by computing this upper confidence bound,\n",
      "[00:00:42.560 --> 00:00:44.760]   which is mean plus some bonus.\n",
      "[00:00:44.760 --> 00:00:47.920]   And by looking at that, we have a better trade out\n",
      "[00:00:47.920 --> 00:00:50.600]   between exploration versus exploitation.\n",
      "[00:00:50.600 --> 00:00:52.960]   So it turns out in the epsilon greedy,\n",
      "[00:00:52.960 --> 00:01:01.360]   we get a regret, which is T2 over 4, something like that.\n",
      "[00:01:01.360 --> 00:01:04.560]   And UCB, we get square root here, right?\n",
      "[00:01:04.560 --> 00:01:09.480]   And this is in the Mount Yum Bandit setting.\n",
      "[00:01:09.480 --> 00:01:15.440]   And today, we will talk about the reinforced learning\n",
      "[00:01:15.440 --> 00:01:17.200]   scenario.\n",
      "[00:01:17.200 --> 00:01:19.080]   So in the reinforced learning scenario,\n",
      "[00:01:19.080 --> 00:01:26.400]   we notice the exploration is much challenging\n",
      "[00:01:26.400 --> 00:01:29.320]   than the Mount Yum Bandit.\n",
      "[00:01:29.320 --> 00:01:32.320]   Specifically, we talk about we have this counter example,\n",
      "[00:01:32.320 --> 00:01:35.200]   which is a combinatorial lock, where\n",
      "[00:01:35.200 --> 00:01:38.440]   if we do random exploration or do epsilon greedy,\n",
      "[00:01:38.440 --> 00:01:44.440]   we actually pay something like A to the H, at least exponential\n",
      "[00:01:44.440 --> 00:01:49.160]   in horizon to first identify what is the optimal R.\n",
      "[00:01:49.160 --> 00:01:52.480]   And in this case, you can easily prove the regret\n",
      "[00:01:52.480 --> 00:01:58.080]   would be something like either linear in T, until you get--\n",
      "[00:01:58.080 --> 00:02:03.040]   the T is greater than A to the H, something like that,\n",
      "[00:02:03.040 --> 00:02:11.840]   A to the omega H.\n",
      "[00:02:11.840 --> 00:02:15.240]   So this is a really bad--\n",
      "[00:02:15.240 --> 00:02:17.440]   so in terms of in the Mount Yum Bandit,\n",
      "[00:02:17.440 --> 00:02:19.040]   the only problem of epsilon greedy\n",
      "[00:02:19.040 --> 00:02:22.800]   is just we get a slightly worse regret in terms of rate.\n",
      "[00:02:22.800 --> 00:02:25.760]   But in the reinforced learning scenario,\n",
      "[00:02:25.760 --> 00:02:27.960]   we're actually paying something exponential in horizon.\n",
      "[00:02:27.960 --> 00:02:29.920]   So this is not horrible.\n",
      "[00:02:29.920 --> 00:02:32.400]   So basically, this lecture, we'll talk about how\n",
      "[00:02:32.400 --> 00:02:35.160]   we're going to use the better idea in exploration\n",
      "[00:02:35.160 --> 00:02:38.680]   to incorporate that to reinforce the idea.\n",
      "[00:02:38.680 --> 00:02:42.400]   So basically, we just incorporate a UCB into the reinforcement\n",
      "[00:02:42.400 --> 00:02:44.720]   learning, and it will get something like--\n",
      "[00:02:44.720 --> 00:02:47.480]   again, we will get square root T, and the polynomial\n",
      "[00:02:47.480 --> 00:02:55.920]   in H. So this is the algorithm we will talk about.\n",
      "[00:02:55.920 --> 00:03:03.400]   We will just by combining value iteration, which\n",
      "[00:03:03.400 --> 00:03:06.720]   is the algorithm we use for reinforcement learning with the UCB.\n",
      "[00:03:06.720 --> 00:03:13.200]   So let's first talk about the algorithm.\n",
      "[00:03:13.200 --> 00:03:19.560]   So the whole idea is basically very similar to the UCB algorithm\n",
      "[00:03:19.560 --> 00:03:21.080]   in the Mount Yum Bandit setting.\n",
      "[00:03:21.080 --> 00:03:25.120]   Instead of just looking at the mean, we also add a bonus.\n",
      "[00:03:25.120 --> 00:03:26.520]   So here it's similar.\n",
      "[00:03:26.520 --> 00:03:30.080]   We also do the bonus type of algorithm.\n",
      "[00:03:30.080 --> 00:03:32.840]   So the algorithm is called a UCBVI algorithm.\n",
      "[00:03:32.840 --> 00:03:40.000]   It's upper confidence bound of value iteration algorithm,\n",
      "[00:03:40.000 --> 00:03:46.120]   which is introduced in Azzard Hall, 2017.\n",
      "[00:03:46.120 --> 00:04:01.160]   So the algorithm we first initialize with some amplitude\n",
      "[00:04:01.160 --> 00:04:01.980]   data set.\n",
      "[00:04:01.980 --> 00:04:07.120]   Let's call this d as data set.\n",
      "[00:04:07.120 --> 00:04:09.160]   And let's say it's empty.\n",
      "[00:04:09.160 --> 00:04:11.440]   Because when we begin the algorithm,\n",
      "[00:04:11.440 --> 00:04:12.520]   we don't have any data.\n",
      "[00:04:12.520 --> 00:04:14.640]   We have an observer energy directory.\n",
      "[00:04:14.640 --> 00:04:19.320]   And we initialize QHSA.\n",
      "[00:04:19.320 --> 00:04:21.200]   I think this is important, because we always\n",
      "[00:04:21.200 --> 00:04:24.920]   want to maintain the upper bounds to the optimal value\n",
      "[00:04:24.920 --> 00:04:25.800]   function.\n",
      "[00:04:25.800 --> 00:04:30.760]   So we just initialize it to be H. Because H is the upper bound,\n",
      "[00:04:30.760 --> 00:04:32.280]   is always an upper bound on the value.\n",
      "[00:04:32.280 --> 00:04:45.320]   Just to recall, our normalization conditions,\n",
      "[00:04:45.320 --> 00:04:48.320]   we assume the reward in each step is 0 to 1.\n",
      "[00:04:48.320 --> 00:04:52.720]   So the maximum value you achieve is capital H.\n",
      "[00:04:52.720 --> 00:04:56.800]   If specifically, it should be capital H minus little H.\n",
      "[00:04:56.800 --> 00:05:07.200]   But for here, let's just say capital H. For all S and H.\n",
      "[00:05:07.200 --> 00:05:14.080]   We will also initialize V H plus 1, capital H plus 1, S.\n",
      "[00:05:14.080 --> 00:05:18.440]   This thing is the value at H plus 1 step.\n",
      "[00:05:18.440 --> 00:05:21.320]   We know there is no reward received after H plus 1\n",
      "[00:05:21.320 --> 00:05:34.880]   step. So this is always equal to 0 for any S.\n",
      "[00:05:34.880 --> 00:05:37.400]   So the main algorithm consists of the following two parts.\n",
      "[00:05:37.400 --> 00:05:42.240]   Again, the exploration setting, we\n",
      "[00:05:42.240 --> 00:05:50.840]   need to say four episodes, k from 1 to 2 capital k.\n",
      "[00:05:50.840 --> 00:06:03.200]   So in each episode, we're doing two major steps.\n",
      "[00:06:03.200 --> 00:06:06.760]   The first step is actually we're calculating\n",
      "[00:06:06.760 --> 00:06:08.880]   the ucb of value.\n",
      "[00:06:08.880 --> 00:06:12.760]   The upper confidence bound of a value at each step.\n",
      "[00:06:12.760 --> 00:06:26.500]   So this is about compute ucb of values.\n",
      "[00:06:26.500 --> 00:06:35.760]   The idea is basically just doing vector iteration plus bonus.\n",
      "[00:06:35.760 --> 00:06:38.440]   So again, when we do vector iteration,\n",
      "[00:06:38.440 --> 00:06:42.000]   we start from the very last step to the first step.\n",
      "[00:06:42.000 --> 00:06:45.200]   We start from capital H to H minus 1 and [INAUDIBLE]\n",
      "[00:06:45.200 --> 00:07:03.520]   So what do we do is we first check if this state action pair,\n",
      "[00:07:03.520 --> 00:07:07.520]   if this sah is already in this data set d.\n",
      "[00:07:07.520 --> 00:07:16.880]   So in a data set, do we have ever observed an sa pair at H stuff?\n",
      "[00:07:16.880 --> 00:07:24.800]   If so, we can compute this empirical estimate.\n",
      "[00:07:24.800 --> 00:07:28.120]   We again use the empirical average as my estimate.\n",
      "[00:07:35.400 --> 00:07:39.480]   We use the ratio, like how many times\n",
      "[00:07:39.480 --> 00:07:45.560]   we have observed sa as prime, pair in the data set,\n",
      "[00:07:45.560 --> 00:07:50.280]   divided by how many times I have observed sah at H stuff.\n",
      "[00:07:50.280 --> 00:07:52.960]   Those are counts in data set.\n",
      "[00:08:05.160 --> 00:08:08.600]   So the only difference is in the similarity setting or generative\n",
      "[00:08:08.600 --> 00:08:12.120]   model setting, we sweep over all the state action pair.\n",
      "[00:08:12.120 --> 00:08:16.480]   So we can guarantee this is some fixed number, an sa is fixed number.\n",
      "[00:08:16.480 --> 00:08:18.160]   Now, because we're doing exploration,\n",
      "[00:08:18.160 --> 00:08:21.760]   sometimes we might never be able to visit some state action.\n",
      "[00:08:21.760 --> 00:08:27.720]   So that one will be 0, and we will treat it in some other way.\n",
      "[00:08:27.720 --> 00:08:30.360]   So as long as this is not 0, we can use the empirical average\n",
      "[00:08:30.360 --> 00:08:32.520]   to estimate the transition.\n",
      "[00:08:32.520 --> 00:08:39.040]   And then all we do is we just do the valetration update,\n",
      "[00:08:39.040 --> 00:08:39.880]   slightly modifies.\n",
      "[00:08:39.880 --> 00:08:48.280]   We first do some truncation so that we guarantee the valetration.\n",
      "[00:08:48.280 --> 00:08:51.360]   The value we got from valetration will never be greater than H,\n",
      "[00:08:51.360 --> 00:08:54.680]   which is our upper bounds, always an upper bounds.\n",
      "[00:08:54.680 --> 00:08:57.280]   And then we just use the Bellman update\n",
      "[00:08:57.280 --> 00:09:04.080]   to do Bellman update and replace the transition p by p hat.\n",
      "[00:09:04.080 --> 00:09:16.040]   And this is a standard valetration update.\n",
      "[00:09:16.040 --> 00:09:20.280]   So our modification is-- in addition to doing this,\n",
      "[00:09:20.280 --> 00:09:23.400]   we also add a bonus.\n",
      "[00:09:23.400 --> 00:09:29.800]   And the bonus will be only as a function of NHSA\n",
      "[00:09:29.800 --> 00:09:33.320]   as how many times we have observed a sa at H staff.\n",
      "[00:09:33.320 --> 00:09:48.160]   So basically, the step is we just do valetration,\n",
      "[00:09:48.160 --> 00:09:51.840]   but we just disappear replaced by empirical average--\n",
      "[00:09:51.840 --> 00:09:55.080]   empirical estimates-- and adding some bonus.\n",
      "[00:09:55.080 --> 00:09:57.080]   And also doing some truncation.\n",
      "[00:09:57.080 --> 00:10:01.520]   And in the case of if we have an observed sa in the data set,\n",
      "[00:10:01.520 --> 00:10:05.080]   basically, we won't do this step, and we will just--\n",
      "[00:10:05.080 --> 00:10:08.080]   the Q value will be remain as the initialized Q value,\n",
      "[00:10:08.080 --> 00:10:10.560]   which is capital H. So it's still well defined.\n",
      "[00:10:10.560 --> 00:10:11.600]   Q value is well defined.\n",
      "[00:10:11.600 --> 00:10:18.680]   It just is not according to this update, but it's just H.\n",
      "[00:10:18.680 --> 00:10:36.880]   So the next step is-- this is saying for all sa.\n",
      "[00:10:46.880 --> 00:10:50.000]   So this is a Belma update for Q value.\n",
      "[00:10:50.000 --> 00:10:55.440]   So next, we will do Belma update for V value,\n",
      "[00:10:55.440 --> 00:10:58.960]   which is essentially the same for all sas.\n",
      "[00:10:58.960 --> 00:11:12.600]   We do this VHS equal to max A in A, QHS A.\n",
      "[00:11:12.600 --> 00:11:16.040]   So again, for any sa has already been observed,\n",
      "[00:11:16.040 --> 00:11:17.880]   the Q value is defined in here.\n",
      "[00:11:17.880 --> 00:11:20.160]   And if the sa has not been observed yet,\n",
      "[00:11:20.160 --> 00:11:22.680]   the Q value will be the initialized Q value.\n",
      "[00:11:22.680 --> 00:11:26.600]   So this V value is, again, also a well defined thing.\n",
      "[00:11:26.600 --> 00:11:29.480]   So basically, this entire part is\n",
      "[00:11:29.480 --> 00:11:32.600]   how we're going to compute the UCB of values.\n",
      "[00:11:32.600 --> 00:11:33.960]   The height of idea is very simple.\n",
      "[00:11:33.960 --> 00:11:37.800]   It's just we add a bonus to the Q value updates.\n",
      "[00:11:37.800 --> 00:11:42.640]   And there are other variants for it.\n",
      "[00:11:42.640 --> 00:11:46.860]   [INAUDIBLE]\n",
      "[00:11:46.860 --> 00:11:50.720]   Why do we want to have this?\n",
      "[00:11:50.720 --> 00:11:54.480]   Because sometimes, in the very big--\n",
      "[00:11:54.480 --> 00:11:57.000]   we'll see later this bonus is something\n",
      "[00:11:57.000 --> 00:12:00.640]   like 1 over square root of n.\n",
      "[00:12:00.640 --> 00:12:07.200]   And times C times H square and log\n",
      "[00:12:07.200 --> 00:12:10.400]   term, something like that.\n",
      "[00:12:10.400 --> 00:12:15.840]   So in some cases, when n is not very large--\n",
      "[00:12:15.840 --> 00:12:19.320]   for example, if we only observe one or two values,\n",
      "[00:12:19.320 --> 00:12:21.880]   we might add a lot of bonus.\n",
      "[00:12:21.880 --> 00:12:26.480]   And in that case, my Q value will be very large.\n",
      "[00:12:26.480 --> 00:12:31.000]   And Q value very large goes down to the next step\n",
      "[00:12:31.000 --> 00:12:35.640]   where we talk about how we construct this bonus.\n",
      "[00:12:35.640 --> 00:12:38.000]   I think the main reason we construct this bonus\n",
      "[00:12:38.000 --> 00:12:40.480]   is we want this to be an upper bound.\n",
      "[00:12:40.480 --> 00:12:43.640]   And if Q value in intermediate Q value become very large,\n",
      "[00:12:43.640 --> 00:12:46.640]   and then there will hurt the entire proof on how\n",
      "[00:12:46.640 --> 00:12:47.840]   it becomes an upper bound.\n",
      "[00:12:47.840 --> 00:12:49.720]   Because essentially, we want to do some concentration.\n",
      "[00:12:49.720 --> 00:12:51.920]   So we will see the reason later.\n",
      "[00:12:51.920 --> 00:12:56.440]   But mostly, just to make sure this Q value is well controlled,\n",
      "[00:12:56.440 --> 00:12:58.360]   it's not extremely large.\n",
      "[00:12:58.360 --> 00:13:01.280]   And everything is well-behaved in terms of concentration\n",
      "[00:13:01.280 --> 00:13:02.200]   and statistical error.\n",
      "[00:13:02.200 --> 00:13:07.520]   I think this will not happen if we\n",
      "[00:13:07.520 --> 00:13:09.960]   don't add this bonus.\n",
      "[00:13:09.960 --> 00:13:14.840]   Only this bonus can make this Q extremely large.\n",
      "[00:13:14.840 --> 00:13:17.240]   So that's why in the earlier step, in the similar setting,\n",
      "[00:13:17.240 --> 00:13:18.400]   we don't do this truncation.\n",
      "[00:13:18.400 --> 00:13:30.200]   So this is a part about we just compute the UCB.\n",
      "[00:13:30.200 --> 00:13:32.720]   But after we construct UCB, and the other step\n",
      "[00:13:32.720 --> 00:13:35.800]   in the multi-ampenet is that we will take greedy action\n",
      "[00:13:35.800 --> 00:13:40.360]   regarding to the upper confidence bound we computed.\n",
      "[00:13:40.360 --> 00:13:41.960]   In the multi-ampenet setting, it's basically\n",
      "[00:13:41.960 --> 00:13:44.440]   we computed the upper confidence bound for each arm.\n",
      "[00:13:44.440 --> 00:13:47.360]   And we just take the arm as high as upper confidence bound.\n",
      "[00:13:47.360 --> 00:13:49.120]   Here is essentially the same.\n",
      "[00:13:49.120 --> 00:13:52.680]   We will just take greedy action regarding to this Q value.\n",
      "[00:13:52.680 --> 00:13:54.680]   We compute it.\n",
      "[00:13:54.680 --> 00:14:00.400]   So for h from 1 to h, we will take action.\n",
      "[00:14:00.400 --> 00:14:27.280]   A h is equal to argmax A in A Q h S H A. And we will observe\n",
      "[00:14:27.280 --> 00:14:28.440]   the next data.\n",
      "[00:14:28.440 --> 00:14:38.860]   Then next states, h plus 1, and the reward,\n",
      "[00:14:38.860 --> 00:14:39.360]   arg.\n",
      "[00:14:39.360 --> 00:14:56.880]   And then we just add this tuple to the data set.\n",
      "[00:14:56.880 --> 00:15:01.320]   d is equal to d union with a new tuple.\n",
      "[00:15:01.320 --> 00:15:07.240]   That is, at h step, we observe S H H and S H plus 1.\n",
      "[00:15:07.240 --> 00:15:23.200]   So the second part is essentially execute greedy policy.\n",
      "[00:15:23.200 --> 00:15:37.000]   So you can think this is just a reinforcement\n",
      "[00:15:37.000 --> 00:15:40.480]   MDP version of a UCB algorithm.\n",
      "[00:15:40.480 --> 00:15:44.520]   Instead of adding bonus to the value of each arm,\n",
      "[00:15:44.520 --> 00:15:46.880]   you're adding bonus to the--\n",
      "[00:15:46.880 --> 00:15:48.560]   in the Bellman update.\n",
      "[00:15:48.560 --> 00:15:51.000]   So that's a computer upper bound of Q value.\n",
      "[00:15:51.000 --> 00:15:53.720]   And then you just take a greedy policy regarding\n",
      "[00:15:53.720 --> 00:15:55.480]   to this upper bound of Q value.\n",
      "[00:15:55.480 --> 00:16:13.360]   Any questions?\n",
      "[00:16:13.360 --> 00:16:15.320]   [INAUDIBLE]\n",
      "[00:16:15.320 --> 00:16:21.280]   D is the data set.\n",
      "[00:16:21.280 --> 00:16:24.520]   Yeah, I think I swapped a little bit.\n",
      "[00:16:24.520 --> 00:16:27.000]   You can think this is a--\n",
      "[00:16:27.000 --> 00:16:30.880]   if you are a rigorous, if there exists any d H S A,\n",
      "[00:16:30.880 --> 00:16:33.920]   and it can be anything in D. That basically just says,\n",
      "[00:16:33.920 --> 00:16:36.720]   I have ever observed an S A pair at H\n",
      "[00:16:36.720 --> 00:16:39.600]   step in this data set.\n",
      "[00:16:39.600 --> 00:16:42.640]   So what is the D of H S A again?\n",
      "[00:16:42.640 --> 00:16:44.480]   Are it just that greedy?\n",
      "[00:16:44.480 --> 00:16:45.840]   What D of H--\n",
      "[00:16:45.840 --> 00:16:50.800]   At the end of the Q H, I say, 1, 1, [INAUDIBLE]\n",
      "[00:16:50.800 --> 00:16:52.780]   [INAUDIBLE]\n",
      "[00:16:52.780 --> 00:16:53.800]   Sorry?\n",
      "[00:16:53.800 --> 00:16:54.440]   [INAUDIBLE]\n",
      "[00:16:54.440 --> 00:16:54.940]   What?\n",
      "[00:16:54.940 --> 00:16:59.200]   [INAUDIBLE]\n",
      "[00:16:59.200 --> 00:17:00.200]   Oh, this one?\n",
      "[00:17:00.200 --> 00:17:00.700]   Yeah.\n",
      "[00:17:00.700 --> 00:17:02.440]   This is a-- oh, our choice of bonus.\n",
      "[00:17:02.440 --> 00:17:08.480]   We'll talk about that.\n",
      "[00:17:08.480 --> 00:17:11.280]   So basically, we already mentioned here our choice of bonus.\n",
      "[00:17:11.280 --> 00:17:17.940]   [INAUDIBLE]\n",
      "[00:17:17.940 --> 00:17:21.040]   The bonus is only a function of the count,\n",
      "[00:17:21.040 --> 00:17:25.480]   like how many times we observe this S A pair at H step.\n",
      "[00:17:25.480 --> 00:17:28.240]   And this choice will be equal to C,\n",
      "[00:17:28.240 --> 00:17:32.600]   some constant times the square root of H squared\n",
      "[00:17:32.600 --> 00:17:37.720]   Yoda over N, where this Yoda is just some log vector,\n",
      "[00:17:37.720 --> 00:17:44.560]   like log S A G over P.\n",
      "[00:17:44.560 --> 00:17:48.040]   So we will actually show why this construction of bonus\n",
      "[00:17:48.040 --> 00:17:51.880]   is valid, and you will see why this thing kind of happens.\n",
      "[00:17:51.880 --> 00:17:57.840]   So essentially, this bonus reflects\n",
      "[00:17:57.840 --> 00:18:03.240]   our confidence of this value after seeing it N times.\n",
      "[00:18:03.240 --> 00:18:23.840]   So it somehow reflects the fluctuation of random variables\n",
      "[00:18:23.840 --> 00:18:29.600]   bounded by H. Because eventually, this\n",
      "[00:18:29.600 --> 00:18:32.560]   is like something random according to the value.\n",
      "[00:18:32.560 --> 00:18:44.760]   So after we introduce the algorithm,\n",
      "[00:18:44.760 --> 00:18:48.880]   we will now talk about what kind of guarantees\n",
      "[00:18:48.880 --> 00:18:50.960]   we can provide for this algorithm.\n",
      "[00:18:50.960 --> 00:18:54.040]   First, any question about the algorithm?\n",
      "[00:18:54.040 --> 00:18:58.640]   The algorithm part is just more or less like a time series\n",
      "[00:18:58.640 --> 00:19:01.920]   version of UCBL with an in-volume bandwidth.\n",
      "[00:19:01.920 --> 00:19:12.600]   So because this is like an exploration setting,\n",
      "[00:19:12.600 --> 00:19:15.600]   we will also talk about regret in DPP.\n",
      "[00:19:15.600 --> 00:19:17.080]   And in our last lecture, we already\n",
      "[00:19:17.080 --> 00:19:19.480]   says once we get a regret, we can easily\n",
      "[00:19:19.480 --> 00:19:26.880]   convert it into the sample complexity.\n",
      "[00:19:26.880 --> 00:19:29.480]   So regret is even stronger notice.\n",
      "[00:19:29.480 --> 00:19:31.400]   I think in a lot of reinforcement setting,\n",
      "[00:19:31.400 --> 00:19:32.760]   we don't really care about regret.\n",
      "[00:19:32.760 --> 00:19:35.120]   But you can think this is like a convenient way\n",
      "[00:19:35.120 --> 00:19:37.560]   of getting sample chemistry as well.\n",
      "[00:19:37.560 --> 00:19:44.760]   So in terms of regret in MDP, we can also define a regret up\n",
      "[00:19:44.760 --> 00:19:54.480]   to k round is equal to k times the optimal value\n",
      "[00:19:54.480 --> 00:20:04.360]   subtracted by the value we get at every episode, v1,\n",
      "[00:20:04.360 --> 00:20:06.200]   pi k as 1.\n",
      "[00:20:06.200 --> 00:20:13.080]   This is like the total value if we always\n",
      "[00:20:13.080 --> 00:20:14.120]   say optimal policy.\n",
      "[00:20:33.960 --> 00:20:43.360]   This is a value achieved by the learner.\n",
      "[00:20:43.360 --> 00:20:54.200]   So it's the same definition.\n",
      "[00:20:54.200 --> 00:20:55.400]   There are only differences.\n",
      "[00:20:55.400 --> 00:21:00.240]   We replace the reward of each arm by the value of MDP.\n",
      "[00:21:00.240 --> 00:21:22.440]   So the most important theorem, we will say,\n",
      "[00:21:22.440 --> 00:21:27.640]   is actually we can show with probability at least 1\n",
      "[00:21:27.640 --> 00:21:47.800]   minus p, the regret of a UCBVI algorithm\n",
      "[00:21:47.800 --> 00:21:58.080]   is upper bounded by the following, that regret k\n",
      "[00:21:58.080 --> 00:22:05.720]   is upper bounded by some constant C times\n",
      "[00:22:05.720 --> 00:22:20.320]   the square root of h cubed sa t yota plus h cubed\n",
      "[00:22:20.320 --> 00:22:29.280]   s square a yota square, where this t is equal to k h.\n",
      "[00:22:29.280 --> 00:22:32.120]   You can also put h to the fourth.\n",
      "[00:22:32.120 --> 00:22:36.140]   So regret up to k round is this amount.\n",
      "[00:22:36.140 --> 00:22:38.040]   So sometimes people write in t because t\n",
      "[00:22:38.040 --> 00:22:40.400]   is a total number of steps you observe.\n",
      "[00:22:40.400 --> 00:22:43.080]   And in case the number of episodes you observe.\n",
      "[00:22:43.080 --> 00:23:09.600]   Let's say-- where there's a yota is, again, the logarithmic\n",
      "[00:23:09.600 --> 00:23:11.840]   term.\n",
      "[00:23:11.840 --> 00:23:12.840]   Usually, we don't care.\n",
      "[00:23:12.840 --> 00:23:15.400]   It's just sa t over p.\n",
      "[00:23:15.400 --> 00:23:18.440]   Basically, you can then log everything.\n",
      "[00:23:18.440 --> 00:23:19.880]   And log h as sa t here.\n",
      "[00:23:19.880 --> 00:23:33.400]   I think we can also put a match here.\n",
      "[00:23:33.400 --> 00:23:35.800]   It doesn't really matter.\n",
      "[00:23:35.800 --> 00:23:40.080]   Let's-- oh, we already have edge.\n",
      "[00:23:40.080 --> 00:23:41.120]   I think sa t is fine.\n",
      "[00:23:41.120 --> 00:23:42.720]   Because t is like a k h.\n",
      "[00:23:42.720 --> 00:23:44.720]   So t is already contained inside.\n",
      "[00:23:44.720 --> 00:23:57.360]   So we kind of notice this is like the formal theorem.\n",
      "[00:23:57.360 --> 00:24:00.920]   But basically, if we want to know the leading order term,\n",
      "[00:24:00.920 --> 00:24:02.800]   this is like square root t term.\n",
      "[00:24:02.800 --> 00:24:05.240]   And this is something independent of t.\n",
      "[00:24:05.240 --> 00:24:07.880]   We know to regret it will be something like--\n",
      "[00:24:07.880 --> 00:24:10.800]   the worst case regret will be greater like a linear in k.\n",
      "[00:24:10.800 --> 00:24:13.720]   So the square root k is already something non-trivial.\n",
      "[00:24:13.720 --> 00:24:16.760]   And this is like something constant independent of k.\n",
      "[00:24:16.760 --> 00:24:19.560]   So this is like the lower order term.\n",
      "[00:24:19.560 --> 00:24:21.480]   This is the leading order term because square root k.\n",
      "[00:24:21.480 --> 00:24:23.600]   This is the lower order term because independent of k.\n",
      "[00:24:23.600 --> 00:24:37.240]   So what we really have done is we kind of achieve this--\n",
      "[00:24:37.240 --> 00:24:39.840]   this guarantee kind of already says something like that.\n",
      "[00:24:39.840 --> 00:24:42.200]   We get square root t or square root k.\n",
      "[00:24:42.200 --> 00:24:43.560]   But the most important thing is we\n",
      "[00:24:43.560 --> 00:24:45.960]   don't pay anything like exponential in h.\n",
      "[00:24:45.960 --> 00:24:49.280]   As in the random exploration or epsilon greedy,\n",
      "[00:24:49.280 --> 00:24:54.320]   where we basically only pay par number in h in the worst case.\n",
      "[00:24:54.320 --> 00:24:57.300]   [VIDEO PLAYBACK]\n",
      "[00:24:57.300 --> 00:25:00.280]   [END PLAYBACK]\n",
      "[00:25:00.280 --> 00:25:03.260]   [VIDEO PLAYBACK]\n",
      "[00:25:03.260 --> 00:25:06.240]   [END PLAYBACK]\n",
      "[00:25:06.240 --> 00:25:09.220]   [END PLAYBACK]\n",
      "[00:25:09.220 --> 00:25:12.200]   [END PLAYBACK]\n",
      "[00:25:12.200 --> 00:25:17.180]   [END PLAYBACK]\n",
      "[00:25:17.180 --> 00:25:19.180]   [END PLAYBACK]\n",
      "[00:25:19.180 --> 00:25:21.180]   [END PLAYBACK]\n",
      "[00:25:21.180 --> 00:25:23.180]   [END PLAYBACK]\n",
      "[00:25:23.180 --> 00:25:25.180]   [END PLAYBACK]\n",
      "[00:25:25.180 --> 00:25:27.180]   [END PLAYBACK]\n",
      "[00:25:27.180 --> 00:25:29.180]   [END PLAYBACK]\n",
      "[00:25:29.180 --> 00:25:30.180]   [END PLAYBACK]\n",
      "[00:25:30.180 --> 00:25:31.180]   [END PLAYBACK]\n",
      "[00:25:31.180 --> 00:25:32.180]   [END PLAYBACK]\n",
      "[00:25:32.180 --> 00:25:33.180]   [END PLAYBACK]\n",
      "[00:25:33.180 --> 00:25:34.180]   [END PLAYBACK]\n",
      "[00:25:34.180 --> 00:25:35.180]   [END PLAYBACK]\n",
      "[00:25:35.180 --> 00:25:36.180]   [END PLAYBACK]\n",
      "[00:25:36.180 --> 00:25:37.180]   [END PLAYBACK]\n",
      "[00:25:37.180 --> 00:25:41.180]   So the question is the t-term is the order of kh.\n",
      "[00:25:41.180 --> 00:25:50.180]   k is just exactly equal to k. I'm saying if you care about what the red is in terms of k, that's why this term matters.\n",
      "[00:25:50.180 --> 00:25:53.180]   This is the leading term because this is like square root k term.\n",
      "[00:25:53.180 --> 00:25:56.180]   So I'm going to take a look at this.\n",
      "[00:25:56.180 --> 00:25:59.180]   I'm going to take a look at this.\n",
      "[00:25:59.180 --> 00:26:02.180]   I'm going to take a look at this.\n",
      "[00:26:02.180 --> 00:26:06.180]   I'm going to take a look at this.\n",
      "[00:26:06.180 --> 00:26:09.180]   I'm going to take a look at this.\n",
      "[00:26:09.180 --> 00:26:12.180]   I'm going to take a look at this.\n",
      "[00:26:12.180 --> 00:26:16.180]   I'm going to take a look at this.\n",
      "[00:26:16.180 --> 00:26:19.180]   I'm going to take a look at this.\n",
      "[00:26:19.180 --> 00:26:30.180]   So now we'll talk a little bit more about why this algorithm would work.\n",
      "[00:26:30.180 --> 00:26:35.180]   Like we intuitively say this is just adding bonus similar to UCB.\n",
      "[00:26:35.180 --> 00:26:42.180]   So we'll actually be a bit more concrete improving theorem 1.\n",
      "[00:26:42.180 --> 00:26:48.180]   So in order to do that, we'll introduce a slightly a bit more notation so that we don't confuse with a quantity.\n",
      "[00:26:48.180 --> 00:27:02.180]   I think the most important notion is that we'll add a superscript k.\n",
      "[00:27:02.180 --> 00:27:08.180]   The reason is here we kind of repeatedly like we update the q-value based on the new data.\n",
      "[00:27:08.180 --> 00:27:15.180]   And it's kind of a little bit confusing if we just say some q-value because at each different episode the q-value is actually different.\n",
      "[00:27:15.180 --> 00:27:37.180]   So we're just adding a superscript k to denote the quantity computed at episode k.\n",
      "[00:27:37.180 --> 00:27:47.180]   For example, we have an NHK SAAS prime.\n",
      "[00:27:47.180 --> 00:27:50.180]   So essentially in the algorithm we have this NHSA.\n",
      "[00:27:50.180 --> 00:28:00.180]   But here to be very specific about which episode the number is showing up in which episode we would like to add a superscript k here.\n",
      "[00:28:00.180 --> 00:28:28.180]   And this will be denoted as a number of HSAAS prime observed up to episode k.\n",
      "[00:28:28.180 --> 00:28:41.180]   And similarly we will also use the pk at HSAAS prime SA denoted the empirical transition matrix we computed at the case episodes.\n",
      "[00:28:41.180 --> 00:28:56.180]   And the q-value at case episode v-value at case episode essentially we'll just denote this notation.\n",
      "[00:28:56.180 --> 00:29:19.180]   So these are the values.\n",
      "[00:29:19.180 --> 00:29:34.180]   And then we will say this the SHK, AHK, or we will say S1K, A1K, dadaatio, S capital HK, A capital HK.\n",
      "[00:29:34.180 --> 00:29:48.180]   This is a trajectory collected at episode k.\n",
      "[00:29:48.180 --> 00:30:13.180]   But I forgot to mention that there's one thing we need to be careful.\n",
      "[00:30:13.180 --> 00:30:20.180]   In the value iteration it turns out in the episodic setting we only do one path of value iteration.\n",
      "[00:30:20.180 --> 00:30:27.180]   But here in the exploration setting we notice that at every episode we're collecting one new trajectory.\n",
      "[00:30:27.180 --> 00:30:32.180]   And after collecting one new trajectory we do a full path of value iteration.\n",
      "[00:30:32.180 --> 00:30:38.180]   This is essentially like we re-compute a value iteration path at every episode.\n",
      "[00:30:38.180 --> 00:30:45.180]   This can be like costy in the end and we will talk about another algorithm killing later because of this.\n",
      "[00:30:45.180 --> 00:31:07.180]   This is required compute a full path of vi at every episode.\n",
      "[00:31:07.180 --> 00:31:14.180]   This is like a very costy because at every episode we only observe one trajectory.\n",
      "[00:31:14.180 --> 00:31:26.180]   However we're updating all the values because it's kind of related to the later step can be updated.\n",
      "[00:31:26.180 --> 00:31:39.180]   We need to re-compute everything and this is like a very costy.\n",
      "[00:31:39.180 --> 00:32:04.180]   What's the word that we did compute a full path of vi?\n",
      "[00:32:04.180 --> 00:32:13.180]   So with this new notation essentially the way we're going to prove theorem 1 is very similar to the UCB in the multi-ant-benefit setting.\n",
      "[00:32:13.180 --> 00:32:15.180]   We essentially have two steps.\n",
      "[00:32:15.180 --> 00:32:40.180]   One is we will show that the kill cage assay is an upper bound of a kill star chassis.\n",
      "[00:32:40.180 --> 00:32:48.180]   This is by the name of UCB essentially the way we construct we design this bonus we want to ensure this is true.\n",
      "[00:32:48.180 --> 00:32:58.180]   And the second step is we do the regret analysis.\n",
      "[00:32:58.180 --> 00:33:03.180]   And by UCB we were eventually able to show something like a regret.\n",
      "[00:33:03.180 --> 00:33:09.180]   It's smaller than summation of all the bonus.\n",
      "[00:33:09.180 --> 00:33:16.180]   And this is what we have been done for the UCB setting in the multi-ant-benefit like up to some constant.\n",
      "[00:33:16.180 --> 00:33:27.180]   And here we will do something after small terms.\n",
      "[00:33:27.180 --> 00:33:36.180]   So the second step is a little bit tedious for this like a lot of computation for the lectures.\n",
      "[00:33:36.180 --> 00:33:45.180]   So we will leave the second step to lecture notes if you're interested like you can look at a specific calculation about how to derive the regret bound.\n",
      "[00:33:45.180 --> 00:33:53.180]   So in the class we will mostly focus on the first step like showcase why our construction justified this is an upper bound.\n",
      "[00:33:53.180 --> 00:34:03.180]   So we will do this in lecture, do this today.\n",
      "[00:34:03.180 --> 00:34:18.180]   And this will defer to lecture notes.\n",
      "[00:34:18.180 --> 00:34:22.180]   So formally for the first step we need to prove the lemma.\n",
      "[00:34:22.180 --> 00:34:29.180]   Essentially this is, we just make this statement more rigorous.\n",
      "[00:34:29.180 --> 00:34:31.180]   This step more rigorous.\n",
      "[00:34:31.180 --> 00:34:53.180]   We say with probability greater equal to 1 minus p, we have qhk sa greater than q star sa\n",
      "[00:34:53.180 --> 00:35:10.180]   and vhk s greater than vh star s\n",
      "[00:35:10.180 --> 00:35:28.180]   for any sahk in set of state, set of action, and the horizon is the upper bounded by h.\n",
      "[00:35:28.180 --> 00:35:42.180]   This episode is upper bounded by capital K.\n",
      "[00:35:42.180 --> 00:36:09.180]   So basically throughout every step and throughout all episodes we will always maintain with high probability this our computed q value will always be in upper bounds of the value of the optimal policy.\n",
      "[00:36:09.180 --> 00:36:35.180]   And the question is about the statement before we proceed to the proof.\n",
      "[00:36:35.180 --> 00:36:45.180]   So essentially the proof a lot of times of reinforcement learning is basically by induction.\n",
      "[00:36:45.180 --> 00:36:49.180]   So here again we will do the induction.\n",
      "[00:36:49.180 --> 00:36:57.180]   We will do induction from capital H to 1.\n",
      "[00:36:57.180 --> 00:37:17.180]   So the base case is for the h plus 1 step.\n",
      "[00:37:17.180 --> 00:37:21.180]   Because in the algorithm we never update the value at h plus 1 step.\n",
      "[00:37:21.180 --> 00:37:22.180]   So this is not updated.\n",
      "[00:37:22.180 --> 00:37:38.180]   So that means at every episode my value at h plus 1 step will be always equal to the initial value we set, which is equal to 0 and equal to the optimal value.\n",
      "[00:37:38.180 --> 00:37:46.180]   So the base case is pretty straightforward.\n",
      "[00:37:46.180 --> 00:38:08.180]   So now we assume we have a V h plus 1 K as greater or equal to V star, which plus 1 is for OS.\n",
      "[00:38:08.180 --> 00:38:17.180]   For all the later steps we already guaranteed this V h plus 1 K is already the upper bound of a V star.\n",
      "[00:38:17.180 --> 00:38:28.180]   So now we want to do the induction so to prove for each step we still have the value is also an upper bounds.\n",
      "[00:38:28.180 --> 00:38:39.180]   So we divide into two cases, the first case is if state action, H s a is not in decay.\n",
      "[00:38:39.180 --> 00:38:46.180]   So this H s a pair has not been observed in the data set up to case time.\n",
      "[00:38:46.180 --> 00:39:05.180]   So in this case we have not update Q h K as a because if we don't observe it and we have a skip update.\n",
      "[00:39:05.180 --> 00:39:09.180]   So it will still remain as an initial value.\n",
      "[00:39:09.180 --> 00:39:26.180]   So this Q K H s a will be equal to the H, which is initial value, which is clearly in the upper bounds of Q star H s a.\n",
      "[00:39:26.180 --> 00:39:54.180]   [ Pause ]\n",
      "[00:39:54.180 --> 00:39:59.180]   And on the other hand, if we have observed this,\n",
      "[00:39:59.180 --> 00:40:12.180]   [ Pause ]\n",
      "[00:40:12.180 --> 00:40:18.180]   And then we kind of divide it into two steps, divided into two cases.\n",
      "[00:40:18.180 --> 00:40:28.180]   If we already know H is less or equal to R H s a plus, what do we have?\n",
      "[00:40:28.180 --> 00:40:56.180]   [ Pause ]\n",
      "[00:40:56.180 --> 00:41:03.180]   In this case, because we're doing a truncation, we know that Q value will be equal to H,\n",
      "[00:41:03.180 --> 00:41:13.180]   because remembering not that we're doing the minimum of the two.\n",
      "[00:41:13.180 --> 00:41:20.180]   This is equal to H, and it's again greater than Q star H s a.\n",
      "[00:41:20.180 --> 00:41:27.180]   So the only non-trivial case is the otherwise, like this is actually smaller than H.\n",
      "[00:41:27.180 --> 00:41:32.180]   [ Pause ]\n",
      "[00:41:32.180 --> 00:41:35.180]   So the last case, we need to do a little bit calculation.\n",
      "[00:41:35.180 --> 00:41:48.180]   This is basically the reason why we picked this specific choice of bonus.\n",
      "[00:41:48.180 --> 00:42:10.180]   [ Pause ]\n",
      "[00:42:10.180 --> 00:42:19.180]   So we look at the Q k H s a, subtract by Q star H s a.\n",
      "[00:42:19.180 --> 00:42:30.180]   [ Pause ]\n",
      "[00:42:30.180 --> 00:42:34.180]   This we can basically by the update equation.\n",
      "[00:42:34.180 --> 00:42:45.180]   We immediately know this Q k by definition is equal to P H k, P hat H k, V H plus 1 k,\n",
      "[00:42:45.180 --> 00:42:49.180]   I say, and plus bonus.\n",
      "[00:42:49.180 --> 00:42:55.180]   [ Pause ]\n",
      "[00:42:55.180 --> 00:43:01.180]   And then subtracted by a second part, which is by Bellman equation.\n",
      "[00:43:01.180 --> 00:43:07.180]   P H, V H plus 1 star, I say.\n",
      "[00:43:07.180 --> 00:43:13.180]   So essentially, we just expand this, just update equation, expand this with Bellman equation,\n",
      "[00:43:13.180 --> 00:43:20.180]   and then we cancel out the reward.\n",
      "[00:43:20.180 --> 00:43:23.180]   So we can rearrange the term a little bit.\n",
      "[00:43:23.180 --> 00:43:39.180]   We say this is equal to P hat H k, V H plus 1 k, subtracted by V H plus 1 star,\n",
      "[00:43:39.180 --> 00:43:58.180]   I say, plus P hat k H, subtracted by P H, V H plus 1 star.\n",
      "[00:43:58.180 --> 00:44:08.180]   I say, and finally, I'm adding the bonus of V of an H k, I say.\n",
      "[00:44:08.180 --> 00:44:14.180]   So all I'm doing is we subtract the P hat H V star and add the P hat H V star back.\n",
      "[00:44:14.180 --> 00:44:21.180]   So I didn't do anything, it's just we kind of decompose the difference into two parts.\n",
      "[00:44:21.180 --> 00:44:28.180]   The most important reason we decompose those terms into two parts is we can basically\n",
      "[00:44:28.180 --> 00:44:32.180]   like conclude the first term is greater or equal to zero by induction.\n",
      "[00:44:32.180 --> 00:44:41.180]   So by induction, we know this is equal to zero, greater equal to zero.\n",
      "[00:44:41.180 --> 00:44:49.180]   Because we know for any step in the future, like H plus 1, we already assume this is already an upper bound.\n",
      "[00:44:49.180 --> 00:44:55.180]   So once this V k is an upper bound of V star, and if you just apply some transition on something positive,\n",
      "[00:44:55.180 --> 00:45:02.180]   this is again positive thing. So this will be immediately greater equal to zero, this is fine.\n",
      "[00:45:02.180 --> 00:45:11.180]   So in order to make this q k to be an upper bound, all we need to make sure is the second term,\n",
      "[00:45:11.180 --> 00:45:28.180]   we want to ensure the second term greater equal to zero.\n",
      "[00:45:28.180 --> 00:45:41.180]   And now we become very familiar with this thing, because essentially what we want to do is we use a bonus term to cover some like stochastic term,\n",
      "[00:45:41.180 --> 00:45:46.180]   which turns out this is like a summation of independent random, maybe not independent random ball.\n",
      "[00:45:46.180 --> 00:45:57.180]   But like summation of marking your terms, and eventually we can say as long as we use some hopping bound to do this.\n",
      "[00:45:57.180 --> 00:46:19.180]   So to more specifically, we can say, hopping is equal to\n",
      "[00:46:19.180 --> 00:46:36.180]   guarantees with probability at least 1 minus p for all k H S A.\n",
      "[00:46:36.180 --> 00:46:52.180]   We have the following, that is this p k H hat, subject by p H, v H plus 1 star S A.\n",
      "[00:46:52.180 --> 00:47:05.180]   So this p v star is the expectation, well this p hat v star is the empirical average of k of like an n H k independent,\n",
      "[00:47:05.180 --> 00:47:16.180]   sorry, like a martingale samples. So we can say this is less or equal to c times h square yoda,\n",
      "[00:47:16.180 --> 00:47:31.180]   where h square is essentially the scaling of v. So we know this is like bounded by H.\n",
      "[00:47:31.180 --> 00:47:47.180]   C times h square yoda of n H k S A. So this is just by hopping inequality, essentially we have this many items summation together.\n",
      "[00:47:47.180 --> 00:47:57.180]   We've taken average, and we know the convergence is like 1 over square root of n. So this is why we put a 1 over square root of n here.\n",
      "[00:47:57.180 --> 00:48:06.180]   And we specifically choose it too, so that we may match the bonus. Essentially we make the bonus to be exactly equal to this,\n",
      "[00:48:06.180 --> 00:48:13.180]   or maybe greater than this by some constant factor. And this is, yes?\n",
      "[00:48:13.180 --> 00:48:28.180]   >> I think this is essentially the same thing we talk about in a UCB scenario. If you're still remembering a UCB scenario,\n",
      "[00:48:28.180 --> 00:48:34.180]   we also need to, like in a multi band, band indicates, we also need to do this type of concentration.\n",
      "[00:48:34.180 --> 00:48:40.180]   We're saying this is essentially a hopping inequality. There was only difference that this n H k,\n",
      "[00:48:40.180 --> 00:48:47.180]   like the number of times you put that arm, here is also the number of times you visited S A, k is like random.\n",
      "[00:48:47.180 --> 00:48:53.180]   So that's why you kind of need to do a little bit more advanced techniques in addition to just the hopping,\n",
      "[00:48:53.180 --> 00:49:02.180]   you need to like do azuma hopping, which is the same like a similar argument to UCB.\n",
      "[00:49:02.180 --> 00:49:25.180]   To handle the randomness\n",
      "[00:49:25.180 --> 00:49:37.180]   on this count n H k, I see.\n",
      "[00:49:37.180 --> 00:49:45.180]   So this essentially justifies our choice of bonus. And we can essentially just put a bonus here.\n",
      "[00:49:45.180 --> 00:49:54.180]   And we say like this will, with high probability, cover whatever the activity happens in this concentration term.\n",
      "[00:49:54.180 --> 00:50:05.180]   And so we can guarantee this is a greater equal to zero. And that finished the proof.\n",
      "[00:50:05.180 --> 00:50:27.180]   There is actually one more step. So the very last step is very, very fast.\n",
      "[00:50:27.180 --> 00:50:39.180]   So once we approve this Q H k assay, it's greater or equal to Q star H S A for all S S S A.\n",
      "[00:50:39.180 --> 00:50:56.180]   We just say the V value is also the same. This implies that V H k assay is equal to the max over A Q H k assay.\n",
      "[00:50:56.180 --> 00:51:01.180]   And this is a greater or equal to we're taking max over a lower bound.\n",
      "[00:51:01.180 --> 00:51:14.180]   That is Q star H assay, which is equal to V H star S. So essentially we can use this to finish the induction and everything is done.\n",
      "[00:51:14.180 --> 00:51:43.180]   [ Pause ]\n",
      "[00:51:43.180 --> 00:52:06.180]   [ Pause ]\n",
      "[00:52:06.180 --> 00:52:16.180]   So we say eventually the proof of theorem one will be just some regret analysis because we already proved the upper bound.\n",
      "[00:52:16.180 --> 00:52:25.180]   And then we can essentially leverage this fact to show that regret k is less or equal to summation of the bonus.\n",
      "[00:52:25.180 --> 00:52:46.180]   For specifically is something like k from one and H of this bonus at nk, nhk, s hk, a hk.\n",
      "[00:52:46.180 --> 00:52:58.180]   So we will only count in a bonus at the state action we visited at the case at episodes and plus some small terms.\n",
      "[00:52:58.180 --> 00:53:12.180]   Some other small concentration terms. So we'll leave this to lecture notes.\n",
      "[00:53:12.180 --> 00:53:21.180]   We'll emphasize like this is usually the way we prove the upper bound like a U C P algorithm. First we prove it's upper bound and then we say regret is upper bounded by bonus.\n",
      "[00:53:21.180 --> 00:53:31.180]   So we can see from here we cannot just like if we only care about the first part of being upper bound we can just make bonus to be extremely large.\n",
      "[00:53:31.180 --> 00:53:37.180]   Like for example I always pick Q value to be capital H and that's clearly an upper bound.\n",
      "[00:53:37.180 --> 00:53:42.180]   The problem is eventually the regret is going to be bounded by some constant times the bonus.\n",
      "[00:53:42.180 --> 00:53:50.180]   So in that kind of case if we make the upper bound very loose this bonus is also very loose and this entire grad will suffer a lot.\n",
      "[00:53:50.180 --> 00:53:56.180]   So essentially the key point here is like we want to make this upper bound as sharp as possible.\n",
      "[00:53:56.180 --> 00:54:19.180]   So in this case we essentially use a concentration inequality to do the upper bounds and then eventually we can do the regret based on this.\n",
      "[00:54:19.180 --> 00:54:26.180]   Q star, what is that, how does that relate to the upper bounding bonus?\n",
      "[00:54:26.180 --> 00:54:29.180]   Upper bounding bonus. Sorry.\n",
      "[00:54:29.180 --> 00:54:35.180]   The level one we just present Q is upper bound.\n",
      "[00:54:35.180 --> 00:54:38.180]   Right.\n",
      "[00:54:38.180 --> 00:54:42.180]   So I'm saying for the level one we can actually choose bonus to be very large.\n",
      "[00:54:42.180 --> 00:54:52.180]   So for example in here all we need always say is like as long as we choose bonus greater than this hopping concentration we can actually guarantee this to be an upper bound.\n",
      "[00:54:52.180 --> 00:54:56.180]   Right. So basically we can choose any bonus that's greater than this.\n",
      "[00:54:56.180 --> 00:55:03.180]   So that won't hurt the level one but that will hurt the second step where we prove the regret because regret is upper bounded by the bonus.\n",
      "[00:55:03.180 --> 00:55:10.180]   So that's why we want the sharp sharp sharpest bonus as we can.\n",
      "[00:55:10.180 --> 00:55:31.180]   So another thing I want to talk about is the design of bonus function.\n",
      "[00:55:31.180 --> 00:55:59.180]   So it turns out this UCP value, UCP via algorithm, the design of the bonus function actually matters because this bonus function actually not only affected analysis but also affected algorithm because in the algorithm we explicitly need to add bonus to the value and then we collect data according to the greedy policy computed by this\n",
      "[00:55:59.180 --> 00:56:02.180]   like upper bound on the value.\n",
      "[00:56:02.180 --> 00:56:28.180]   So this design of bonus function actually affects the algorithm and the data we collect.\n",
      "[00:56:28.180 --> 00:56:41.180]   So this is why it goes back to the previous point I said we want to essentially use the sharpest bonus function we want but on the other hand it's still computable, it doesn't depend on some uncomputable quantity.\n",
      "[00:56:41.180 --> 00:56:52.180]   So the way we introduce the lecture like this BN is equal to like H square Yoda C over N. This is like what we previously did.\n",
      "[00:56:52.180 --> 00:56:59.180]   So we previously this is like essentially designed by Houghton's concentration in quality.\n",
      "[00:56:59.180 --> 00:57:20.180]   So as we learn from the concentration in quality we actually know like this is a Houghton's concentration in quality is not the sharpest concentration in quality.\n",
      "[00:57:20.180 --> 00:57:26.180]   A lot of times we can actually do burn sensing in quality and that's actually give a sharper bound.\n",
      "[00:57:26.180 --> 00:57:41.180]   So it turns out that you can also design and improve the sharper bonus\n",
      "[00:57:41.180 --> 00:57:53.180]   based on burn sensing in quality.\n",
      "[00:57:53.180 --> 00:57:56.180]   So presumably this is going to help.\n",
      "[00:57:56.180 --> 00:58:09.180]   So the only complication here is because burn sensing in quality actually depends on the variance where the variance which is different from Houghton which is essentially just bounded by the variable.\n",
      "[00:58:09.180 --> 00:58:12.180]   We know it's bounded by H so we just directly put an H here.\n",
      "[00:58:12.180 --> 00:58:27.180]   Well here we essentially need to estimate the variance.\n",
      "[00:58:27.180 --> 00:58:38.180]   So this creates a lot more a little bit more like steps because essentially we don't know the variance so we need to estimate the variance and use the estimated variance into our like a bonus design.\n",
      "[00:58:38.180 --> 00:59:02.180]   And the analysis is a lot more involved but eventually if we make everything work you can actually improve a square root of H factor in regret.\n",
      "[00:59:02.180 --> 00:59:08.180]   So what I mean is like actually doing a sharper bonus indeed going to help in terms of regret.\n",
      "[00:59:08.180 --> 00:59:22.180]   So originally we got some square root of H cube SAT and now we get a square root of H square SAT.\n",
      "[00:59:22.180 --> 00:59:51.180]   [BLANK_AUDIO]\n",
      "[00:59:51.180 --> 00:59:57.180]   So any question up to now?\n",
      "[00:59:57.180 --> 01:00:16.180]   So after we introduce the entire use of VVL, we want to revisit the hard instance we talk about which kind of make it very difficult to experiment the combinatorial log.\n",
      "[01:00:16.180 --> 01:00:26.180]   Like why use of V is able to solve this problem.\n",
      "[01:00:26.180 --> 01:00:33.180]   [BLANK_AUDIO]\n",
      "[01:00:33.180 --> 01:00:44.180]   If you still remember combinatorial log is a two-state MDP where essentially there is only one specific action sequence.\n",
      "[01:00:44.180 --> 01:00:49.180]   That eventually going to reach the and the state of that S0.\n",
      "[01:00:49.180 --> 01:00:57.180]   Everything else is going to leads to the absorbing state S1.\n",
      "[01:00:57.180 --> 01:01:04.180]   [BLANK_AUDIO]\n",
      "[01:01:04.180 --> 01:01:09.180]   And never goes back.\n",
      "[01:01:09.180 --> 01:01:26.180]   So we say if we just do some random exploration, this problem actually is very difficult because the chance we sampled precise action sequence that a one star, a two star, a three star till a H star is like exponentially small.\n",
      "[01:01:26.180 --> 01:01:34.180]   And it's extremely difficult to find this optimal policy by just doing some random exploration.\n",
      "[01:01:34.180 --> 01:01:46.180]   However, we can see by doing UCP, it's actually doing a lot better, in a sense like it eliminates incorrect action very fast.\n",
      "[01:01:46.180 --> 01:01:58.180]   So we will essentially start with all the value to be like H or the Q value to be H.\n",
      "[01:01:58.180 --> 01:02:02.180]   [BLANK_AUDIO]\n",
      "[01:02:02.180 --> 01:02:25.180]   So after we observe some multiple trajectories, for example at this step, if we observe a lot of other actions which is different than a three, we notice the secure value of other actions that is not a three will decrease very fast.\n",
      "[01:02:25.180 --> 01:02:31.180]   So in that case, we'll eventually very fast eliminate the incorrect action.\n",
      "[01:02:31.180 --> 01:02:42.180]   We'll do this step by step and eventually we'll be able to find this optimal value in some part of our time.\n",
      "[01:02:42.180 --> 01:02:52.180]   So actually the.\n",
      "[01:02:52.180 --> 01:03:02.180]   [BLANK_AUDIO]\n",
      "[01:03:02.180 --> 01:03:20.180]   So essentially the whole idea is again, we will do the dynamic programming type of way to find the optimal policy.\n",
      "[01:03:20.180 --> 01:03:38.180]   [BLANK_AUDIO]\n",
      "[01:03:38.180 --> 01:03:45.180]   So this also says the commentary log thing is actually only hard for specific algorithm.\n",
      "[01:03:45.180 --> 01:03:50.180]   Like for random exploration, it's actually not very hard for for well designed algorithm.\n",
      "[01:03:50.180 --> 01:03:55.180]   For example, for VIUCB, we can actually guarantee this one will not suffer from some exponential age.\n",
      "[01:03:55.180 --> 01:04:00.180]   And it's actually with polynomial time, we can already find the optimal policy.\n",
      "[01:04:00.180 --> 01:04:10.180]   [BLANK_AUDIO]\n",
      "[01:04:10.180 --> 01:04:20.180]   [BLANK_AUDIO]\n",
      "[01:04:20.180 --> 01:04:38.180]   [BLANK_AUDIO]\n",
      "[01:04:38.180 --> 01:04:44.180]   So this is about how we gonna combine the upper confidence bound idea with the vector iteration.\n",
      "[01:04:44.180 --> 01:04:52.180]   So remember in a similar setting, we not only talk about the vector equation, we also have our Q-learning algorithm, which is doing incremental updates.\n",
      "[01:04:52.180 --> 01:04:59.180]   And we also noticed in the VIUCB setting, we noticed that there is a one very big computational problem.\n",
      "[01:04:59.180 --> 01:05:07.180]   Like in every episode, we need to do a full path of value iteration, although we only like observe one trajectory, but we need to do an entire path.\n",
      "[01:05:07.180 --> 01:05:13.180]   So essentially, we can do the Q-learning with the UCB as well.\n",
      "[01:05:13.180 --> 01:05:19.180]   And this algorithm will actually fix that computational issue and do the truly online learning algorithm.\n",
      "[01:05:19.180 --> 01:05:22.180]   So the algorithm is as follows.\n",
      "[01:05:22.180 --> 01:05:29.180]   [BLANK_AUDIO]\n",
      "[01:05:29.180 --> 01:05:34.180]   And this is in our previous paper in 2018.\n",
      "[01:05:34.180 --> 01:05:41.180]   [BLANK_AUDIO]\n",
      "[01:05:41.180 --> 01:05:46.180]   So again, we initialize all the Q-values to BH.\n",
      "[01:05:46.180 --> 01:05:57.180]   [BLANK_AUDIO]\n",
      "[01:05:57.180 --> 01:06:04.180]   And NHSA equal to zero, that is the count for all HSA.\n",
      "[01:06:04.180 --> 01:06:14.180]   [BLANK_AUDIO]\n",
      "[01:06:14.180 --> 01:06:26.180]   [BLANK_AUDIO]\n",
      "[01:06:26.180 --> 01:06:31.180]   So we do the for loop, that is for episode.\n",
      "[01:06:31.180 --> 01:06:36.180]   [BLANK_AUDIO]\n",
      "[01:06:36.180 --> 01:06:38.180]   Okay, from one to capital K.\n",
      "[01:06:38.180 --> 01:06:41.180]   [BLANK_AUDIO]\n",
      "[01:06:41.180 --> 01:06:58.180]   We first receive first state initial state as one, and then for a step H equal to one to capital H.\n",
      "[01:06:58.180 --> 01:07:00.180]   [BLANK_AUDIO]\n",
      "[01:07:00.180 --> 01:07:06.180]   So we no longer decompose the entire thing into two big trunk, like the VI or collecting data.\n",
      "[01:07:06.180 --> 01:07:13.180]   So the benefit of Q learning is like we do increment update, so every time we collect one new data, we will immediately do the update.\n",
      "[01:07:13.180 --> 01:07:18.180]   So we combine a taken trajectory with this value update.\n",
      "[01:07:18.180 --> 01:07:19.180]   [BLANK_AUDIO]\n",
      "[01:07:19.180 --> 01:07:24.180]   So we will take action H is equal to argmax.\n",
      "[01:07:24.180 --> 01:07:27.180]   [BLANK_AUDIO]\n",
      "[01:07:27.180 --> 01:07:32.180]   A of the current value we maintain.\n",
      "[01:07:32.180 --> 01:07:36.180]   [BLANK_AUDIO]\n",
      "[01:07:36.180 --> 01:07:38.180]   And observe the next state.\n",
      "[01:07:38.180 --> 01:07:57.180]   [BLANK_AUDIO]\n",
      "[01:07:57.180 --> 01:08:09.180]   And we will set some T is equal to count of SHH, which will be incremented by one, because we observe this SHH now.\n",
      "[01:08:09.180 --> 01:08:12.180]   [BLANK_AUDIO]\n",
      "[01:08:12.180 --> 01:08:15.180]   So this SHH is always the count.\n",
      "[01:08:15.180 --> 01:08:20.180]   How many times we have observed this SHH at H stuff?\n",
      "[01:08:20.180 --> 01:08:24.180]   So most important part is we do this Q value update.\n",
      "[01:08:24.180 --> 01:08:29.180]   So this QH, SA, SHH.\n",
      "[01:08:29.180 --> 01:08:39.180]   If you still remember, and the Q value is Q learning update is equal to one minus R for T fraction of the previous Q value.\n",
      "[01:08:39.180 --> 01:08:43.180]   QH, SHH.\n",
      "[01:08:43.180 --> 01:08:46.180]   Plus the R for T fraction of the new value.\n",
      "[01:08:46.180 --> 01:08:56.180]   [BLANK_AUDIO]\n",
      "[01:08:56.180 --> 01:09:05.180]   [BLANK_AUDIO]\n",
      "[01:09:05.180 --> 01:09:12.180]   R for T fraction of the update, where update is equal to the reward.\n",
      "[01:09:12.180 --> 01:09:18.180]   Plus the value of the next step taking on the states we observe in the current data.\n",
      "[01:09:18.180 --> 01:09:25.180]   And the plus of bonus B, where B is only depends on this T.\n",
      "[01:09:25.180 --> 01:09:32.180]   [BLANK_AUDIO]\n",
      "[01:09:32.180 --> 01:09:39.180]   So again, we just adding this bonus.\n",
      "[01:09:39.180 --> 01:09:45.180]   [BLANK_AUDIO]\n",
      "[01:09:45.180 --> 01:09:59.180]   Where we choose this bonus is equal to C times, with slightly worse dependence on H, H, Yoda, H, Q, Yoda over T.\n",
      "[01:09:59.180 --> 01:10:04.180]   [BLANK_AUDIO]\n",
      "[01:10:04.180 --> 01:10:12.180]   And then we just do the V value is equal to the max of Q value, but truncated with H.\n",
      "[01:10:12.180 --> 01:10:31.180]   [BLANK_AUDIO]\n",
      "[01:10:31.180 --> 01:10:32.180]   That's it.\n",
      "[01:10:32.180 --> 01:11:01.180]   [BLANK_AUDIO]\n",
      "[01:11:01.180 --> 01:11:13.180]   So we see this, there's no need to compute the P hat, which is like the estimate of transition.\n",
      "[01:11:13.180 --> 01:11:17.180]   We're in this entire Q learning, we don't actually compute P hat.\n",
      "[01:11:17.180 --> 01:11:20.180]   So this is typically considered as a model free algorithm.\n",
      "[01:11:20.180 --> 01:11:27.180]   [BLANK_AUDIO]\n",
      "[01:11:27.180 --> 01:11:39.180]   And also we only update the value of the trajectory we visited.\n",
      "[01:11:39.180 --> 01:11:56.180]   [BLANK_AUDIO]\n",
      "[01:11:56.180 --> 01:12:04.180]   So instead of doing a full pass of value iteration, which will need to essentially recompute the value for all state action and H, at every step.\n",
      "[01:12:04.180 --> 01:12:08.180]   Here we only update the value at the state and action we visited.\n",
      "[01:12:08.180 --> 01:12:12.180]   So this is a lot faster in computation than VIO could be.\n",
      "[01:12:12.180 --> 01:12:32.180]   [BLANK_AUDIO]\n",
      "[01:12:32.180 --> 01:12:36.180]   So remember we have this like a memory, memory advantage.\n",
      "[01:12:36.180 --> 01:12:42.180]   [BLANK_AUDIO]\n",
      "[01:12:42.180 --> 01:13:10.180]   [BLANK_AUDIO]\n",
      "[01:13:10.180 --> 01:13:23.180]   So again, key thing here is we need to essentially put more remember in the Q learning one very important thing is doing like this bias variance trade off.\n",
      "[01:13:23.180 --> 01:13:29.180]   So we need to put more weights on later updates.\n",
      "[01:13:29.180 --> 01:13:33.180]   [BLANK_AUDIO]\n",
      "[01:13:33.180 --> 01:13:41.180]   Like reduce the variance, which kind of if you still remember in a similar setting, which is equivalent to like a choosing.\n",
      "[01:13:41.180 --> 01:13:44.180]   [BLANK_AUDIO]\n",
      "[01:13:44.180 --> 01:13:50.180]   Choosing this learning with R for T is something of order H over T.\n",
      "[01:13:50.180 --> 01:13:56.180]   So we noted if it's chosen as one over T, this is exactly equal to like a uniform average.\n",
      "[01:13:56.180 --> 01:14:04.180]   Well now I'm like putting it greater than one over T so that means I put more weights on the like later updates than the previous updates.\n",
      "[01:14:04.180 --> 01:14:09.180]   So once we do this, everything like that, we will essentially be able to prove the next theorem.\n",
      "[01:14:09.180 --> 01:14:12.180]   [BLANK_AUDIO]\n",
      "[01:14:12.180 --> 01:14:20.180]   With probability greater or equal to 1 minus p, the regret of Q learning UCB.\n",
      "[01:14:20.180 --> 01:14:39.180]   [BLANK_AUDIO]\n",
      "[01:14:39.180 --> 01:14:57.180]   Is that most, less or equal to C times square root of H to the fourth S A T Yoda.\n",
      "[01:14:57.180 --> 01:15:07.180]   Where Yoda is again a log term.\n",
      "[01:15:07.180 --> 01:15:12.180]   [BLANK_AUDIO]\n",
      "[01:15:12.180 --> 01:15:17.180]   And we also notice this required is one square root H factor worse than.\n",
      "[01:15:17.180 --> 01:15:25.180]   [BLANK_AUDIO]\n",
      "[01:15:25.180 --> 01:15:35.180]   This is due to the same reason like this is like incremental updates.\n",
      "[01:15:35.180 --> 01:15:45.180]   Similar in a generator model like similar setting, we also observe a slightly worse sample complexity because we essentially need to favor the later updates.\n",
      "[01:15:45.180 --> 01:15:57.180]   So that we're not handling noise as good as like a VIUCB which is like basically remembering all the data and use the smallest like variance.\n",
      "[01:15:57.180 --> 01:16:05.180]   Here's online updates and we kind of scrolling away data so we're less efficient in terms of sample complexity than the user BVI.\n",
      "[01:16:05.180 --> 01:16:15.180]   [BLANK_AUDIO]\n",
      "[01:16:15.180 --> 01:16:25.180]   [BLANK_AUDIO]\n",
      "[01:16:25.180 --> 01:16:32.180]   [BLANK_AUDIO]\n",
      "[01:16:32.180 --> 01:16:35.180]   Okay, I think we're going to stop here today.\n",
      "[01:16:35.180 --> 01:16:42.180]   So starting from next lecture, we're going to summarize this exploration setting and start talking about lower bound.\n",
      "[01:16:42.180 --> 01:16:43.180]   Yes, questions?\n",
      "[01:16:43.180 --> 01:16:55.180]   [BLANK_AUDIO]\n",
      "[01:16:55.180 --> 01:16:59.180]   So RFRT is always need to be decreasing because you have more and more samples.\n",
      "[01:16:59.180 --> 01:17:07.180]   So even if you're doing uniform weight, you should expect RFRT to be decreasing because if you have T data, every data only get weight one over T.\n",
      "[01:17:07.180 --> 01:17:12.180]   If you had like two T data, every data should only have like weight one over two T.\n",
      "[01:17:12.180 --> 01:17:13.180]   That's the uniform.\n",
      "[01:17:13.180 --> 01:17:27.180]   So I think in the last lecture, the lecture before we kind of exist and mention if this is a one over T and by doing this one minus RFRT of previous one and RFRT fraction of the new update,\n",
      "[01:17:27.180 --> 01:17:29.180]   you get precisely uniform average.\n",
      "[01:17:29.180 --> 01:17:37.180]   And whatever you make it greater than one over T, then it's like a favoring later one than the previous one.\n",
      "[01:17:37.180 --> 01:17:46.180]   [BLANK_AUDIO]\n",
      "[01:17:46.180 --> 01:17:54.180]   So the fast to compute in terms of the pre-toration is per episode is much faster to compute.\n",
      "[01:17:54.180 --> 01:17:59.180]   But the last sample version I mean is it probably need slightly more episodes.\n",
      "[01:17:59.180 --> 01:18:04.180]   [BLANK_AUDIO]\n",
      "[01:18:04.180 --> 01:18:11.180]   Will not because this is only like a square root of H factor worse in terms of semi-relatities.\n",
      "[01:18:11.180 --> 01:18:15.180]   So that means essentially you need H more factor of episodes.\n",
      "[01:18:15.180 --> 01:18:21.180]   But per episode, you're only like computing one trajectory, like update one trajectory.\n",
      "[01:18:21.180 --> 01:18:29.180]   Well, in a UCBI, you're essentially doing a full passive added version that you need to look at all states and all actions.\n",
      "[01:18:29.180 --> 01:18:34.180]   So there essentially you have a SA factor more in terms of computation.\n",
      "[01:18:34.180 --> 01:18:36.180]   So there's a trade-off there.\n",
      "[01:18:36.180 --> 01:18:39.180]   And in the next lecture, we'll actually talk about that trade-off.\n",
      "[01:18:39.180 --> 01:18:43.180]   [MUSIC]\n",
      "[01:18:43.180 --> 01:18:51.180]   [BLANK_AUDIO]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/VU73LRk8Zjw.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/VU73LRk8Zjw.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/VU73LRk8Zjw.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/VU73LRk8Zjw.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1277.54 ms\n",
      "whisper_print_timings:     fallbacks =  11 p /  30 h\n",
      "whisper_print_timings:      mel time =  2709.71 ms\n",
      "whisper_print_timings:   sample time = 23196.17 ms / 56336 runs (    0.41 ms per run)\n",
      "whisper_print_timings:   encode time =   378.60 ms /   210 runs (    1.80 ms per run)\n",
      "whisper_print_timings:   decode time =  1276.93 ms /   723 runs (    1.77 ms per run)\n",
      "whisper_print_timings:   batchd time = 29123.94 ms / 54511 runs (    0.53 ms per run)\n",
      "whisper_print_timings:   prompt time = 10759.58 ms / 48807 runs (    0.22 ms per run)\n",
      "whisper_print_timings:    total time = 69194.66 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=lCCT9JGkLw8 started\n",
      "lCCT9JGkLw8\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/lCCT9JGkLw8.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/lCCT9JGkLw8.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:20:05.90, start: 0.000000, bitrate: 244 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 145 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/lCCT9JGkLw8.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  150185kB time=01:20:05.90 bitrate= 256.0kbits/s speed=1.39e+03x    \n",
      "video:0kB audio:150184kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000051%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/lCCT9JGkLw8.wav' (76894459 samples, 4805.9 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.240]   [MUSIC PLAYING]\n",
      "[00:00:03.240 --> 00:00:04.720]   So in the very end of last lecture,\n",
      "[00:00:04.720 --> 00:00:06.360]   we talk about multi-rom bandage.\n",
      "[00:00:06.360 --> 00:00:15.920]   There are two objective.\n",
      "[00:00:15.920 --> 00:00:22.720]   And the objective one is we want to identify the epsilon\n",
      "[00:00:22.720 --> 00:00:34.760]   optimal arm. In terms of the mean reward,\n",
      "[00:00:34.760 --> 00:00:39.680]   we want the arm we identify is at most an epsilon\n",
      "[00:00:39.680 --> 00:00:42.520]   worse than the optimal arm.\n",
      "[00:00:42.520 --> 00:00:45.000]   In a second objective, we're saying\n",
      "[00:00:45.000 --> 00:00:47.480]   we want to achieve low regret.\n",
      "[00:00:47.480 --> 00:00:50.940]   [MUSIC PLAYING]\n",
      "[00:00:50.940 --> 00:01:07.040]   In a sense, we define a regret to be t times r star.\n",
      "[00:01:07.040 --> 00:01:09.520]   That is the total reward if you're\n",
      "[00:01:09.520 --> 00:01:13.240]   going to receive, if you always put the optimal arm,\n",
      "[00:01:13.240 --> 00:01:16.360]   and subtracted by the expected reward you're\n",
      "[00:01:16.360 --> 00:01:20.200]   going to receive by the algorithm,\n",
      "[00:01:20.200 --> 00:01:23.920]   where i t is the arm, the agent for actual t's time.\n",
      "[00:01:23.920 --> 00:01:28.960]   So I think we say the major difference\n",
      "[00:01:28.960 --> 00:01:32.360]   is in the first objective, we just eventually\n",
      "[00:01:32.360 --> 00:01:34.280]   want to find the epsilon optimal arm.\n",
      "[00:01:34.280 --> 00:01:36.400]   We don't really care about it in the middle,\n",
      "[00:01:36.400 --> 00:01:39.200]   like during the learning process, what do we pull?\n",
      "[00:01:39.200 --> 00:01:41.080]   We can just pull arbitrary bad arms.\n",
      "[00:01:41.080 --> 00:01:43.520]   As long as in the end, we find epsilon optimal arm.\n",
      "[00:01:43.520 --> 00:01:44.920]   This is good.\n",
      "[00:01:44.920 --> 00:01:49.200]   On the other hand, object 2 seems like a more difficult.\n",
      "[00:01:49.200 --> 00:01:53.120]   So it's like a harder objective, which not only says,\n",
      "[00:01:53.120 --> 00:01:55.400]   you want to eventually find some optimal arm.\n",
      "[00:01:55.400 --> 00:01:59.200]   Because eventually you want this regret to be small,\n",
      "[00:01:59.200 --> 00:02:03.240]   you can already make sure it's it to be approximate,\n",
      "[00:02:03.240 --> 00:02:05.400]   to be i star.\n",
      "[00:02:05.400 --> 00:02:07.960]   Not only want to eventually you put a right arm,\n",
      "[00:02:07.960 --> 00:02:10.840]   but also in the middle, like during the entire learning\n",
      "[00:02:10.840 --> 00:02:16.000]   process, you want your arm pull is also like nearly optimal.\n",
      "[00:02:16.000 --> 00:02:17.880]   Even if in the earlier phase, you\n",
      "[00:02:17.880 --> 00:02:19.480]   don't want to pull some terrible arm,\n",
      "[00:02:19.480 --> 00:02:22.640]   like suffer significant regret.\n",
      "[00:02:22.640 --> 00:02:24.720]   So this is a more difficult objective.\n",
      "[00:02:24.720 --> 00:02:36.680]   And we need to do more work to balance\n",
      "[00:02:36.680 --> 00:02:40.400]   the trade-off between exploration versus exploitation.\n",
      "[00:02:40.400 --> 00:02:49.440]   Exploration, in the sense, eventually\n",
      "[00:02:49.440 --> 00:02:51.400]   we want to find an epsilon optimal arm,\n",
      "[00:02:51.400 --> 00:02:54.560]   so that our asymptotic performance of regret is good.\n",
      "[00:02:54.560 --> 00:02:58.600]   And exploitation, in the sense, during the entire place\n",
      "[00:02:58.600 --> 00:03:01.400]   rule, we should not just play some arbitrary bad arm,\n",
      "[00:03:01.400 --> 00:03:03.480]   so that our suffer a lot of regret.\n",
      "[00:03:03.480 --> 00:03:05.640]   So that's why it's like a big balance between those two.\n",
      "[00:03:05.640 --> 00:03:12.200]   So we will first talk about the conversion\n",
      "[00:03:12.200 --> 00:03:14.300]   between these two objectives.\n",
      "[00:03:14.300 --> 00:03:18.160]   So for a lot of times, we can do some conversion,\n",
      "[00:03:18.160 --> 00:03:21.080]   but we will see later this conversion actually\n",
      "[00:03:21.080 --> 00:03:26.840]   will suffer some looseness if we do a lot of conversion.\n",
      "[00:03:26.840 --> 00:03:28.440]   This is online to batch conversion.\n",
      "[00:03:28.440 --> 00:03:40.280]   [INAUDIBLE]\n",
      "[00:03:40.280 --> 00:03:42.960]   So this is the level one.\n",
      "[00:03:42.960 --> 00:03:47.000]   Although we say this conversion in the multi arm\n",
      "[00:03:47.000 --> 00:03:49.440]   balance setting, but this conversion\n",
      "[00:03:49.440 --> 00:03:53.920]   can also be extended to reinforcement learning setting\n",
      "[00:03:53.920 --> 00:03:54.920]   very easily.\n",
      "[00:03:54.920 --> 00:03:56.560]   It's very straightforward extension.\n",
      "[00:03:56.560 --> 00:04:06.440]   [INAUDIBLE]\n",
      "[00:04:06.440 --> 00:04:08.760]   So we will talk about the relation between the regret\n",
      "[00:04:08.760 --> 00:04:11.960]   guarantee and the sample capacity of identified epsilon\n",
      "[00:04:11.960 --> 00:04:12.480]   best arm.\n",
      "[00:04:12.480 --> 00:04:17.560]   So the conversion stays in two ways.\n",
      "[00:04:17.560 --> 00:04:26.520]   The first is if algorithm A has a regret.\n",
      "[00:04:26.520 --> 00:04:31.880]   [INAUDIBLE]\n",
      "[00:04:31.880 --> 00:04:36.600]   C times t to the 1 minus alpha.\n",
      "[00:04:36.600 --> 00:04:40.200]   So we talk about why we want to regret to be this form.\n",
      "[00:04:40.200 --> 00:04:44.120]   Essentially, because if the reward is bounded by 0 to 1,\n",
      "[00:04:44.120 --> 00:04:47.620]   then this regret is always in 0 to t.\n",
      "[00:04:47.620 --> 00:04:55.160]   So anything linear in t regret is like trivial.\n",
      "[00:04:55.160 --> 00:04:56.880]   It's not very interesting.\n",
      "[00:04:56.880 --> 00:05:00.320]   So we always want some sublinear regret.\n",
      "[00:05:00.320 --> 00:05:03.280]   That is 1 minus alpha, where alpha is some constants.\n",
      "[00:05:03.280 --> 00:05:07.320]   In 0 to 1.\n",
      "[00:05:07.320 --> 00:05:15.420]   So regret bound is always going to be like this sub-linear in t.\n",
      "[00:05:15.420 --> 00:05:17.120]   This is like non-trivial regret.\n",
      "[00:05:17.120 --> 00:05:23.520]   If we have an algorithm A that has a regret of specific form\n",
      "[00:05:23.520 --> 00:05:26.680]   like this, c times t to the 1 minus alpha,\n",
      "[00:05:26.680 --> 00:05:28.840]   we'll see some absolute constant.\n",
      "[00:05:28.840 --> 00:05:37.640]   Then algorithm A will find an epsilon\n",
      "[00:05:37.640 --> 00:06:00.600]   optimal distribution of R in c over epsilon to the 1 minus r\n",
      "[00:06:00.600 --> 00:06:10.560]   for samples.\n",
      "[00:06:10.560 --> 00:06:13.580]   So basically, t to the 1 minus alpha regret\n",
      "[00:06:13.580 --> 00:06:15.480]   will convert to the sample complexity that\n",
      "[00:06:15.480 --> 00:06:19.840]   is 1 over epsilon to the 1 over alpha,\n",
      "[00:06:19.840 --> 00:06:21.280]   total power of 1 over alpha.\n",
      "[00:06:25.320 --> 00:06:33.560]   And on the other side, conversely,\n",
      "[00:06:33.560 --> 00:06:54.620]   if algorithm A finds epsilon optimal R,\n",
      "[00:06:54.620 --> 00:06:58.220]   this cause of distribution, it doesn't matter.\n",
      "[00:06:58.220 --> 00:07:01.460]   It doesn't matter if the single R marks the distribution.\n",
      "[00:07:01.460 --> 00:07:16.660]   In c times epsilon to the minus beta samples,\n",
      "[00:07:16.660 --> 00:07:22.940]   then we can use algorithm A to achieve regret.\n",
      "[00:07:22.940 --> 00:07:51.460]   Two times c of 1 over 1 plus beta times t to the beta 1\n",
      "[00:07:51.460 --> 00:07:52.140]   plus beta.\n",
      "[00:07:52.140 --> 00:08:01.220]   So we'll talk about the exponent later.\n",
      "[00:08:01.220 --> 00:08:04.620]   But the most important thing is if we forget about the constant\n",
      "[00:08:04.620 --> 00:08:07.980]   thing, so what do we say is any sub-linear regret t\n",
      "[00:08:07.980 --> 00:08:10.660]   to the 1 minus alpha can be converted to some sample\n",
      "[00:08:10.660 --> 00:08:14.180]   complexity that is 1 of epsilon to the power of 1 over alpha.\n",
      "[00:08:14.180 --> 00:08:16.460]   And on the other hand, if we have some sample complexity\n",
      "[00:08:16.460 --> 00:08:19.140]   that is 1 of epsilon to the beta,\n",
      "[00:08:19.140 --> 00:08:21.740]   then we can convert it to a regret that\n",
      "[00:08:21.740 --> 00:08:26.220]   is t to the beta over 1 plus beta.\n",
      "[00:08:26.220 --> 00:08:27.700]   The other constant doesn't really matter.\n",
      "[00:08:27.700 --> 00:08:29.780]   You can just switch to some other constants.\n",
      "[00:08:29.780 --> 00:08:37.020]   So this result holds for any r phi and beta.\n",
      "[00:08:37.020 --> 00:08:39.940]   So we can convert a regret that is t to the 3/4\n",
      "[00:08:39.940 --> 00:08:41.780]   or convert a regret at a square root t\n",
      "[00:08:41.780 --> 00:08:43.820]   to something like sample complexity.\n",
      "[00:08:43.820 --> 00:08:49.940]   So we will first look at some particular examples.\n",
      "[00:08:49.940 --> 00:08:55.380]   I think a lot of times we will handle in the square root\n",
      "[00:08:55.380 --> 00:08:56.580]   t regret.\n",
      "[00:08:56.580 --> 00:08:59.300]   So for example, if we have some algorithm that\n",
      "[00:08:59.300 --> 00:09:04.140]   achieves square root t regret, in that case, r\n",
      "[00:09:04.140 --> 00:09:06.980]   phi is equal to 1/2.\n",
      "[00:09:06.980 --> 00:09:09.700]   And this implies we can achieve something\n",
      "[00:09:09.700 --> 00:09:14.460]   that is 1 over epsilon squared, because r phi is just 1/2.\n",
      "[00:09:14.460 --> 00:09:17.860]   So this will imply some sample complexity\n",
      "[00:09:17.860 --> 00:09:20.340]   that is 1 over epsilon squared samples.\n",
      "[00:09:20.340 --> 00:09:27.180]   And on the other hand, we can use the second rule.\n",
      "[00:09:27.180 --> 00:09:29.860]   That is, if we have some algorithm that\n",
      "[00:09:29.860 --> 00:09:35.060]   achieves 1 over epsilon squared samples,\n",
      "[00:09:35.060 --> 00:09:37.820]   we can convert it to a regret guarantee.\n",
      "[00:09:37.820 --> 00:09:42.100]   Now, in this case, it's clear, we just choose beta to be 2.\n",
      "[00:09:42.100 --> 00:09:45.140]   And now we notice we no longer get back square root t.\n",
      "[00:09:45.140 --> 00:09:47.060]   What we get is t to the 2/3.\n",
      "[00:09:47.060 --> 00:10:03.140]   And we note in the worst case regret is t.\n",
      "[00:10:03.140 --> 00:10:06.780]   So in terms of power, square root t\n",
      "[00:10:06.780 --> 00:10:09.020]   regret is actually better than t to the 2/3.\n",
      "[00:10:09.020 --> 00:10:11.780]   The closer you get to t, the worse you get the regret.\n",
      "[00:10:11.780 --> 00:10:13.900]   So this is a like, worse than square root t regret.\n",
      "[00:10:13.900 --> 00:10:26.020]   So we know that when we do the conversion,\n",
      "[00:10:26.020 --> 00:10:27.860]   we convert the regret to some of us here,\n",
      "[00:10:27.860 --> 00:10:30.940]   and convert some of us here back to the regret.\n",
      "[00:10:30.940 --> 00:10:35.500]   In this black box manner, we don't get back to original result\n",
      "[00:10:35.500 --> 00:10:36.700]   for free.\n",
      "[00:10:36.700 --> 00:10:40.500]   I think the biggest reason is, as we already said,\n",
      "[00:10:40.500 --> 00:10:42.300]   this regret is the harder objective.\n",
      "[00:10:42.300 --> 00:10:52.540]   So we convert a harder objective, which inherently require\n",
      "[00:10:52.540 --> 00:10:55.540]   way to balance the exploration versus exploitation,\n",
      "[00:10:55.540 --> 00:10:58.700]   to something we only require exploration.\n",
      "[00:10:58.700 --> 00:11:02.140]   And then we convert it back to the original harder objective.\n",
      "[00:11:02.140 --> 00:11:05.180]   We lose something during this entire process.\n",
      "[00:11:05.180 --> 00:11:09.140]   So this conversion is down like a not free light.\n",
      "[00:11:09.140 --> 00:11:11.620]   We lose something during the entire conversion.\n",
      "[00:11:11.620 --> 00:11:18.660]   So I just want to make sure you understand,\n",
      "[00:11:18.660 --> 00:11:21.260]   this conversion can give a connection\n",
      "[00:11:21.260 --> 00:11:22.700]   between two objectives.\n",
      "[00:11:22.700 --> 00:11:25.660]   But what I also want to say is, when you do this conversion,\n",
      "[00:11:25.660 --> 00:11:29.300]   you lose something in terms of the strength of the objective\n",
      "[00:11:29.300 --> 00:11:30.980]   or the rates.\n",
      "[00:11:30.980 --> 00:11:37.700]   [INAUDIBLE]\n",
      "[00:11:37.700 --> 00:11:39.500]   So we'll actually talk about proof,\n",
      "[00:11:39.500 --> 00:11:40.900]   and that will be much clearer.\n",
      "[00:11:40.900 --> 00:11:44.260]   So basically, whenever I have done algorithm,\n",
      "[00:11:44.260 --> 00:11:46.260]   I already achieved this regret.\n",
      "[00:11:46.260 --> 00:11:48.220]   Then this algorithm will immediately-- we\n",
      "[00:11:48.220 --> 00:11:49.980]   can use this algorithm immediately\n",
      "[00:11:49.980 --> 00:11:52.460]   to achieve this sample complexity.\n",
      "[00:11:52.460 --> 00:11:54.740]   And conversely, it's also the same.\n",
      "[00:11:54.740 --> 00:11:57.460]   If I have algorithm that already achieved this somehow\n",
      "[00:11:57.460 --> 00:12:00.460]   method, I can very easily modify the algorithm\n",
      "[00:12:00.460 --> 00:12:01.660]   to achieve this regret.\n",
      "[00:12:01.660 --> 00:12:20.060]   So this lemma is only talk about the rate conversion.\n",
      "[00:12:20.060 --> 00:12:23.860]   And we still need to provide an explicit construction.\n",
      "[00:12:23.860 --> 00:12:25.780]   When we have algorithm A achieved this regret,\n",
      "[00:12:25.780 --> 00:12:27.980]   why this algorithm already achieved this sample\n",
      "[00:12:27.980 --> 00:12:29.180]   complexity?\n",
      "[00:12:29.180 --> 00:12:30.260]   So we need to do the proof.\n",
      "[00:12:30.260 --> 00:12:42.260]   Any question about the claim and the qualitative results?\n",
      "[00:12:42.260 --> 00:12:44.780]   OK, if no problem, let's just do the first one.\n",
      "[00:12:44.780 --> 00:12:48.460]   This conversion actually is pretty straightforward.\n",
      "[00:12:48.460 --> 00:12:52.660]   So by first claim, for the conversion from regret,\n",
      "[00:12:52.660 --> 00:12:56.620]   for sample complexity, we're first\n",
      "[00:12:56.620 --> 00:13:04.900]   by precondition of regret.\n",
      "[00:13:04.900 --> 00:13:10.180]   We know by the definition of regret\n",
      "[00:13:10.180 --> 00:13:21.900]   that is equal to t times r star, i star subtract by summation\n",
      "[00:13:21.900 --> 00:13:26.860]   t from 1 to capital T, r i t.\n",
      "[00:13:26.860 --> 00:13:28.900]   And this is less or equal to--\n",
      "[00:13:28.900 --> 00:13:31.380]   because we have the precondition of the regret,\n",
      "[00:13:31.380 --> 00:13:34.140]   we know this is less or equal to some constant times\n",
      "[00:13:34.140 --> 00:13:35.580]   t to the 1 minus r of r.\n",
      "[00:13:35.580 --> 00:13:45.140]   So this implies that we can divide both sides by t\n",
      "[00:13:45.140 --> 00:13:47.300]   and the rearrange the terms.\n",
      "[00:13:47.300 --> 00:13:51.020]   So this implies that 1 over t summation t\n",
      "[00:13:51.020 --> 00:14:00.180]   from 1 to capital T, i t is greater or equal to i star\n",
      "[00:14:00.180 --> 00:14:04.020]   subtract by c over t to the r of r.\n",
      "[00:14:04.020 --> 00:14:16.340]   So we notice this is more or less what we want.\n",
      "[00:14:16.340 --> 00:14:21.660]   So left-hand side can be viewed as an expected reward\n",
      "[00:14:21.660 --> 00:14:26.900]   if I pull arm, according to some empirical distribution,\n",
      "[00:14:26.900 --> 00:14:39.020]   d hat, i, where this d hat is uniform over all the arms\n",
      "[00:14:39.020 --> 00:14:40.060]   I have pulled so far.\n",
      "[00:14:40.060 --> 00:14:46.820]   So i1, i2, about t o, i t.\n",
      "[00:14:46.820 --> 00:14:50.460]   That is, if I uniformly pull an arm,\n",
      "[00:14:50.460 --> 00:14:53.260]   this induce distribution.\n",
      "[00:14:53.260 --> 00:14:56.460]   And if I just uniformly pull through this distribution,\n",
      "[00:14:56.460 --> 00:15:00.140]   then this is corresponding to the left-hand side.\n",
      "[00:15:00.140 --> 00:15:02.140]   We calculate.\n",
      "[00:15:02.140 --> 00:15:07.420]   So this is an epsilon optimal distribution over arms.\n",
      "[00:15:07.420 --> 00:15:10.340]   So the only thing we need to do is we make this thing\n",
      "[00:15:10.340 --> 00:15:14.380]   to be epsilon, so that we guarantee this d hat distribution\n",
      "[00:15:14.380 --> 00:15:15.340]   is epsilon optimal.\n",
      "[00:15:15.340 --> 00:15:24.100]   And then we can just solve this, which we got t\n",
      "[00:15:24.100 --> 00:15:29.260]   is equal to c over epsilon to the 1 over alpha.\n",
      "[00:15:29.260 --> 00:15:32.340]   So that's why when we have this many of samples,\n",
      "[00:15:32.340 --> 00:15:36.940]   we can achieve epsilon optimal, epsilon optimal r.\n",
      "[00:15:36.940 --> 00:15:47.500]   OK, I think the first part is relatively simple,\n",
      "[00:15:47.500 --> 00:15:51.580]   because, as we said, regret is just by definition\n",
      "[00:15:51.580 --> 00:15:54.060]   a harder objective than samples.\n",
      "[00:15:54.060 --> 00:15:56.900]   So whenever we achieve the regret,\n",
      "[00:15:56.900 --> 00:16:00.500]   all we need to do is just divide it by t and do an average,\n",
      "[00:16:00.500 --> 00:16:03.420]   and we already achieve the epsilon optimal arm.\n",
      "[00:16:03.420 --> 00:16:05.380]   We already get the optimal arm.\n",
      "[00:16:06.380 --> 00:16:08.060]   So we'll do the second claim.\n",
      "[00:16:08.060 --> 00:16:14.980]   Second claim requires slightly more work,\n",
      "[00:16:14.980 --> 00:16:17.540]   because, like we say, the regret is a harder objective,\n",
      "[00:16:17.540 --> 00:16:20.380]   while sample complexity is like an easier objective.\n",
      "[00:16:20.380 --> 00:16:23.540]   So we want to convert an easier objective to a harder objective.\n",
      "[00:16:23.540 --> 00:16:25.340]   So that's why we need to lose something,\n",
      "[00:16:25.340 --> 00:16:27.700]   and then we need to do a little bit more work.\n",
      "[00:16:27.700 --> 00:16:31.940]   The strategy we're going to do is called explore and commit.\n",
      "[00:16:31.940 --> 00:16:58.700]   Explore and commit means our first spend C times epsilon\n",
      "[00:16:58.700 --> 00:17:06.380]   for the minus beta round to find epsilon optimal arm.\n",
      "[00:17:06.380 --> 00:17:18.700]   So this is-- by preconditioning, we\n",
      "[00:17:18.700 --> 00:17:21.500]   know this is the assembly method of arm A. So we'll first\n",
      "[00:17:21.500 --> 00:17:27.420]   just spend this much round to find epsilon optimal arm.\n",
      "[00:17:27.420 --> 00:17:32.660]   And then we will spend remaining rounds, remaining rounds.\n",
      "[00:17:32.660 --> 00:17:36.700]   That is t minus c times epsilon for the minus.\n",
      "[00:17:36.700 --> 00:17:45.500]   To always pull this epsilon optimal arm.\n",
      "[00:17:45.500 --> 00:17:51.740]   So it's very intuitive.\n",
      "[00:17:51.740 --> 00:17:52.740]   Explore and commit.\n",
      "[00:17:52.740 --> 00:17:55.220]   That is, in the first phase, I would just do exploration.\n",
      "[00:17:55.220 --> 00:17:57.260]   I will do whatever the algorithm do.\n",
      "[00:17:57.260 --> 00:17:59.260]   And to find the best arm.\n",
      "[00:17:59.260 --> 00:18:01.260]   To find the approximate best arm.\n",
      "[00:18:01.260 --> 00:18:04.340]   And the second phase, I would just commit to this approximate best\n",
      "[00:18:04.340 --> 00:18:04.620]   arm.\n",
      "[00:18:04.620 --> 00:18:06.140]   So I just always going to pull this arm.\n",
      "[00:18:06.140 --> 00:18:14.540]   So we'll calculate what is a regret achieved by this algorithm\n",
      "[00:18:14.540 --> 00:18:23.420]   induced by A. So the regret T we can compute.\n",
      "[00:18:23.420 --> 00:18:27.260]   That is less or equal to-- in the first phase,\n",
      "[00:18:27.260 --> 00:18:29.500]   we spend this many rounds.\n",
      "[00:18:29.500 --> 00:18:33.420]   C times epsilon to the minus beta.\n",
      "[00:18:33.420 --> 00:18:36.820]   And because in the first round, in the first phase,\n",
      "[00:18:36.820 --> 00:18:39.460]   we treat this algorithm as a black box.\n",
      "[00:18:39.460 --> 00:18:42.140]   We don't really know whether this algorithm has done any\n",
      "[00:18:42.140 --> 00:18:44.820]   exploration versus exploitation trade-off.\n",
      "[00:18:44.820 --> 00:18:47.900]   So we just assume the worst case where this algorithm probably\n",
      "[00:18:47.900 --> 00:18:50.340]   just do some random exploration, or I don't know.\n",
      "[00:18:50.340 --> 00:18:51.900]   In the worst case, probably the regret\n",
      "[00:18:51.900 --> 00:18:56.340]   is going to be one at each round.\n",
      "[00:18:56.340 --> 00:19:00.100]   So at each round, maybe at most, I was suffer loss one.\n",
      "[00:19:00.100 --> 00:19:06.220]   And in the second phase, I have this many rounds.\n",
      "[00:19:06.220 --> 00:19:11.340]   And because we know every time I already commit,\n",
      "[00:19:11.340 --> 00:19:14.460]   I pull the arm, which is guaranteed to be epsilon optimal.\n",
      "[00:19:14.460 --> 00:19:16.860]   So that means in the second phase, in every round,\n",
      "[00:19:16.860 --> 00:19:19.300]   I will at most suffer epsilon regret.\n",
      "[00:19:19.300 --> 00:19:29.820]   So we can just simplify this expression a little bit.\n",
      "[00:19:29.820 --> 00:19:32.840]   And this is less or equal to C times epsilon to the minus\n",
      "[00:19:32.840 --> 00:19:34.540]   beta.\n",
      "[00:19:34.540 --> 00:19:37.420]   And plus, in a lot of cases, this T\n",
      "[00:19:37.420 --> 00:19:41.380]   will be significantly greater than the C times epsilon\n",
      "[00:19:41.380 --> 00:19:42.180]   to the minus beta.\n",
      "[00:19:42.180 --> 00:19:43.980]   So we just ignore the second term.\n",
      "[00:19:43.980 --> 00:19:48.540]   Ignore the thing we subtract and provide an upper bound.\n",
      "[00:19:48.540 --> 00:19:50.580]   This is plus T times epsilon.\n",
      "[00:19:50.580 --> 00:20:11.580]   And finally, we just pick the epsilon that minimize the regret.\n",
      "[00:20:16.820 --> 00:20:19.620]   So the minimizer is regret, which is-- this epsilon\n",
      "[00:20:19.620 --> 00:20:25.660]   is equal to C over T to the power of 1 over 1 plus beta.\n",
      "[00:20:25.660 --> 00:20:29.140]   And we get the regret that is equal to 2 times C\n",
      "[00:20:29.140 --> 00:20:36.100]   to the 1 over 1 plus beta times T to the power of beta\n",
      "[00:20:36.100 --> 00:20:37.940]   over 1 plus beta.\n",
      "[00:20:37.940 --> 00:20:39.180]   And this finishes to prove.\n",
      "[00:20:39.180 --> 00:20:43.340]   Yes?\n",
      "[00:20:43.340 --> 00:20:53.060]   [INAUDIBLE]\n",
      "[00:20:53.060 --> 00:20:58.340]   So the exploit and commit algorithm\n",
      "[00:20:58.340 --> 00:20:59.940]   by contains two phase.\n",
      "[00:20:59.940 --> 00:21:03.020]   The first phase, I just used the algorithm A\n",
      "[00:21:03.020 --> 00:21:05.260]   to find the epsilon optimal arm, which\n",
      "[00:21:05.260 --> 00:21:10.340]   takes this many rounds by the precondition of algorithm A.\n",
      "[00:21:10.340 --> 00:21:13.580]   And the second phase is, I just commit to this optimal arm.\n",
      "[00:21:13.580 --> 00:21:15.940]   So in the second phase, every round,\n",
      "[00:21:15.940 --> 00:21:18.060]   I pay a regret of epsilon.\n",
      "[00:21:18.060 --> 00:21:19.660]   And in the first phase, I don't really\n",
      "[00:21:19.660 --> 00:21:21.100]   know what algorithm A is doing.\n",
      "[00:21:21.100 --> 00:21:24.180]   So I just assume the worst that we know every round\n",
      "[00:21:24.180 --> 00:21:25.540]   at my regret is at most 1.\n",
      "[00:21:25.540 --> 00:21:33.500]   Because regret, you can also write this regret in terms\n",
      "[00:21:33.500 --> 00:21:39.220]   of equal to summation T from 1 to capital T,\n",
      "[00:21:39.220 --> 00:21:43.980]   and which is i star subtract i T.\n",
      "[00:21:43.980 --> 00:21:47.900]   So we can talk about the calculated regret per round.\n",
      "[00:21:47.900 --> 00:21:50.120]   So each round, I will suffer regret at most 1.\n",
      "[00:21:50.120 --> 00:21:55.100]   And in the later phase, regret is at most epsilon.\n",
      "[00:21:55.100 --> 00:22:02.740]   Is this clear?\n",
      "[00:22:02.740 --> 00:22:05.220]   [INAUDIBLE]\n",
      "[00:22:05.220 --> 00:22:06.940]   This one equal to this one?\n",
      "[00:22:06.940 --> 00:22:08.620]   This summation is the same.\n",
      "[00:22:08.620 --> 00:22:14.380]   And this is just T. So summation of i star is a fixed thing.\n",
      "[00:22:14.380 --> 00:22:17.020]   So all you need to prove is T i star is equal to summation.\n",
      "[00:22:17.020 --> 00:22:18.340]   [INAUDIBLE]\n",
      "[00:22:18.340 --> 00:22:20.340]   [INAUDIBLE]\n",
      "[00:22:20.340 --> 00:22:23.900]   A product, product.\n",
      "[00:22:23.900 --> 00:22:26.500]   This is like the total reward that we receive by the optimal arm.\n",
      "[00:22:26.500 --> 00:22:42.860]   [INAUDIBLE]\n",
      "[00:22:42.860 --> 00:22:49.020]   To pick epsilon to minimize this term, you can think.\n",
      "[00:22:49.020 --> 00:22:51.380]   So you can think of this regret is basically--\n",
      "[00:22:51.380 --> 00:22:54.300]   beta is something positive.\n",
      "[00:22:54.300 --> 00:22:56.440]   So this is a C over epsilon to the beta.\n",
      "[00:22:56.440 --> 00:22:58.480]   And plus T times beta epsilon.\n",
      "[00:22:58.480 --> 00:23:00.380]   So you can think when we increase epsilon,\n",
      "[00:23:00.380 --> 00:23:03.300]   this term goes up, and this term goes down.\n",
      "[00:23:03.300 --> 00:23:05.180]   So there are some epsilon term which\n",
      "[00:23:05.180 --> 00:23:06.540]   minimize this entire thing.\n",
      "[00:23:06.540 --> 00:23:08.580]   And you can take derivative of equal to 0,\n",
      "[00:23:08.580 --> 00:23:10.060]   and which you will find.\n",
      "[00:23:10.060 --> 00:23:10.980]   This is equal to this.\n",
      "[00:23:10.980 --> 00:23:34.060]   [INAUDIBLE]\n",
      "[00:23:34.060 --> 00:23:35.660]   OK, those are all good questions.\n",
      "[00:23:35.660 --> 00:23:53.580]   [INAUDIBLE]\n",
      "[00:23:53.580 --> 00:23:55.620]   If no other questions, I guess we'll\n",
      "[00:23:55.620 --> 00:24:00.980]   start talking about the algorithm for multi-arm bandits,\n",
      "[00:24:00.980 --> 00:24:05.260]   and the regret and the sum of the four-dose algorithms.\n",
      "[00:24:05.260 --> 00:24:08.740]   Those are just-- and so far, we haven't talked about any algorithm\n",
      "[00:24:08.740 --> 00:24:10.060]   for multi-arm bandits.\n",
      "[00:24:10.060 --> 00:24:11.340]   We just talk about how we're going\n",
      "[00:24:11.340 --> 00:24:14.100]   to convert the two different objectives.\n",
      "[00:24:14.100 --> 00:24:16.660]   And later on, we will just purely focus on regret,\n",
      "[00:24:16.660 --> 00:24:19.060]   because this is like a standard notion people\n",
      "[00:24:19.060 --> 00:24:20.780]   use for online learning.\n",
      "[00:24:20.780 --> 00:24:23.340]   And we know regret is stronger than some other message.\n",
      "[00:24:23.340 --> 00:24:24.880]   So whenever we want some other message,\n",
      "[00:24:24.880 --> 00:24:27.340]   we can just use this conversion to convert it back\n",
      "[00:24:27.340 --> 00:24:30.380]   to some other message.\n",
      "[00:24:30.380 --> 00:24:33.780]   So the first algorithm we will introduce for the multi-arm\n",
      "[00:24:33.780 --> 00:24:35.620]   bandit is called Epsilon first algorithm.\n",
      "[00:24:35.620 --> 00:24:48.300]   Epsilon first algorithm essentially\n",
      "[00:24:48.300 --> 00:24:50.900]   just used to explore and commit a strategy.\n",
      "[00:24:50.900 --> 00:25:02.580]   Only difference is in this conversion,\n",
      "[00:25:02.580 --> 00:25:04.780]   we will actually say--\n",
      "[00:25:04.780 --> 00:25:14.340]   we will first spend this one to use algorithm A\n",
      "[00:25:14.340 --> 00:25:16.340]   to find Epsilon optimal arm.\n",
      "[00:25:16.340 --> 00:25:18.340]   Well, here, we will be very specific.\n",
      "[00:25:18.340 --> 00:25:23.460]   We just do some random exploration in the first phase.\n",
      "[00:25:23.460 --> 00:25:26.180]   So the first phase, we will first pull each arm.\n",
      "[00:25:26.180 --> 00:25:42.620]   And for n times, and obtain an empirical estimate.\n",
      "[00:25:42.620 --> 00:26:06.540]   And that is equal to summation of j equal to 1 of nn ij.\n",
      "[00:26:06.540 --> 00:26:34.180]   This ij denoted the random reward that we received for arm i\n",
      "[00:26:34.180 --> 00:26:35.140]   at j's pool.\n",
      "[00:26:35.140 --> 00:26:45.700]   So j is the index of-- like the index of pools\n",
      "[00:26:45.700 --> 00:26:48.020]   we get for arm i.\n",
      "[00:26:48.020 --> 00:26:53.140]   And we use an empirical average that divided by 1 over n.\n",
      "[00:26:53.140 --> 00:26:53.980]   Divided by n.\n",
      "[00:26:53.980 --> 00:26:58.220]   So essentially, the algorithm is very simple.\n",
      "[00:26:58.220 --> 00:26:59.940]   I just pull each arm n times.\n",
      "[00:26:59.940 --> 00:27:01.980]   And I use the average as my estimate\n",
      "[00:27:01.980 --> 00:27:03.540]   of the mean reward of which arm.\n",
      "[00:27:03.540 --> 00:27:11.500]   And then in the second phase, I will just play i hat equal\n",
      "[00:27:11.500 --> 00:27:19.740]   to argmax i of i hat.\n",
      "[00:27:19.740 --> 00:27:21.860]   That is, I will just pull the arm that\n",
      "[00:27:21.860 --> 00:27:25.780]   has the highest estimates in the first phase\n",
      "[00:27:25.780 --> 00:27:26.900]   for the remaining steps.\n",
      "[00:27:26.900 --> 00:27:28.900]   [SQUEAKING]\n",
      "[00:27:28.900 --> 00:27:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:30.900 --> 00:27:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:32.900 --> 00:27:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:34.900 --> 00:27:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:36.900 --> 00:27:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:38.900 --> 00:27:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:40.900 --> 00:27:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:42.900 --> 00:27:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:44.900 --> 00:27:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:46.900 --> 00:27:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:48.900 --> 00:27:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:50.900 --> 00:27:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:52.900 --> 00:27:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:54.900 --> 00:27:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:56.900 --> 00:27:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:27:58.900 --> 00:28:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:00.900 --> 00:28:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:02.900 --> 00:28:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:04.900 --> 00:28:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:06.900 --> 00:28:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:08.900 --> 00:28:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:10.900 --> 00:28:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:12.900 --> 00:28:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:14.900 --> 00:28:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:16.900 --> 00:28:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:18.900 --> 00:28:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:20.900 --> 00:28:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:22.900 --> 00:28:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:24.900 --> 00:28:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:26.900 --> 00:28:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:28.900 --> 00:28:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:30.900 --> 00:28:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:32.900 --> 00:28:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:34.900 --> 00:28:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:36.900 --> 00:28:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:38.900 --> 00:28:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:40.900 --> 00:28:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:42.900 --> 00:28:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:44.900 --> 00:28:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:46.900 --> 00:28:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:48.900 --> 00:28:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:50.900 --> 00:28:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:52.900 --> 00:28:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:54.900 --> 00:28:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:56.900 --> 00:28:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:28:58.900 --> 00:29:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:00.900 --> 00:29:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:02.900 --> 00:29:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:04.900 --> 00:29:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:06.900 --> 00:29:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:08.900 --> 00:29:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:10.900 --> 00:29:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:12.900 --> 00:29:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:14.900 --> 00:29:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:16.900 --> 00:29:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:18.900 --> 00:29:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:20.900 --> 00:29:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:22.900 --> 00:29:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:24.900 --> 00:29:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:26.900 --> 00:29:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:28.900 --> 00:29:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:30.900 --> 00:29:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:32.900 --> 00:29:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:34.900 --> 00:29:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:36.900 --> 00:29:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:38.900 --> 00:29:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:40.900 --> 00:29:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:42.900 --> 00:29:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:44.900 --> 00:29:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:46.900 --> 00:29:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:48.900 --> 00:29:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:50.900 --> 00:29:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:52.900 --> 00:29:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:54.900 --> 00:29:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:56.900 --> 00:29:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:29:58.900 --> 00:30:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:00.900 --> 00:30:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:02.900 --> 00:30:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:04.900 --> 00:30:06.900]   This more or less implies\n",
      "[00:30:06.900 --> 00:30:10.900]   that we just pay number of samples,\n",
      "[00:30:10.900 --> 00:30:14.900]   sample complexity that is n times a,\n",
      "[00:30:14.900 --> 00:30:16.900]   because this is the number of pools\n",
      "[00:30:16.900 --> 00:30:18.900]   we need for each arm, so the total number\n",
      "[00:30:18.900 --> 00:30:20.900]   of samples is like n times a,\n",
      "[00:30:20.900 --> 00:30:22.900]   which is equal to\n",
      "[00:30:22.900 --> 00:30:26.900]   this c a yota over epsilon square,\n",
      "[00:30:26.900 --> 00:30:30.900]   we will find epsilon optimal arm.\n",
      "[00:30:30.900 --> 00:30:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:32.900 --> 00:30:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:34.900 --> 00:30:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:36.900 --> 00:30:38.900]   Because after this many samples,\n",
      "[00:30:38.900 --> 00:30:40.900]   we guarantee every estimate is close\n",
      "[00:30:40.900 --> 00:30:42.900]   to the optimal by epsilon.\n",
      "[00:30:42.900 --> 00:30:44.900]   If I do the maximum over my estimate,\n",
      "[00:30:44.900 --> 00:30:46.900]   I guarantee this is epsilon optimal.\n",
      "[00:30:46.900 --> 00:30:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:48.900 --> 00:30:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:50.900 --> 00:30:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:52.900 --> 00:30:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:54.900 --> 00:30:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:30:56.900 --> 00:30:58.900]   Any questions about this theorem?\n",
      "[00:30:58.900 --> 00:30:59.900]   Yes?\n",
      "[00:30:59.900 --> 00:31:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:00.900 --> 00:31:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:01.900 --> 00:31:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:02.900 --> 00:31:04.900]   Yes, that's a very good question.\n",
      "[00:31:04.900 --> 00:31:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:06.900 --> 00:31:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:08.900 --> 00:31:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:10.900 --> 00:31:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:12.900 --> 00:31:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:14.900 --> 00:31:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:16.900 --> 00:31:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:18.900 --> 00:31:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:20.900 --> 00:31:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:22.900 --> 00:31:24.900]   And it's a number of poles for each arm.\n",
      "[00:31:24.900 --> 00:31:26.900]   For every arm I just put at times.\n",
      "[00:31:26.900 --> 00:31:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:28.900 --> 00:31:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:30.900 --> 00:31:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:32.900 --> 00:31:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:34.900 --> 00:31:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:37.900 --> 00:31:41.900]   So that's why a total number of samples is like n times the number of arms you have.\n",
      "[00:31:41.900 --> 00:31:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:43.900 --> 00:31:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:44.900 --> 00:31:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:46.900 --> 00:31:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:48.900 --> 00:31:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:50.900 --> 00:31:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:51.900 --> 00:31:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:52.900 --> 00:31:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:53.900 --> 00:31:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:54.900 --> 00:31:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:55.900 --> 00:31:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:56.900 --> 00:31:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:57.900 --> 00:31:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:58.900 --> 00:31:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:31:59.900 --> 00:32:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:00.900 --> 00:32:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:05.900 --> 00:32:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:06.900 --> 00:32:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:07.900 --> 00:32:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:08.900 --> 00:32:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:09.900 --> 00:32:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:10.900 --> 00:32:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:11.900 --> 00:32:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:12.900 --> 00:32:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:13.900 --> 00:32:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:14.900 --> 00:32:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:15.900 --> 00:32:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:16.900 --> 00:32:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:17.900 --> 00:32:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:18.900 --> 00:32:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:19.900 --> 00:32:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:20.900 --> 00:32:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:22.900 --> 00:32:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:23.900 --> 00:32:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:24.900 --> 00:32:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:25.900 --> 00:32:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:26.900 --> 00:32:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:27.900 --> 00:32:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:28.900 --> 00:32:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:29.900 --> 00:32:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:30.900 --> 00:32:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:31.900 --> 00:32:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:32.900 --> 00:32:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:33.900 --> 00:32:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:34.900 --> 00:32:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:35.900 --> 00:32:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:36.900 --> 00:32:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:37.900 --> 00:32:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:42.900 --> 00:32:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:43.900 --> 00:32:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:44.900 --> 00:32:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:45.900 --> 00:32:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:46.900 --> 00:32:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:47.900 --> 00:32:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:48.900 --> 00:32:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:55.900 --> 00:32:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:56.900 --> 00:32:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:57.900 --> 00:32:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:58.900 --> 00:32:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:32:59.900 --> 00:33:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:00.900 --> 00:33:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:01.900 --> 00:33:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:02.900 --> 00:33:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:03.900 --> 00:33:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:05.900 --> 00:33:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:06.900 --> 00:33:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:07.900 --> 00:33:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:08.900 --> 00:33:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:09.900 --> 00:33:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:10.900 --> 00:33:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:17.900 --> 00:33:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:18.900 --> 00:33:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:19.900 --> 00:33:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:20.900 --> 00:33:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:21.900 --> 00:33:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:22.900 --> 00:33:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:23.900 --> 00:33:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:24.900 --> 00:33:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:25.900 --> 00:33:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:26.900 --> 00:33:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:31.900 --> 00:33:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:32.900 --> 00:33:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:33.900 --> 00:33:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:34.900 --> 00:33:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:35.900 --> 00:33:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:36.900 --> 00:33:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:37.900 --> 00:33:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:38.900 --> 00:33:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:39.900 --> 00:33:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:40.900 --> 00:33:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:41.900 --> 00:33:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:42.900 --> 00:33:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:43.900 --> 00:33:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:44.900 --> 00:33:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:45.900 --> 00:33:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:46.900 --> 00:33:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:48.900 --> 00:33:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:49.900 --> 00:33:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:50.900 --> 00:33:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:51.900 --> 00:33:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:52.900 --> 00:33:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:53.900 --> 00:33:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:54.900 --> 00:33:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:55.900 --> 00:33:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:56.900 --> 00:33:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:57.900 --> 00:33:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:58.900 --> 00:33:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:33:59.900 --> 00:34:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:00.900 --> 00:34:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:15.900 --> 00:34:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:16.900 --> 00:34:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:17.900 --> 00:34:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:18.900 --> 00:34:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:19.900 --> 00:34:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:20.900 --> 00:34:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:21.900 --> 00:34:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:22.900 --> 00:34:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:23.900 --> 00:34:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:24.900 --> 00:34:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:25.900 --> 00:34:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:26.900 --> 00:34:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:27.900 --> 00:34:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:28.900 --> 00:34:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:29.900 --> 00:34:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:30.900 --> 00:34:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:31.900 --> 00:34:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:32.900 --> 00:34:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:33.900 --> 00:34:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:34.900 --> 00:34:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:36.900 --> 00:34:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:44.900 --> 00:34:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:45.900 --> 00:34:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:46.900 --> 00:34:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:47.900 --> 00:34:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:48.900 --> 00:34:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:49.900 --> 00:34:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:50.900 --> 00:34:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:51.900 --> 00:34:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:52.900 --> 00:34:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:34:53.900 --> 00:35:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:00.900 --> 00:35:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:01.900 --> 00:35:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:03.900 --> 00:35:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:04.900 --> 00:35:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:05.900 --> 00:35:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:06.900 --> 00:35:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:07.900 --> 00:35:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:08.900 --> 00:35:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:09.900 --> 00:35:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:10.900 --> 00:35:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:11.900 --> 00:35:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:12.900 --> 00:35:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:13.900 --> 00:35:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:14.900 --> 00:35:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:15.900 --> 00:35:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:16.900 --> 00:35:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:21.900 --> 00:35:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:22.900 --> 00:35:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:23.900 --> 00:35:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:24.900 --> 00:35:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:25.900 --> 00:35:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:26.900 --> 00:35:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:27.900 --> 00:35:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:28.900 --> 00:35:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:31.900 --> 00:35:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:32.900 --> 00:35:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:33.900 --> 00:35:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:34.900 --> 00:35:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:35.900 --> 00:35:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:36.900 --> 00:35:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:37.900 --> 00:35:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:38.900 --> 00:35:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:45.900 --> 00:35:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:46.900 --> 00:35:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:47.900 --> 00:35:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:48.900 --> 00:35:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:49.900 --> 00:35:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:50.900 --> 00:35:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:51.900 --> 00:35:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:52.900 --> 00:35:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:53.900 --> 00:35:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:54.900 --> 00:35:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:55.900 --> 00:35:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:56.900 --> 00:35:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:57.900 --> 00:35:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:58.900 --> 00:35:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:35:59.900 --> 00:36:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:00.900 --> 00:36:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:10.900 --> 00:36:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:11.900 --> 00:36:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:12.900 --> 00:36:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:13.900 --> 00:36:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:14.900 --> 00:36:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:15.900 --> 00:36:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:16.900 --> 00:36:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:17.900 --> 00:36:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:18.900 --> 00:36:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:19.900 --> 00:36:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:20.900 --> 00:36:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:21.900 --> 00:36:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:22.900 --> 00:36:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:23.900 --> 00:36:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:24.900 --> 00:36:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:25.900 --> 00:36:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:30.900 --> 00:36:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:31.900 --> 00:36:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:32.900 --> 00:36:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:33.900 --> 00:36:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:34.900 --> 00:36:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:35.900 --> 00:36:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:36.900 --> 00:36:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:37.900 --> 00:36:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:38.900 --> 00:36:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:39.900 --> 00:36:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:40.900 --> 00:36:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:41.900 --> 00:36:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:42.900 --> 00:36:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:43.900 --> 00:36:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:44.900 --> 00:36:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:36:45.900 --> 00:37:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:02.900 --> 00:37:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:03.900 --> 00:37:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:04.900 --> 00:37:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:05.900 --> 00:37:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:06.900 --> 00:37:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:07.900 --> 00:37:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:08.900 --> 00:37:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:09.900 --> 00:37:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:10.900 --> 00:37:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:11.900 --> 00:37:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:12.900 --> 00:37:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:13.900 --> 00:37:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:14.900 --> 00:37:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:15.900 --> 00:37:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:16.900 --> 00:37:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:17.900 --> 00:37:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:18.900 --> 00:37:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:19.900 --> 00:37:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:20.900 --> 00:37:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:21.900 --> 00:37:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:22.900 --> 00:37:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:23.900 --> 00:37:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:24.900 --> 00:37:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:37:43.900 --> 00:38:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:03.900 --> 00:38:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:04.900 --> 00:38:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:05.900 --> 00:38:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:06.900 --> 00:38:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:07.900 --> 00:38:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:08.900 --> 00:38:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:09.900 --> 00:38:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:10.900 --> 00:38:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:11.900 --> 00:38:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:12.900 --> 00:38:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:13.900 --> 00:38:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:14.900 --> 00:38:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:15.900 --> 00:38:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:16.900 --> 00:38:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:17.900 --> 00:38:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:29.900 --> 00:38:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:30.900 --> 00:38:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:31.900 --> 00:38:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:32.900 --> 00:38:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:33.900 --> 00:38:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:34.900 --> 00:38:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:35.900 --> 00:38:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:43.900 --> 00:38:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:45.900 --> 00:38:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:46.900 --> 00:38:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:47.900 --> 00:38:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:48.900 --> 00:38:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:49.900 --> 00:38:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:50.900 --> 00:38:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:51.900 --> 00:38:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:52.900 --> 00:38:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:53.900 --> 00:38:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:54.900 --> 00:38:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:55.900 --> 00:38:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:56.900 --> 00:38:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:57.900 --> 00:38:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:58.900 --> 00:38:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:38:59.900 --> 00:39:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:00.900 --> 00:39:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:01.900 --> 00:39:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:02.900 --> 00:39:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:03.900 --> 00:39:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:04.900 --> 00:39:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:05.900 --> 00:39:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:06.900 --> 00:39:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:07.900 --> 00:39:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:08.900 --> 00:39:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:09.900 --> 00:39:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:10.900 --> 00:39:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:11.900 --> 00:39:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:12.900 --> 00:39:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:13.900 --> 00:39:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:14.900 --> 00:39:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:15.900 --> 00:39:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:17.900 --> 00:39:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:18.900 --> 00:39:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:19.900 --> 00:39:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:20.900 --> 00:39:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:21.900 --> 00:39:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:22.900 --> 00:39:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:23.900 --> 00:39:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:24.900 --> 00:39:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:25.900 --> 00:39:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:26.900 --> 00:39:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:27.900 --> 00:39:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:28.900 --> 00:39:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:29.900 --> 00:39:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:30.900 --> 00:39:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:31.900 --> 00:39:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:32.900 --> 00:39:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:33.900 --> 00:39:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:40.900 --> 00:39:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:41.900 --> 00:39:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:42.900 --> 00:39:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:43.900 --> 00:39:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:44.900 --> 00:39:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:45.900 --> 00:39:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:46.900 --> 00:39:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:47.900 --> 00:39:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:48.900 --> 00:39:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:49.900 --> 00:39:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:50.900 --> 00:39:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:51.900 --> 00:39:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:52.900 --> 00:39:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:53.900 --> 00:39:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:54.900 --> 00:39:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:39:55.900 --> 00:40:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:03.900 --> 00:40:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:05.900 --> 00:40:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:06.900 --> 00:40:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:07.900 --> 00:40:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:08.900 --> 00:40:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:09.900 --> 00:40:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:10.900 --> 00:40:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:11.900 --> 00:40:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:12.900 --> 00:40:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:13.900 --> 00:40:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:14.900 --> 00:40:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:15.900 --> 00:40:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:16.900 --> 00:40:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:17.900 --> 00:40:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:18.900 --> 00:40:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:19.900 --> 00:40:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:24.900 --> 00:40:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:25.900 --> 00:40:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:26.900 --> 00:40:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:27.900 --> 00:40:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:28.900 --> 00:40:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:29.900 --> 00:40:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:30.900 --> 00:40:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:31.900 --> 00:40:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:32.900 --> 00:40:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:33.900 --> 00:40:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:34.900 --> 00:40:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:35.900 --> 00:40:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:36.900 --> 00:40:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:37.900 --> 00:40:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:38.900 --> 00:40:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:39.900 --> 00:40:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:41.900 --> 00:40:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:42.900 --> 00:40:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:43.900 --> 00:40:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:44.900 --> 00:40:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:45.900 --> 00:40:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:46.900 --> 00:40:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:47.900 --> 00:40:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:48.900 --> 00:40:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:49.900 --> 00:40:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:50.900 --> 00:40:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:51.900 --> 00:40:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:52.900 --> 00:40:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:53.900 --> 00:40:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:54.900 --> 00:40:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:55.900 --> 00:40:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:56.900 --> 00:40:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:57.900 --> 00:40:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:58.900 --> 00:40:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:59.900 --> 00:41:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:00.900 --> 00:41:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:01.900 --> 00:41:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:02.900 --> 00:41:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:03.900 --> 00:41:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:04.900 --> 00:41:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:05.900 --> 00:41:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:06.900 --> 00:41:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:07.900 --> 00:41:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:08.900 --> 00:41:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:09.900 --> 00:41:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:10.900 --> 00:41:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:11.900 --> 00:41:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:12.900 --> 00:41:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:25.900 --> 00:41:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:27.900 --> 00:41:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:28.900 --> 00:41:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:29.900 --> 00:41:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:30.900 --> 00:41:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:31.900 --> 00:41:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:32.900 --> 00:41:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:33.900 --> 00:41:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:34.900 --> 00:41:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:35.900 --> 00:41:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:36.900 --> 00:41:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:37.900 --> 00:41:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:38.900 --> 00:41:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:39.900 --> 00:41:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:40.900 --> 00:41:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:41.900 --> 00:41:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:42.900 --> 00:41:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:43.900 --> 00:41:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:44.900 --> 00:41:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:45.900 --> 00:41:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:46.900 --> 00:41:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:47.900 --> 00:41:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:48.900 --> 00:41:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:49.900 --> 00:41:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:50.900 --> 00:41:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:51.900 --> 00:41:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:52.900 --> 00:41:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:53.900 --> 00:41:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:41:54.900 --> 00:42:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:02.900 --> 00:42:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:10.900 --> 00:42:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:20.900 --> 00:42:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:24.900 --> 00:42:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:25.900 --> 00:42:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:26.900 --> 00:42:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:27.900 --> 00:42:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:28.900 --> 00:42:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:29.900 --> 00:42:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:30.900 --> 00:42:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:31.900 --> 00:42:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:32.900 --> 00:42:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:33.900 --> 00:42:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:34.900 --> 00:42:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:35.900 --> 00:42:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:36.900 --> 00:42:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:37.900 --> 00:42:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:38.900 --> 00:42:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:39.900 --> 00:42:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:40.900 --> 00:42:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:41.900 --> 00:42:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:42.900 --> 00:42:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:43.900 --> 00:42:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:44.900 --> 00:42:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:45.900 --> 00:42:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:46.900 --> 00:42:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:47.900 --> 00:42:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:48.900 --> 00:42:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:49.900 --> 00:42:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:50.900 --> 00:42:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:57.900 --> 00:42:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:58.900 --> 00:42:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:42:59.900 --> 00:43:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:00.900 --> 00:43:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:01.900 --> 00:43:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:02.900 --> 00:43:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:03.900 --> 00:43:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:04.900 --> 00:43:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:05.900 --> 00:43:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:06.900 --> 00:43:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:07.900 --> 00:43:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:08.900 --> 00:43:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:09.900 --> 00:43:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:10.900 --> 00:43:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:11.900 --> 00:43:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:12.900 --> 00:43:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:13.900 --> 00:43:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:14.900 --> 00:43:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:15.900 --> 00:43:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:17.900 --> 00:43:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:18.900 --> 00:43:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:19.900 --> 00:43:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:20.900 --> 00:43:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:21.900 --> 00:43:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:22.900 --> 00:43:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:23.900 --> 00:43:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:24.900 --> 00:43:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:25.900 --> 00:43:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:26.900 --> 00:43:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:27.900 --> 00:43:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:28.900 --> 00:43:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:29.900 --> 00:43:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:30.900 --> 00:43:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:31.900 --> 00:43:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:32.900 --> 00:43:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:33.900 --> 00:43:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:37.900 --> 00:43:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:38.900 --> 00:43:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:39.900 --> 00:43:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:40.900 --> 00:43:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:41.900 --> 00:43:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:42.900 --> 00:43:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:43.900 --> 00:43:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:44.900 --> 00:43:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:45.900 --> 00:43:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:46.900 --> 00:43:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:47.900 --> 00:43:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:48.900 --> 00:43:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:49.900 --> 00:43:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:50.900 --> 00:43:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:51.900 --> 00:43:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:52.900 --> 00:43:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:53.900 --> 00:43:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:54.900 --> 00:43:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:55.900 --> 00:43:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:56.900 --> 00:43:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:57.900 --> 00:43:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:58.900 --> 00:43:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:43:59.900 --> 00:44:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:00.900 --> 00:44:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:01.900 --> 00:44:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:02.900 --> 00:44:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:03.900 --> 00:44:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:04.900 --> 00:44:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:05.900 --> 00:44:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:06.900 --> 00:44:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:07.900 --> 00:44:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:09.900 --> 00:44:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:10.900 --> 00:44:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:11.900 --> 00:44:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:12.900 --> 00:44:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:13.900 --> 00:44:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:14.900 --> 00:44:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:15.900 --> 00:44:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:16.900 --> 00:44:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:17.900 --> 00:44:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:18.900 --> 00:44:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:19.900 --> 00:44:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:20.900 --> 00:44:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:21.900 --> 00:44:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:22.900 --> 00:44:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:23.900 --> 00:44:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:24.900 --> 00:44:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:25.900 --> 00:44:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:26.900 --> 00:44:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:27.900 --> 00:44:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:28.900 --> 00:44:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:29.900 --> 00:44:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:30.900 --> 00:44:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:31.900 --> 00:44:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:32.900 --> 00:44:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:33.900 --> 00:44:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:34.900 --> 00:44:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:35.900 --> 00:44:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:36.900 --> 00:44:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:37.900 --> 00:44:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:38.900 --> 00:44:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:39.900 --> 00:44:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:40.900 --> 00:44:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:41.900 --> 00:44:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:42.900 --> 00:44:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:43.900 --> 00:44:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:44.900 --> 00:44:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:45.900 --> 00:44:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:46.900 --> 00:44:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:47.900 --> 00:44:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:48.900 --> 00:44:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:49.900 --> 00:44:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:50.900 --> 00:44:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:51.900 --> 00:44:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:52.900 --> 00:44:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:53.900 --> 00:44:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:54.900 --> 00:44:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:55.900 --> 00:44:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:56.900 --> 00:44:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:57.900 --> 00:44:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:58.900 --> 00:44:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:44:59.900 --> 00:45:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:00.900 --> 00:45:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:01.900 --> 00:45:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:02.900 --> 00:45:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:03.900 --> 00:45:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:04.900 --> 00:45:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:05.900 --> 00:45:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:06.900 --> 00:45:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:07.900 --> 00:45:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:08.900 --> 00:45:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:09.900 --> 00:45:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:10.900 --> 00:45:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:11.900 --> 00:45:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:12.900 --> 00:45:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:13.900 --> 00:45:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:20.900 --> 00:45:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:28.900 --> 00:45:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:29.900 --> 00:45:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:30.900 --> 00:45:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:31.900 --> 00:45:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:32.900 --> 00:45:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:33.900 --> 00:45:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:34.900 --> 00:45:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:35.900 --> 00:45:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:36.900 --> 00:45:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:37.900 --> 00:45:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:38.900 --> 00:45:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:39.900 --> 00:45:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:40.900 --> 00:45:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:41.900 --> 00:45:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:42.900 --> 00:45:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:43.900 --> 00:45:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:44.900 --> 00:45:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:45.900 --> 00:45:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:46.900 --> 00:45:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:47.900 --> 00:45:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:48.900 --> 00:45:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:49.900 --> 00:45:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:50.900 --> 00:45:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:51.900 --> 00:45:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:52.900 --> 00:45:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:53.900 --> 00:45:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:54.900 --> 00:45:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:55.900 --> 00:45:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:56.900 --> 00:45:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:57.900 --> 00:45:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:45:58.900 --> 00:46:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:02.900 --> 00:46:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:03.900 --> 00:46:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:04.900 --> 00:46:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:05.900 --> 00:46:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:06.900 --> 00:46:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:07.900 --> 00:46:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:08.900 --> 00:46:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:09.900 --> 00:46:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:10.900 --> 00:46:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:11.900 --> 00:46:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:12.900 --> 00:46:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:13.900 --> 00:46:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:14.900 --> 00:46:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:15.900 --> 00:46:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:16.900 --> 00:46:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:17.900 --> 00:46:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:18.900 --> 00:46:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:19.900 --> 00:46:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:20.900 --> 00:46:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:21.900 --> 00:46:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:22.900 --> 00:46:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:23.900 --> 00:46:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:24.900 --> 00:46:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:25.900 --> 00:46:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:26.900 --> 00:46:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:27.900 --> 00:46:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:28.900 --> 00:46:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:29.900 --> 00:46:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:30.900 --> 00:46:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:31.900 --> 00:46:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:32.900 --> 00:46:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:33.900 --> 00:46:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:44.900 --> 00:46:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:46.900 --> 00:46:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:47.900 --> 00:46:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:48.900 --> 00:46:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:49.900 --> 00:46:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:50.900 --> 00:46:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:51.900 --> 00:46:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:52.900 --> 00:46:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:53.900 --> 00:46:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:54.900 --> 00:46:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:55.900 --> 00:46:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:56.900 --> 00:46:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:57.900 --> 00:46:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:58.900 --> 00:46:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:46:59.900 --> 00:47:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:00.900 --> 00:47:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:01.900 --> 00:47:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:02.900 --> 00:47:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:03.900 --> 00:47:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:04.900 --> 00:47:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:30.900 --> 00:47:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:31.900 --> 00:47:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:32.900 --> 00:47:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:33.900 --> 00:47:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:34.900 --> 00:47:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:35.900 --> 00:47:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:36.900 --> 00:47:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:37.900 --> 00:47:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:38.900 --> 00:47:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:39.900 --> 00:47:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:40.900 --> 00:47:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:41.900 --> 00:47:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:42.900 --> 00:47:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:43.900 --> 00:47:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:44.900 --> 00:47:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:45.900 --> 00:47:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:46.900 --> 00:47:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:47.900 --> 00:47:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:48.900 --> 00:47:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:49.900 --> 00:47:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:50.900 --> 00:47:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:51.900 --> 00:47:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:52.900 --> 00:47:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:53.900 --> 00:47:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:54.900 --> 00:47:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:55.900 --> 00:47:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:56.900 --> 00:47:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:57.900 --> 00:47:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:58.900 --> 00:47:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:47:59.900 --> 00:48:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:00.900 --> 00:48:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:01.900 --> 00:48:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:02.900 --> 00:48:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:03.900 --> 00:48:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:04.900 --> 00:48:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:05.900 --> 00:48:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:06.900 --> 00:48:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:07.900 --> 00:48:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:08.900 --> 00:48:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:09.900 --> 00:48:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:10.900 --> 00:48:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:11.900 --> 00:48:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:12.900 --> 00:48:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:13.900 --> 00:48:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:14.900 --> 00:48:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:15.900 --> 00:48:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:16.900 --> 00:48:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:17.900 --> 00:48:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:18.900 --> 00:48:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:19.900 --> 00:48:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:20.900 --> 00:48:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:21.900 --> 00:48:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:22.900 --> 00:48:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:23.900 --> 00:48:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:24.900 --> 00:48:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:25.900 --> 00:48:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:26.900 --> 00:48:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:27.900 --> 00:48:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:28.900 --> 00:48:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:29.900 --> 00:48:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:30.900 --> 00:48:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:31.900 --> 00:48:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:32.900 --> 00:48:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:33.900 --> 00:48:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:34.900 --> 00:48:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:35.900 --> 00:48:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:36.900 --> 00:48:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:37.900 --> 00:48:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:38.900 --> 00:48:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:39.900 --> 00:48:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:40.900 --> 00:48:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:41.900 --> 00:48:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:42.900 --> 00:48:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:43.900 --> 00:48:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:44.900 --> 00:48:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:45.900 --> 00:48:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:46.900 --> 00:48:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:47.900 --> 00:48:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:48.900 --> 00:48:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:49.900 --> 00:48:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:50.900 --> 00:48:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:56.900 --> 00:48:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:48:58.900 --> 00:49:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:05.900 --> 00:49:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:10.900 --> 00:49:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:18.900 --> 00:49:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:19.900 --> 00:49:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:20.900 --> 00:49:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:21.900 --> 00:49:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:22.900 --> 00:49:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:23.900 --> 00:49:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:24.900 --> 00:49:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:25.900 --> 00:49:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:26.900 --> 00:49:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:27.900 --> 00:49:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:28.900 --> 00:49:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:29.900 --> 00:49:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:30.900 --> 00:49:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:31.900 --> 00:49:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:32.900 --> 00:49:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:33.900 --> 00:49:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:34.900 --> 00:49:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:35.900 --> 00:49:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:36.900 --> 00:49:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:37.900 --> 00:49:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:38.900 --> 00:49:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:39.900 --> 00:49:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:40.900 --> 00:49:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:41.900 --> 00:49:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:42.900 --> 00:49:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:43.900 --> 00:49:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:44.900 --> 00:49:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:45.900 --> 00:49:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:46.900 --> 00:49:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:47.900 --> 00:49:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:48.900 --> 00:49:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:49.900 --> 00:49:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:50.900 --> 00:49:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:51.900 --> 00:49:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:52.900 --> 00:49:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:53.900 --> 00:49:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:54.900 --> 00:49:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:55.900 --> 00:49:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:56.900 --> 00:49:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:49:57.900 --> 00:50:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:05.900 --> 00:50:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:06.900 --> 00:50:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:07.900 --> 00:50:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:08.900 --> 00:50:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:09.900 --> 00:50:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:10.900 --> 00:50:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:11.900 --> 00:50:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:12.900 --> 00:50:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:13.900 --> 00:50:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:31.900 --> 00:50:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:32.900 --> 00:50:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:33.900 --> 00:50:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:34.900 --> 00:50:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:35.900 --> 00:50:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:36.900 --> 00:50:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:37.900 --> 00:50:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:38.900 --> 00:50:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:39.900 --> 00:50:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:40.900 --> 00:50:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:41.900 --> 00:50:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:42.900 --> 00:50:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:43.900 --> 00:50:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:44.900 --> 00:50:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:45.900 --> 00:50:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:46.900 --> 00:50:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:47.900 --> 00:50:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:55.900 --> 00:50:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:57.900 --> 00:50:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:58.900 --> 00:50:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:50:59.900 --> 00:51:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:00.900 --> 00:51:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:01.900 --> 00:51:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:02.900 --> 00:51:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:03.900 --> 00:51:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:04.900 --> 00:51:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:05.900 --> 00:51:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:06.900 --> 00:51:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:12.900 --> 00:51:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:13.900 --> 00:51:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:14.900 --> 00:51:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:15.900 --> 00:51:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:16.900 --> 00:51:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:17.900 --> 00:51:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:18.900 --> 00:51:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:19.900 --> 00:51:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:20.900 --> 00:51:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:21.900 --> 00:51:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:22.900 --> 00:51:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:23.900 --> 00:51:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:24.900 --> 00:51:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:25.900 --> 00:51:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:26.900 --> 00:51:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:27.900 --> 00:51:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:31.900 --> 00:51:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:32.900 --> 00:51:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:33.900 --> 00:51:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:34.900 --> 00:51:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:42.900 --> 00:51:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:43.900 --> 00:51:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:44.900 --> 00:51:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:45.900 --> 00:51:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:46.900 --> 00:51:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:47.900 --> 00:51:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:48.900 --> 00:51:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:49.900 --> 00:51:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:50.900 --> 00:51:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:51.900 --> 00:51:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:52.900 --> 00:51:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:53.900 --> 00:51:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:54.900 --> 00:51:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:55.900 --> 00:51:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:56.900 --> 00:51:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:57.900 --> 00:51:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:58.900 --> 00:51:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:51:59.900 --> 00:52:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:00.900 --> 00:52:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:01.900 --> 00:52:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:02.900 --> 00:52:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:03.900 --> 00:52:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:04.900 --> 00:52:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:05.900 --> 00:52:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:06.900 --> 00:52:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:07.900 --> 00:52:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:08.900 --> 00:52:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:09.900 --> 00:52:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:10.900 --> 00:52:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:11.900 --> 00:52:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:12.900 --> 00:52:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:13.900 --> 00:52:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:14.900 --> 00:52:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:15.900 --> 00:52:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:16.900 --> 00:52:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:17.900 --> 00:52:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:18.900 --> 00:52:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:19.900 --> 00:52:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:20.900 --> 00:52:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:21.900 --> 00:52:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:22.900 --> 00:52:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:23.900 --> 00:52:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:24.900 --> 00:52:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:25.900 --> 00:52:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:39.900 --> 00:52:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:40.900 --> 00:52:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:41.900 --> 00:52:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:42.900 --> 00:52:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:43.900 --> 00:52:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:44.900 --> 00:52:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:45.900 --> 00:52:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:46.900 --> 00:52:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:47.900 --> 00:52:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:48.900 --> 00:52:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:49.900 --> 00:52:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:50.900 --> 00:52:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:51.900 --> 00:52:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:52.900 --> 00:52:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:53.900 --> 00:52:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:54.900 --> 00:52:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:55.900 --> 00:52:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:56.900 --> 00:52:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:57.900 --> 00:52:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:58.900 --> 00:52:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:52:59.900 --> 00:53:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:00.900 --> 00:53:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:03.900 --> 00:53:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:04.900 --> 00:53:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:05.900 --> 00:53:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:06.900 --> 00:53:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:07.900 --> 00:53:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:08.900 --> 00:53:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:09.900 --> 00:53:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:10.900 --> 00:53:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:11.900 --> 00:53:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:12.900 --> 00:53:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:19.900 --> 00:53:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:20.900 --> 00:53:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:21.900 --> 00:53:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:22.900 --> 00:53:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:23.900 --> 00:53:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:24.900 --> 00:53:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:25.900 --> 00:53:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:26.900 --> 00:53:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:27.900 --> 00:53:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:28.900 --> 00:53:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:29.900 --> 00:53:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:30.900 --> 00:53:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:31.900 --> 00:53:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:32.900 --> 00:53:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:33.900 --> 00:53:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:34.900 --> 00:53:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:37.900 --> 00:53:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:38.900 --> 00:53:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:39.900 --> 00:53:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:40.900 --> 00:53:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:41.900 --> 00:53:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:42.900 --> 00:53:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:43.900 --> 00:53:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:44.900 --> 00:53:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:45.900 --> 00:53:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:46.900 --> 00:53:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:47.900 --> 00:53:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:48.900 --> 00:53:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:49.900 --> 00:53:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:50.900 --> 00:53:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:51.900 --> 00:53:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:52.900 --> 00:53:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:53:59.900 --> 00:54:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:02.900 --> 00:54:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:03.900 --> 00:54:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:04.900 --> 00:54:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:05.900 --> 00:54:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:06.900 --> 00:54:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:07.900 --> 00:54:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:08.900 --> 00:54:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:09.900 --> 00:54:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:10.900 --> 00:54:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:11.900 --> 00:54:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:12.900 --> 00:54:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:13.900 --> 00:54:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:14.900 --> 00:54:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:15.900 --> 00:54:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:16.900 --> 00:54:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:23.900 --> 00:54:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:24.900 --> 00:54:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:25.900 --> 00:54:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:26.900 --> 00:54:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:27.900 --> 00:54:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:28.900 --> 00:54:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:29.900 --> 00:54:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:30.900 --> 00:54:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:31.900 --> 00:54:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:32.900 --> 00:54:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:33.900 --> 00:54:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:34.900 --> 00:54:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:35.900 --> 00:54:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:36.900 --> 00:54:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:37.900 --> 00:54:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:38.900 --> 00:54:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:46.900 --> 00:54:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:47.900 --> 00:54:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:48.900 --> 00:54:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:49.900 --> 00:54:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:50.900 --> 00:54:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:51.900 --> 00:54:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:52.900 --> 00:54:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:53.900 --> 00:54:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:54.900 --> 00:54:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:55.900 --> 00:54:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:56.900 --> 00:54:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:57.900 --> 00:54:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:58.900 --> 00:54:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:54:59.900 --> 00:55:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:00.900 --> 00:55:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:01.900 --> 00:55:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:10.900 --> 00:55:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:11.900 --> 00:55:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:12.900 --> 00:55:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:13.900 --> 00:55:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:14.900 --> 00:55:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:15.900 --> 00:55:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:16.900 --> 00:55:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:17.900 --> 00:55:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:18.900 --> 00:55:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:19.900 --> 00:55:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:20.900 --> 00:55:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:21.900 --> 00:55:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:22.900 --> 00:55:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:23.900 --> 00:55:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:24.900 --> 00:55:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:25.900 --> 00:55:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:30.900 --> 00:55:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:31.900 --> 00:55:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:32.900 --> 00:55:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:33.900 --> 00:55:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:34.900 --> 00:55:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:35.900 --> 00:55:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:36.900 --> 00:55:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:37.900 --> 00:55:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:38.900 --> 00:55:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:39.900 --> 00:55:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:40.900 --> 00:55:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:41.900 --> 00:55:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:42.900 --> 00:55:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:43.900 --> 00:55:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:44.900 --> 00:55:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:45.900 --> 00:55:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:52.900 --> 00:55:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:53.900 --> 00:55:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:54.900 --> 00:55:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:55.900 --> 00:55:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:56.900 --> 00:55:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:57.900 --> 00:55:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:58.900 --> 00:55:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:55:59.900 --> 00:56:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:00.900 --> 00:56:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:01.900 --> 00:56:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:02.900 --> 00:56:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:03.900 --> 00:56:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:04.900 --> 00:56:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:05.900 --> 00:56:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:06.900 --> 00:56:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:07.900 --> 00:56:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:14.900 --> 00:56:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:15.900 --> 00:56:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:16.900 --> 00:56:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:17.900 --> 00:56:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:18.900 --> 00:56:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:19.900 --> 00:56:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:20.900 --> 00:56:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:21.900 --> 00:56:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:22.900 --> 00:56:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:23.900 --> 00:56:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:24.900 --> 00:56:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:25.900 --> 00:56:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:26.900 --> 00:56:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:27.900 --> 00:56:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:28.900 --> 00:56:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:29.900 --> 00:56:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:36.900 --> 00:56:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:37.900 --> 00:56:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:38.900 --> 00:56:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:39.900 --> 00:56:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:40.900 --> 00:56:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:41.900 --> 00:56:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:42.900 --> 00:56:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:43.900 --> 00:56:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:44.900 --> 00:56:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:45.900 --> 00:56:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:46.900 --> 00:56:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:47.900 --> 00:56:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:48.900 --> 00:56:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:49.900 --> 00:56:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:50.900 --> 00:56:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:51.900 --> 00:56:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:58.900 --> 00:56:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:56:59.900 --> 00:57:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:00.900 --> 00:57:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:01.900 --> 00:57:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:02.900 --> 00:57:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:03.900 --> 00:57:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:04.900 --> 00:57:05.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:05.900 --> 00:57:06.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:06.900 --> 00:57:07.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:07.900 --> 00:57:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:08.900 --> 00:57:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:09.900 --> 00:57:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:10.900 --> 00:57:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:11.900 --> 00:57:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:12.900 --> 00:57:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:13.900 --> 00:57:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:21.900 --> 00:57:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:22.900 --> 00:57:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:23.900 --> 00:57:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:24.900 --> 00:57:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:25.900 --> 00:57:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:26.900 --> 00:57:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:27.900 --> 00:57:28.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:28.900 --> 00:57:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:29.900 --> 00:57:30.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:30.900 --> 00:57:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:31.900 --> 00:57:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:32.900 --> 00:57:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:33.900 --> 00:57:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:34.900 --> 00:57:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:35.900 --> 00:57:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:36.900 --> 00:57:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:41.900 --> 00:57:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:42.900 --> 00:57:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:43.900 --> 00:57:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:44.900 --> 00:57:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:45.900 --> 00:57:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:46.900 --> 00:57:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:47.900 --> 00:57:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:48.900 --> 00:57:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:49.900 --> 00:57:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:50.900 --> 00:57:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:51.900 --> 00:57:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:52.900 --> 00:57:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:53.900 --> 00:57:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:54.900 --> 00:57:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:55.900 --> 00:57:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:57:56.900 --> 00:58:01.900]   Any questions about this high-level picture of the algorithm?\n",
      "[00:58:01.900 --> 00:58:08.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:08.900 --> 00:58:09.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:09.900 --> 00:58:10.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:10.900 --> 00:58:11.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:11.900 --> 00:58:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:12.900 --> 00:58:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:13.900 --> 00:58:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:14.900 --> 00:58:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:15.900 --> 00:58:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:16.900 --> 00:58:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:17.900 --> 00:58:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:18.900 --> 00:58:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:19.900 --> 00:58:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:20.900 --> 00:58:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:21.900 --> 00:58:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:22.900 --> 00:58:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:25.900 --> 00:58:28.900]   So, the algorithm is actually very simple.\n",
      "[00:58:28.900 --> 00:58:29.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:29.900 --> 00:58:31.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:31.900 --> 00:58:32.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:32.900 --> 00:58:33.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:33.900 --> 00:58:34.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:34.900 --> 00:58:35.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:35.900 --> 00:58:36.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:36.900 --> 00:58:37.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:37.900 --> 00:58:38.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:38.900 --> 00:58:39.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:39.900 --> 00:58:40.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:40.900 --> 00:58:41.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:41.900 --> 00:58:42.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:42.900 --> 00:58:43.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:43.900 --> 00:58:44.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:44.900 --> 00:58:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:49.900 --> 00:58:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:50.900 --> 00:58:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:51.900 --> 00:58:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:52.900 --> 00:58:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:53.900 --> 00:58:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:54.900 --> 00:58:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:55.900 --> 00:58:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:56.900 --> 00:58:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:57.900 --> 00:58:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:58.900 --> 00:58:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:58:59.900 --> 00:59:00.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:00.900 --> 00:59:01.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:01.900 --> 00:59:02.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:02.900 --> 00:59:03.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:03.900 --> 00:59:04.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:04.900 --> 00:59:12.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:12.900 --> 00:59:13.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:13.900 --> 00:59:14.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:14.900 --> 00:59:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:15.900 --> 00:59:16.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:16.900 --> 00:59:17.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:17.900 --> 00:59:18.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:18.900 --> 00:59:19.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:19.900 --> 00:59:20.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:20.900 --> 00:59:21.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:21.900 --> 00:59:22.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:22.900 --> 00:59:23.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:23.900 --> 00:59:24.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:24.900 --> 00:59:25.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:25.900 --> 00:59:26.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:26.900 --> 00:59:27.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:27.900 --> 00:59:45.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:45.900 --> 00:59:46.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:46.900 --> 00:59:47.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:47.900 --> 00:59:48.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:48.900 --> 00:59:49.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:49.900 --> 00:59:50.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:50.900 --> 00:59:51.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:51.900 --> 00:59:52.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:52.900 --> 00:59:53.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:53.900 --> 00:59:54.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:54.900 --> 00:59:55.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:55.900 --> 00:59:56.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:56.900 --> 00:59:57.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:57.900 --> 00:59:58.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:58.900 --> 00:59:59.900]   [NON-ENGLISH SPEECH]\n",
      "[00:59:59.900 --> 01:00:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:00.900 --> 01:00:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:01.900 --> 01:00:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:02.900 --> 01:00:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:03.900 --> 01:00:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:04.900 --> 01:00:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:05.900 --> 01:00:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:06.900 --> 01:00:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:07.900 --> 01:00:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:08.900 --> 01:00:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:09.900 --> 01:00:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:10.900 --> 01:00:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:11.900 --> 01:00:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:12.900 --> 01:00:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:13.900 --> 01:00:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:14.900 --> 01:00:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:15.900 --> 01:00:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:16.900 --> 01:00:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:17.900 --> 01:00:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:18.900 --> 01:00:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:19.900 --> 01:00:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:20.900 --> 01:00:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:21.900 --> 01:00:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:22.900 --> 01:00:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:23.900 --> 01:00:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:24.900 --> 01:00:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:25.900 --> 01:00:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:26.900 --> 01:00:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:27.900 --> 01:00:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:28.900 --> 01:00:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:29.900 --> 01:00:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:30.900 --> 01:00:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:31.900 --> 01:00:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:32.900 --> 01:00:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:33.900 --> 01:00:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:34.900 --> 01:00:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:35.900 --> 01:00:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:36.900 --> 01:00:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:37.900 --> 01:00:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:38.900 --> 01:00:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:39.900 --> 01:00:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:40.900 --> 01:00:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:41.900 --> 01:00:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:42.900 --> 01:00:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:43.900 --> 01:00:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:44.900 --> 01:00:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:46.900 --> 01:00:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:47.900 --> 01:00:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:48.900 --> 01:00:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:49.900 --> 01:00:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:50.900 --> 01:00:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:51.900 --> 01:00:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:52.900 --> 01:00:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:53.900 --> 01:00:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:54.900 --> 01:00:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:55.900 --> 01:00:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:56.900 --> 01:00:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:57.900 --> 01:00:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:58.900 --> 01:00:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:00:59.900 --> 01:01:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:00.900 --> 01:01:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:01.900 --> 01:01:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:06.900 --> 01:01:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:14.900 --> 01:01:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:16.900 --> 01:01:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:17.900 --> 01:01:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:18.900 --> 01:01:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:19.900 --> 01:01:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:20.900 --> 01:01:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:21.900 --> 01:01:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:22.900 --> 01:01:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:23.900 --> 01:01:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:24.900 --> 01:01:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:25.900 --> 01:01:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:26.900 --> 01:01:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:27.900 --> 01:01:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:28.900 --> 01:01:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:29.900 --> 01:01:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:31.900 --> 01:01:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:39.900 --> 01:01:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:40.900 --> 01:01:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:41.900 --> 01:01:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:42.900 --> 01:01:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:43.900 --> 01:01:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:44.900 --> 01:01:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:45.900 --> 01:01:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:46.900 --> 01:01:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:47.900 --> 01:01:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:48.900 --> 01:01:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:49.900 --> 01:01:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:50.900 --> 01:01:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:51.900 --> 01:01:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:52.900 --> 01:01:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:53.900 --> 01:01:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:58.900 --> 01:01:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:01:59.900 --> 01:02:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:00.900 --> 01:02:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:01.900 --> 01:02:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:02.900 --> 01:02:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:03.900 --> 01:02:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:04.900 --> 01:02:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:05.900 --> 01:02:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:06.900 --> 01:02:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:07.900 --> 01:02:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:08.900 --> 01:02:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:09.900 --> 01:02:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:10.900 --> 01:02:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:11.900 --> 01:02:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:12.900 --> 01:02:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:13.900 --> 01:02:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:34.900 --> 01:02:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:35.900 --> 01:02:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:36.900 --> 01:02:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:37.900 --> 01:02:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:38.900 --> 01:02:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:39.900 --> 01:02:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:40.900 --> 01:02:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:41.900 --> 01:02:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:43.900 --> 01:02:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:44.900 --> 01:02:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:45.900 --> 01:02:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:46.900 --> 01:02:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:47.900 --> 01:02:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:48.900 --> 01:02:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:49.900 --> 01:02:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:50.900 --> 01:02:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:51.900 --> 01:02:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:52.900 --> 01:02:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:53.900 --> 01:02:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:54.900 --> 01:02:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:55.900 --> 01:02:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:56.900 --> 01:02:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:57.900 --> 01:02:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:02:58.900 --> 01:03:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:08.900 --> 01:03:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:09.900 --> 01:03:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:10.900 --> 01:03:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:11.900 --> 01:03:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:13.900 --> 01:03:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:14.900 --> 01:03:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:15.900 --> 01:03:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:16.900 --> 01:03:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:17.900 --> 01:03:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:18.900 --> 01:03:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:19.900 --> 01:03:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:20.900 --> 01:03:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:21.900 --> 01:03:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:22.900 --> 01:03:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:23.900 --> 01:03:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:24.900 --> 01:03:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:26.900 --> 01:03:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:27.900 --> 01:03:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:28.900 --> 01:03:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:29.900 --> 01:03:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:30.900 --> 01:03:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:31.900 --> 01:03:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:32.900 --> 01:03:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:33.900 --> 01:03:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:34.900 --> 01:03:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:35.900 --> 01:03:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:36.900 --> 01:03:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:37.900 --> 01:03:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:38.900 --> 01:03:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:39.900 --> 01:03:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:40.900 --> 01:03:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:41.900 --> 01:03:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:55.900 --> 01:03:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:56.900 --> 01:03:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:57.900 --> 01:03:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:58.900 --> 01:03:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:03:59.900 --> 01:04:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:00.900 --> 01:04:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:01.900 --> 01:04:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:02.900 --> 01:04:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:03.900 --> 01:04:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:04.900 --> 01:04:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:05.900 --> 01:04:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:06.900 --> 01:04:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:07.900 --> 01:04:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:08.900 --> 01:04:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:09.900 --> 01:04:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:10.900 --> 01:04:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:11.900 --> 01:04:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:12.900 --> 01:04:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:13.900 --> 01:04:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:14.900 --> 01:04:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:15.900 --> 01:04:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:20.900 --> 01:04:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:21.900 --> 01:04:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:22.900 --> 01:04:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:23.900 --> 01:04:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:24.900 --> 01:04:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:25.900 --> 01:04:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:26.900 --> 01:04:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:27.900 --> 01:04:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:28.900 --> 01:04:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:29.900 --> 01:04:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:30.900 --> 01:04:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:31.900 --> 01:04:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:32.900 --> 01:04:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:33.900 --> 01:04:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:34.900 --> 01:04:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:35.900 --> 01:04:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:42.900 --> 01:04:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:43.900 --> 01:04:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:44.900 --> 01:04:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:45.900 --> 01:04:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:46.900 --> 01:04:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:49.900 --> 01:04:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:50.900 --> 01:04:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:51.900 --> 01:04:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:52.900 --> 01:04:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:53.900 --> 01:04:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:54.900 --> 01:04:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:55.900 --> 01:04:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:56.900 --> 01:04:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:57.900 --> 01:04:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:58.900 --> 01:04:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:04:59.900 --> 01:05:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:04.900 --> 01:05:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:09.900 --> 01:05:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:10.900 --> 01:05:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:11.900 --> 01:05:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:12.900 --> 01:05:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:16.900 --> 01:05:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:17.900 --> 01:05:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:18.900 --> 01:05:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:19.900 --> 01:05:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:20.900 --> 01:05:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:21.900 --> 01:05:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:22.900 --> 01:05:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:23.900 --> 01:05:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:24.900 --> 01:05:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:25.900 --> 01:05:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:26.900 --> 01:05:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:27.900 --> 01:05:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:28.900 --> 01:05:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:29.900 --> 01:05:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:30.900 --> 01:05:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:31.900 --> 01:05:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:32.900 --> 01:05:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:34.900 --> 01:05:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:35.900 --> 01:05:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:36.900 --> 01:05:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:37.900 --> 01:05:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:38.900 --> 01:05:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:39.900 --> 01:05:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:40.900 --> 01:05:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:41.900 --> 01:05:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:42.900 --> 01:05:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:43.900 --> 01:05:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:45.900 --> 01:05:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:47.900 --> 01:05:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:48.900 --> 01:05:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:49.900 --> 01:05:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:50.900 --> 01:05:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:51.900 --> 01:05:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:52.900 --> 01:05:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:53.900 --> 01:05:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:54.900 --> 01:05:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:55.900 --> 01:05:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:56.900 --> 01:05:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:57.900 --> 01:05:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:58.900 --> 01:05:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:05:59.900 --> 01:06:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:00.900 --> 01:06:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:01.900 --> 01:06:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:07.900 --> 01:06:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:09.900 --> 01:06:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:10.900 --> 01:06:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:11.900 --> 01:06:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:12.900 --> 01:06:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:13.900 --> 01:06:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:14.900 --> 01:06:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:15.900 --> 01:06:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:16.900 --> 01:06:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:17.900 --> 01:06:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:18.900 --> 01:06:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:19.900 --> 01:06:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:20.900 --> 01:06:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:21.900 --> 01:06:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:22.900 --> 01:06:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:23.900 --> 01:06:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:28.900 --> 01:06:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:29.900 --> 01:06:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:30.900 --> 01:06:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:31.900 --> 01:06:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:38.900 --> 01:06:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:39.900 --> 01:06:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:40.900 --> 01:06:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:41.900 --> 01:06:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:42.900 --> 01:06:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:43.900 --> 01:06:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:44.900 --> 01:06:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:45.900 --> 01:06:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:46.900 --> 01:06:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:47.900 --> 01:06:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:48.900 --> 01:06:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:49.900 --> 01:06:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:50.900 --> 01:06:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:51.900 --> 01:06:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:52.900 --> 01:06:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:53.900 --> 01:06:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:54.900 --> 01:06:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:55.900 --> 01:06:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:56.900 --> 01:06:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:57.900 --> 01:06:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:58.900 --> 01:06:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:06:59.900 --> 01:07:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:00.900 --> 01:07:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:01.900 --> 01:07:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:02.900 --> 01:07:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:03.900 --> 01:07:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:04.900 --> 01:07:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:05.900 --> 01:07:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:10.900 --> 01:07:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:12.900 --> 01:07:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:13.900 --> 01:07:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:14.900 --> 01:07:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:15.900 --> 01:07:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:16.900 --> 01:07:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:17.900 --> 01:07:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:18.900 --> 01:07:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:19.900 --> 01:07:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:20.900 --> 01:07:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:21.900 --> 01:07:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:22.900 --> 01:07:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:23.900 --> 01:07:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:24.900 --> 01:07:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:25.900 --> 01:07:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:26.900 --> 01:07:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:29.900 --> 01:07:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:30.900 --> 01:07:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:31.900 --> 01:07:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:32.900 --> 01:07:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:34.900 --> 01:07:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:35.900 --> 01:07:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:36.900 --> 01:07:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:37.900 --> 01:07:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:38.900 --> 01:07:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:39.900 --> 01:07:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:40.900 --> 01:07:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:41.900 --> 01:07:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:42.900 --> 01:07:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:43.900 --> 01:07:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:44.900 --> 01:07:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:45.900 --> 01:07:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:49.900 --> 01:07:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:51.900 --> 01:07:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:53.900 --> 01:07:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:54.900 --> 01:07:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:55.900 --> 01:07:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:56.900 --> 01:07:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:57.900 --> 01:07:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:58.900 --> 01:07:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:07:59.900 --> 01:08:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:00.900 --> 01:08:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:01.900 --> 01:08:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:02.900 --> 01:08:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:03.900 --> 01:08:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:04.900 --> 01:08:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:05.900 --> 01:08:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:06.900 --> 01:08:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:16.900 --> 01:08:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:17.900 --> 01:08:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:19.900 --> 01:08:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:20.900 --> 01:08:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:21.900 --> 01:08:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:22.900 --> 01:08:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:23.900 --> 01:08:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:24.900 --> 01:08:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:25.900 --> 01:08:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:26.900 --> 01:08:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:27.900 --> 01:08:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:28.900 --> 01:08:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:29.900 --> 01:08:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:30.900 --> 01:08:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:31.900 --> 01:08:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:32.900 --> 01:08:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:37.900 --> 01:08:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:38.900 --> 01:08:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:39.900 --> 01:08:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:40.900 --> 01:08:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:41.900 --> 01:08:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:42.900 --> 01:08:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:43.900 --> 01:08:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:44.900 --> 01:08:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:45.900 --> 01:08:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:46.900 --> 01:08:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:47.900 --> 01:08:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:48.900 --> 01:08:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:49.900 --> 01:08:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:50.900 --> 01:08:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:51.900 --> 01:08:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:52.900 --> 01:08:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:54.900 --> 01:08:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:55.900 --> 01:08:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:56.900 --> 01:08:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:57.900 --> 01:08:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:58.900 --> 01:08:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:08:59.900 --> 01:09:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:00.900 --> 01:09:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:01.900 --> 01:09:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:02.900 --> 01:09:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:03.900 --> 01:09:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:04.900 --> 01:09:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:05.900 --> 01:09:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:06.900 --> 01:09:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:08.900 --> 01:09:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:09.900 --> 01:09:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:10.900 --> 01:09:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:11.900 --> 01:09:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:12.900 --> 01:09:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:13.900 --> 01:09:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:14.900 --> 01:09:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:15.900 --> 01:09:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:16.900 --> 01:09:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:17.900 --> 01:09:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:18.900 --> 01:09:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:19.900 --> 01:09:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:20.900 --> 01:09:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:21.900 --> 01:09:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:22.900 --> 01:09:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:23.900 --> 01:09:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:25.900 --> 01:09:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:27.900 --> 01:09:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:29.900 --> 01:09:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:31.900 --> 01:09:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:32.900 --> 01:09:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:33.900 --> 01:09:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:34.900 --> 01:09:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:35.900 --> 01:09:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:36.900 --> 01:09:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:37.900 --> 01:09:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:38.900 --> 01:09:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:39.900 --> 01:09:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:40.900 --> 01:09:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:41.900 --> 01:09:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:42.900 --> 01:09:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:43.900 --> 01:09:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:56.900 --> 01:09:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:57.900 --> 01:09:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:58.900 --> 01:09:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:09:59.900 --> 01:10:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:00.900 --> 01:10:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:01.900 --> 01:10:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:02.900 --> 01:10:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:03.900 --> 01:10:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:04.900 --> 01:10:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:05.900 --> 01:10:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:06.900 --> 01:10:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:07.900 --> 01:10:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:08.900 --> 01:10:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:09.900 --> 01:10:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:10.900 --> 01:10:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:11.900 --> 01:10:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:12.900 --> 01:10:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:13.900 --> 01:10:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:14.900 --> 01:10:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:15.900 --> 01:10:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:16.900 --> 01:10:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:17.900 --> 01:10:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:18.900 --> 01:10:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:19.900 --> 01:10:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:20.900 --> 01:10:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:21.900 --> 01:10:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:22.900 --> 01:10:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:23.900 --> 01:10:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:24.900 --> 01:10:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:25.900 --> 01:10:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:26.900 --> 01:10:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:28.900 --> 01:10:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:29.900 --> 01:10:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:30.900 --> 01:10:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:31.900 --> 01:10:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:32.900 --> 01:10:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:33.900 --> 01:10:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:34.900 --> 01:10:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:35.900 --> 01:10:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:36.900 --> 01:10:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:37.900 --> 01:10:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:38.900 --> 01:10:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:39.900 --> 01:10:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:40.900 --> 01:10:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:41.900 --> 01:10:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:42.900 --> 01:10:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:43.900 --> 01:10:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:44.900 --> 01:10:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:54.900 --> 01:10:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:55.900 --> 01:10:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:56.900 --> 01:10:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:57.900 --> 01:10:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:58.900 --> 01:10:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:10:59.900 --> 01:11:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:00.900 --> 01:11:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:01.900 --> 01:11:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:02.900 --> 01:11:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:03.900 --> 01:11:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:04.900 --> 01:11:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:05.900 --> 01:11:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:06.900 --> 01:11:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:07.900 --> 01:11:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:08.900 --> 01:11:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:09.900 --> 01:11:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:14.900 --> 01:11:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:15.900 --> 01:11:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:16.900 --> 01:11:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:17.900 --> 01:11:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:18.900 --> 01:11:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:19.900 --> 01:11:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:20.900 --> 01:11:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:21.900 --> 01:11:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:22.900 --> 01:11:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:23.900 --> 01:11:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:24.900 --> 01:11:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:25.900 --> 01:11:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:26.900 --> 01:11:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:27.900 --> 01:11:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:28.900 --> 01:11:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:29.900 --> 01:11:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:30.900 --> 01:11:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:31.900 --> 01:11:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:32.900 --> 01:11:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:33.900 --> 01:11:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:43.900 --> 01:11:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:44.900 --> 01:11:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:45.900 --> 01:11:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:46.900 --> 01:11:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:47.900 --> 01:11:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:48.900 --> 01:11:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:49.900 --> 01:11:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:50.900 --> 01:11:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:51.900 --> 01:11:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:52.900 --> 01:11:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:53.900 --> 01:11:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:54.900 --> 01:11:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:11:59.900 --> 01:12:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:00.900 --> 01:12:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:01.900 --> 01:12:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:02.900 --> 01:12:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:03.900 --> 01:12:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:23.900 --> 01:12:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:24.900 --> 01:12:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:25.900 --> 01:12:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:26.900 --> 01:12:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:27.900 --> 01:12:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:28.900 --> 01:12:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:29.900 --> 01:12:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:30.900 --> 01:12:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:31.900 --> 01:12:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:32.900 --> 01:12:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:33.900 --> 01:12:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:34.900 --> 01:12:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:35.900 --> 01:12:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:36.900 --> 01:12:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:37.900 --> 01:12:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:38.900 --> 01:12:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:39.900 --> 01:12:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:40.900 --> 01:12:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:41.900 --> 01:12:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:42.900 --> 01:12:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:43.900 --> 01:12:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:44.900 --> 01:12:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:45.900 --> 01:12:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:46.900 --> 01:12:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:48.900 --> 01:12:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:49.900 --> 01:12:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:50.900 --> 01:12:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:51.900 --> 01:12:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:52.900 --> 01:12:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:53.900 --> 01:12:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:54.900 --> 01:12:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:55.900 --> 01:12:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:56.900 --> 01:12:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:57.900 --> 01:12:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:58.900 --> 01:12:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:12:59.900 --> 01:13:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:00.900 --> 01:13:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:01.900 --> 01:13:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:02.900 --> 01:13:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:03.900 --> 01:13:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:04.900 --> 01:13:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:10.900 --> 01:13:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:11.900 --> 01:13:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:12.900 --> 01:13:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:13.900 --> 01:13:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:14.900 --> 01:13:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:15.900 --> 01:13:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:16.900 --> 01:13:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:17.900 --> 01:13:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:18.900 --> 01:13:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:19.900 --> 01:13:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:20.900 --> 01:13:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:21.900 --> 01:13:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:22.900 --> 01:13:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:23.900 --> 01:13:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:24.900 --> 01:13:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:25.900 --> 01:13:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:31.900 --> 01:13:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:32.900 --> 01:13:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:33.900 --> 01:13:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:34.900 --> 01:13:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:35.900 --> 01:13:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:36.900 --> 01:13:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:37.900 --> 01:13:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:38.900 --> 01:13:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:39.900 --> 01:13:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:40.900 --> 01:13:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:41.900 --> 01:13:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:42.900 --> 01:13:43.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:43.900 --> 01:13:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:44.900 --> 01:13:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:45.900 --> 01:13:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:46.900 --> 01:13:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:53.900 --> 01:13:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:54.900 --> 01:13:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:55.900 --> 01:13:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:56.900 --> 01:13:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:57.900 --> 01:13:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:58.900 --> 01:13:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:13:59.900 --> 01:14:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:00.900 --> 01:14:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:01.900 --> 01:14:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:02.900 --> 01:14:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:03.900 --> 01:14:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:04.900 --> 01:14:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:05.900 --> 01:14:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:06.900 --> 01:14:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:07.900 --> 01:14:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:08.900 --> 01:14:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:15.900 --> 01:14:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:16.900 --> 01:14:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:17.900 --> 01:14:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:18.900 --> 01:14:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:19.900 --> 01:14:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:20.900 --> 01:14:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:21.900 --> 01:14:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:22.900 --> 01:14:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:23.900 --> 01:14:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:24.900 --> 01:14:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:25.900 --> 01:14:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:26.900 --> 01:14:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:27.900 --> 01:14:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:28.900 --> 01:14:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:29.900 --> 01:14:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:30.900 --> 01:14:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:40.900 --> 01:14:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:44.900 --> 01:14:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:45.900 --> 01:14:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:46.900 --> 01:14:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:47.900 --> 01:14:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:48.900 --> 01:14:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:49.900 --> 01:14:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:50.900 --> 01:14:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:51.900 --> 01:14:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:52.900 --> 01:14:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:53.900 --> 01:14:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:54.900 --> 01:14:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:55.900 --> 01:14:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:56.900 --> 01:14:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:57.900 --> 01:14:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:58.900 --> 01:14:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:14:59.900 --> 01:15:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:00.900 --> 01:15:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:01.900 --> 01:15:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:02.900 --> 01:15:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:03.900 --> 01:15:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:04.900 --> 01:15:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:05.900 --> 01:15:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:06.900 --> 01:15:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:07.900 --> 01:15:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:08.900 --> 01:15:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:09.900 --> 01:15:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:10.900 --> 01:15:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:11.900 --> 01:15:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:12.900 --> 01:15:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:13.900 --> 01:15:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:14.900 --> 01:15:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:24.900 --> 01:15:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:25.900 --> 01:15:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:26.900 --> 01:15:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:27.900 --> 01:15:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:28.900 --> 01:15:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:29.900 --> 01:15:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:30.900 --> 01:15:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:31.900 --> 01:15:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:32.900 --> 01:15:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:33.900 --> 01:15:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:34.900 --> 01:15:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:35.900 --> 01:15:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:36.900 --> 01:15:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:37.900 --> 01:15:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:38.900 --> 01:15:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:39.900 --> 01:15:44.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:44.900 --> 01:15:45.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:45.900 --> 01:15:46.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:46.900 --> 01:15:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:47.900 --> 01:15:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:48.900 --> 01:15:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:49.900 --> 01:15:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:50.900 --> 01:15:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:51.900 --> 01:15:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:52.900 --> 01:15:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:53.900 --> 01:15:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:54.900 --> 01:15:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:55.900 --> 01:15:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:56.900 --> 01:15:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:57.900 --> 01:15:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:58.900 --> 01:15:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:15:59.900 --> 01:16:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:04.900 --> 01:16:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:05.900 --> 01:16:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:06.900 --> 01:16:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:07.900 --> 01:16:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:08.900 --> 01:16:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:09.900 --> 01:16:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:10.900 --> 01:16:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:11.900 --> 01:16:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:12.900 --> 01:16:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:13.900 --> 01:16:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:14.900 --> 01:16:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:15.900 --> 01:16:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:16.900 --> 01:16:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:17.900 --> 01:16:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:18.900 --> 01:16:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:19.900 --> 01:16:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:24.900 --> 01:16:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:25.900 --> 01:16:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:26.900 --> 01:16:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:27.900 --> 01:16:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:28.900 --> 01:16:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:29.900 --> 01:16:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:30.900 --> 01:16:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:31.900 --> 01:16:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:32.900 --> 01:16:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:33.900 --> 01:16:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:34.900 --> 01:16:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:35.900 --> 01:16:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:36.900 --> 01:16:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:37.900 --> 01:16:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:38.900 --> 01:16:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:39.900 --> 01:16:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:53.900 --> 01:16:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:54.900 --> 01:16:55.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:55.900 --> 01:16:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:56.900 --> 01:16:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:57.900 --> 01:16:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:58.900 --> 01:16:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:16:59.900 --> 01:17:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:00.900 --> 01:17:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:01.900 --> 01:17:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:02.900 --> 01:17:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:03.900 --> 01:17:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:04.900 --> 01:17:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:05.900 --> 01:17:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:06.900 --> 01:17:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:07.900 --> 01:17:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:08.900 --> 01:17:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:09.900 --> 01:17:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:10.900 --> 01:17:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:11.900 --> 01:17:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:12.900 --> 01:17:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:13.900 --> 01:17:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:14.900 --> 01:17:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:15.900 --> 01:17:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:16.900 --> 01:17:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:18.900 --> 01:17:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:19.900 --> 01:17:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:20.900 --> 01:17:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:21.900 --> 01:17:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:22.900 --> 01:17:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:23.900 --> 01:17:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:24.900 --> 01:17:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:25.900 --> 01:17:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:27.900 --> 01:17:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:28.900 --> 01:17:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:29.900 --> 01:17:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:30.900 --> 01:17:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:31.900 --> 01:17:32.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:32.900 --> 01:17:33.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:33.900 --> 01:17:34.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:34.900 --> 01:17:35.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:35.900 --> 01:17:36.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:36.900 --> 01:17:37.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:37.900 --> 01:17:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:38.900 --> 01:17:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:39.900 --> 01:17:40.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:40.900 --> 01:17:41.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:41.900 --> 01:17:42.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:42.900 --> 01:17:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:49.900 --> 01:17:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:50.900 --> 01:17:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:51.900 --> 01:17:52.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:52.900 --> 01:17:53.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:53.900 --> 01:17:54.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:54.900 --> 01:17:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:56.900 --> 01:17:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:57.900 --> 01:17:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:58.900 --> 01:17:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:17:59.900 --> 01:18:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:00.900 --> 01:18:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:01.900 --> 01:18:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:02.900 --> 01:18:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:03.900 --> 01:18:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:04.900 --> 01:18:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:05.900 --> 01:18:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:10.900 --> 01:18:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:11.900 --> 01:18:12.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:12.900 --> 01:18:13.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:13.900 --> 01:18:14.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:14.900 --> 01:18:15.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:15.900 --> 01:18:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:16.900 --> 01:18:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:17.900 --> 01:18:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:18.900 --> 01:18:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:19.900 --> 01:18:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:20.900 --> 01:18:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:21.900 --> 01:18:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:22.900 --> 01:18:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:23.900 --> 01:18:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:24.900 --> 01:18:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:27.900 --> 01:18:47.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:47.900 --> 01:18:48.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:48.900 --> 01:18:49.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:49.900 --> 01:18:50.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:50.900 --> 01:18:51.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:51.900 --> 01:18:56.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:56.900 --> 01:18:57.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:57.900 --> 01:18:58.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:58.900 --> 01:18:59.900]   [NON-ENGLISH SPEECH]\n",
      "[01:18:59.900 --> 01:19:00.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:00.900 --> 01:19:01.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:01.900 --> 01:19:02.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:02.900 --> 01:19:03.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:03.900 --> 01:19:04.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:04.900 --> 01:19:05.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:05.900 --> 01:19:06.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:06.900 --> 01:19:07.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:07.900 --> 01:19:08.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:08.900 --> 01:19:09.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:09.900 --> 01:19:10.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:10.900 --> 01:19:11.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:11.900 --> 01:19:16.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:16.900 --> 01:19:17.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:17.900 --> 01:19:18.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:18.900 --> 01:19:19.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:19.900 --> 01:19:20.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:20.900 --> 01:19:21.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:21.900 --> 01:19:22.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:22.900 --> 01:19:23.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:23.900 --> 01:19:24.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:24.900 --> 01:19:25.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:25.900 --> 01:19:26.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:26.900 --> 01:19:27.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:27.900 --> 01:19:28.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:28.900 --> 01:19:29.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:29.900 --> 01:19:30.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:30.900 --> 01:19:31.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:31.900 --> 01:19:38.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:38.900 --> 01:19:39.900]   [NON-ENGLISH SPEECH]\n",
      "[01:19:39.900 --> 01:19:42.900]   This essentially wrap up the everything we want to talk about\n",
      "[01:19:42.900 --> 01:19:44.900]   for multi-armed Bennett.\n",
      "[01:19:44.900 --> 01:19:48.900]   In the next lecture, we will extend this UCB idea,\n",
      "[01:19:48.900 --> 01:19:50.900]   optimism idea to reinforce learning and how --\n",
      "[01:19:50.900 --> 01:19:53.900]   and see like how this idea will actually help us,\n",
      "[01:19:53.900 --> 01:19:58.900]   avoid the counter lock, the hard instance for exploration\n",
      "[01:19:58.900 --> 01:20:00.900]   we talked about in the last lecture.\n",
      "[01:20:00.900 --> 01:20:03.480]   (upbeat music)\n",
      "[01:20:03.480 --> 01:20:05.480]   (chime)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/lCCT9JGkLw8.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/lCCT9JGkLw8.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/lCCT9JGkLw8.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/lCCT9JGkLw8.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1271.90 ms\n",
      "whisper_print_timings:     fallbacks =   2 p /   4 h\n",
      "whisper_print_timings:      mel time =  2705.07 ms\n",
      "whisper_print_timings:   sample time = 63481.90 ms / 144765 runs (    0.44 ms per run)\n",
      "whisper_print_timings:   encode time =   388.35 ms /   221 runs (    1.76 ms per run)\n",
      "whisper_print_timings:   decode time =  4137.60 ms /  2384 runs (    1.74 ms per run)\n",
      "whisper_print_timings:   batchd time = 81951.70 ms / 141694 runs (    0.58 ms per run)\n",
      "whisper_print_timings:   prompt time = 11198.25 ms / 49154 runs (    0.23 ms per run)\n",
      "whisper_print_timings:    total time = 165551.11 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=739Ddvx1OIU started\n",
      "739Ddvx1OIU\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/739Ddvx1OIU.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/739Ddvx1OIU.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:15:47.61, start: 0.000000, bitrate: 264 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 164 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/739Ddvx1OIU.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  142113kB time=01:15:47.60 bitrate= 256.0kbits/s speed=1.43e+03x    \n",
      "video:0kB audio:142113kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000054%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/739Ddvx1OIU.wav' (72761679 samples, 4547.6 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.080]   [MUSIC PLAYING]\n",
      "[00:00:03.080 --> 00:00:06.880]   In the last lecture, we talk about the simulator setting,\n",
      "[00:00:06.880 --> 00:00:08.080]   or a generative model.\n",
      "[00:00:08.080 --> 00:00:15.960]   I think the simulator setting is the most important thing.\n",
      "[00:00:15.960 --> 00:00:17.040]   So we have a simulator.\n",
      "[00:00:17.040 --> 00:00:21.840]   So we can query any state action and step edge pair.\n",
      "[00:00:21.840 --> 00:00:26.720]   And then we get a next stage and the reward as a feedback.\n",
      "[00:00:26.720 --> 00:00:30.800]   So we spend the most time focused on the first algorithm\n",
      "[00:00:30.800 --> 00:00:31.720]   called a value iteration.\n",
      "[00:00:31.720 --> 00:00:37.200]   In this course, we'll just--\n",
      "[00:00:37.200 --> 00:00:39.000]   as a strategy called VI.\n",
      "[00:00:39.000 --> 00:00:40.920]   And at the very end of the last lecture,\n",
      "[00:00:40.920 --> 00:00:43.640]   we talk about an R9 algorithm called Q-learning.\n",
      "[00:00:43.640 --> 00:00:48.320]   The major difference is in the value iteration,\n",
      "[00:00:48.320 --> 00:00:50.040]   we do the batch update.\n",
      "[00:00:50.040 --> 00:00:51.600]   Well, in the Q-learning, we're trying\n",
      "[00:00:51.600 --> 00:00:56.040]   to do the online update, or like incremental update.\n",
      "[00:00:56.040 --> 00:00:57.680]   This is like, we crack all the data,\n",
      "[00:00:57.680 --> 00:00:59.280]   and we do one pass-off update.\n",
      "[00:00:59.280 --> 00:01:01.400]   This is like, as long as we crack one data,\n",
      "[00:01:01.400 --> 00:01:03.520]   we do a new update to it.\n",
      "[00:01:03.520 --> 00:01:07.360]   So today, we will be a little bit more concrete\n",
      "[00:01:07.360 --> 00:01:09.000]   on the Q-learning algorithm.\n",
      "[00:01:09.000 --> 00:01:13.440]   And we will talk about the results of what\n",
      "[00:01:13.440 --> 00:01:15.600]   we can get for Q-learning.\n",
      "[00:01:15.600 --> 00:01:17.320]   So we'll talk about the formal algorithm.\n",
      "[00:01:17.320 --> 00:01:34.320]   So Q-learning algorithm need to input a sequence.\n",
      "[00:01:34.320 --> 00:01:45.360]   As we said, last time is like the learning rate.\n",
      "[00:01:46.280 --> 00:01:56.920]   Which is a sequence of R for T. That is T from 1 to infinity.\n",
      "[00:01:56.920 --> 00:01:59.080]   Essentially, R for T is trade-off,\n",
      "[00:01:59.080 --> 00:02:02.240]   like how many fractions we trust the previous Q value.\n",
      "[00:02:02.240 --> 00:02:04.240]   That is 1 minus R for T fraction.\n",
      "[00:02:04.240 --> 00:02:07.280]   And R for T times the new update.\n",
      "[00:02:07.280 --> 00:02:10.320]   So you can think this is like a sequence of learning rate.\n",
      "[00:02:10.320 --> 00:02:12.960]   And we'll talk about how we choose this learning rate\n",
      "[00:02:12.960 --> 00:02:13.880]   at the very end.\n",
      "[00:02:14.880 --> 00:02:17.360]   So in terms of the main algorithm,\n",
      "[00:02:17.360 --> 00:02:31.520]   we first initialize with a Q-H-0-SA.\n",
      "[00:02:31.520 --> 00:02:33.840]   You can initialize it to be 0, or actually,\n",
      "[00:02:33.840 --> 00:02:39.560]   you can initialize it to be anything for any S-H.\n",
      "[00:02:39.560 --> 00:02:42.000]   That is, I will just initially assign some value\n",
      "[00:02:42.000 --> 00:02:45.960]   to the Q value function.\n",
      "[00:02:45.960 --> 00:02:47.880]   Then Q-learning algorithm iteratively\n",
      "[00:02:47.880 --> 00:02:49.880]   doing the following.\n",
      "[00:02:49.880 --> 00:02:56.840]   That is for T from 1 to da-da-da.\n",
      "[00:02:56.840 --> 00:03:11.600]   And then inside the loop, we say for H equal to 1 to capital H.\n",
      "[00:03:11.600 --> 00:03:22.520]   For all state action in this step,\n",
      "[00:03:22.520 --> 00:03:38.480]   we will basically query the simulator, this S-A-H.\n",
      "[00:03:38.480 --> 00:03:41.840]   And the important thing is now we only get one sample\n",
      "[00:03:41.840 --> 00:03:42.840]   of the next states.\n",
      "[00:03:42.840 --> 00:04:07.080]   And then we will do the following update.\n",
      "[00:04:07.080 --> 00:04:18.600]   That is Q-H-T-SA is equal to 1 minus r for T.\n",
      "[00:04:18.600 --> 00:04:20.720]   This is like the same update we said last time.\n",
      "[00:04:20.720 --> 00:04:28.120]   That is 1 minus r for T times the previous Q value\n",
      "[00:04:28.120 --> 00:04:31.720]   plus r for T times the new update.\n",
      "[00:04:31.720 --> 00:05:01.200]   OK, this is for all state action pair.\n",
      "[00:05:01.200 --> 00:05:22.760]   Then we will also say for all state in this step just\n",
      "[00:05:22.760 --> 00:05:40.960]   do V-H-T-S equal to max A of Q-H-T-S S-A.\n",
      "[00:05:40.960 --> 00:05:43.720]   So the indent is the same.\n",
      "[00:05:43.720 --> 00:05:46.760]   So this follow-up is inside the two follow-up here.\n",
      "[00:05:50.840 --> 00:05:53.320]   So what we essentially have is the four algorithm,\n",
      "[00:05:53.320 --> 00:05:55.920]   which the key step is what we talked about last time\n",
      "[00:05:55.920 --> 00:05:58.280]   where we do the incremental updates.\n",
      "[00:05:58.280 --> 00:06:01.760]   That we will basically assign Q value to be 1 minus r\n",
      "[00:06:01.760 --> 00:06:04.280]   for fraction of the previous Q value plus the r\n",
      "[00:06:04.280 --> 00:06:05.760]   for fraction of the new update.\n",
      "[00:06:05.760 --> 00:06:24.880]   And the remaining thing is just after we get in a Q value,\n",
      "[00:06:24.880 --> 00:06:26.640]   we will calculate the V value that's\n",
      "[00:06:26.640 --> 00:06:30.040]   equal to the max of the Q value taking over action.\n",
      "[00:06:30.040 --> 00:06:43.560]   So we can think this algorithm kind of combined\n",
      "[00:06:43.560 --> 00:06:46.440]   this sample next stage with the update together.\n",
      "[00:06:46.440 --> 00:06:49.440]   Well, instead of like a V-I penetration,\n",
      "[00:06:49.440 --> 00:06:52.000]   we have a first phase that is collecting data\n",
      "[00:06:52.000 --> 00:06:53.960]   and then second phase like doing updates.\n",
      "[00:06:53.960 --> 00:07:05.120]   So I want to make a few comments about the algorithm.\n",
      "[00:07:05.120 --> 00:07:09.080]   The first important thing is that in this algorithm,\n",
      "[00:07:09.080 --> 00:07:14.200]   we no longer explicitly constructed the transition.\n",
      "[00:07:14.200 --> 00:07:16.640]   So this is some difference, I think,\n",
      "[00:07:16.640 --> 00:07:20.320]   in the classical literature across model-based versus model-free.\n",
      "[00:07:20.320 --> 00:07:35.880]   I think this separation is, in some sense,\n",
      "[00:07:35.880 --> 00:07:39.800]   a bit like in the high level, it's not like a very mathematical.\n",
      "[00:07:39.800 --> 00:07:42.840]   But basically, the conceptual idea\n",
      "[00:07:42.840 --> 00:07:46.480]   is like seeing this approach is like you construct a model.\n",
      "[00:07:46.920 --> 00:07:54.520]   Like, construction model means you construct\n",
      "[00:07:54.520 --> 00:07:56.760]   what is the p hat, what is the reward.\n",
      "[00:07:56.760 --> 00:08:01.280]   Like you directly estimate what is transition, what is the reward.\n",
      "[00:08:01.280 --> 00:08:04.680]   And this model-free means you don't construct the model.\n",
      "[00:08:15.600 --> 00:08:17.320]   So if you're using this definition,\n",
      "[00:08:17.320 --> 00:08:19.400]   you can think this value-tourism algorithm\n",
      "[00:08:19.400 --> 00:08:20.920]   is actually a model-based algorithm.\n",
      "[00:08:20.920 --> 00:08:23.440]   Because in the first phase, we directly construct\n",
      "[00:08:23.440 --> 00:08:25.360]   what is transition probability.\n",
      "[00:08:25.360 --> 00:08:28.000]   Well, this Q-learning algorithm will actually\n",
      "[00:08:28.000 --> 00:08:30.160]   we never construct what is transition.\n",
      "[00:08:30.160 --> 00:08:33.800]   And the learning transition part is kind of implicitly\n",
      "[00:08:33.800 --> 00:08:35.840]   in this incremental update here.\n",
      "[00:08:35.840 --> 00:08:37.920]   So we don't directly construct a model.\n",
      "[00:08:37.920 --> 00:08:40.080]   And this is like a model-free algorithm.\n",
      "[00:08:40.080 --> 00:08:41.720]   So Q-learning is model-free.\n",
      "[00:08:41.720 --> 00:08:57.400]   So if you're saying this seems like a two artificial,\n",
      "[00:08:57.400 --> 00:09:00.640]   like what is the mathematical difference between those two\n",
      "[00:09:00.640 --> 00:09:11.360]   styles of algorithm, we can actually\n",
      "[00:09:11.360 --> 00:09:13.160]   use the space complexity to differentiate\n",
      "[00:09:13.160 --> 00:09:14.400]   this two class of algorithm.\n",
      "[00:09:14.400 --> 00:09:24.560]   So in the space complexity, in terms of this,\n",
      "[00:09:24.560 --> 00:09:37.800]   that is how many digits you need to store for each algorithm,\n",
      "[00:09:37.800 --> 00:09:39.720]   like the value of this algorithm.\n",
      "[00:09:39.720 --> 00:09:43.000]   Because we need to estimate the transition.\n",
      "[00:09:43.000 --> 00:09:48.000]   So it's roughly at least an order hs square.\n",
      "[00:09:48.000 --> 00:09:51.720]   Because at each step, we will directly construct a transition\n",
      "[00:09:51.720 --> 00:09:54.160]   and use transition to do like this in matrix vector\n",
      "[00:09:54.160 --> 00:09:55.560]   multiplication.\n",
      "[00:09:55.560 --> 00:10:04.720]   Well, in a Q-learning, we only use hsa space.\n",
      "[00:10:04.720 --> 00:10:09.320]   The reason is now all we need to store in algorithm\n",
      "[00:10:09.320 --> 00:10:10.600]   is just this Q-value.\n",
      "[00:10:10.600 --> 00:10:15.960]   Well, Q-value only has the sa h different numbers,\n",
      "[00:10:15.960 --> 00:10:17.240]   different numbers in the table.\n",
      "[00:10:17.240 --> 00:10:31.520]   So we can think this is a sub-linear in the model size.\n",
      "[00:10:38.960 --> 00:10:42.080]   By model size, I mean the number of parameters you have.\n",
      "[00:10:42.080 --> 00:10:44.200]   This is precisely the number of parameters.\n",
      "[00:10:44.200 --> 00:10:46.800]   We're roughly the number of parameters up to some constants.\n",
      "[00:10:46.800 --> 00:10:49.320]   Well, this is a sub-linear in the number of parameters.\n",
      "[00:10:49.320 --> 00:10:51.480]   So that's why it's called model-free.\n",
      "[00:10:51.480 --> 00:10:54.840]   [APPLAUSE]\n",
      "[00:10:54.840 --> 00:11:13.120]   The second point I want to come in and out\n",
      "[00:11:13.120 --> 00:11:16.440]   is I want to say the sample complexity.\n",
      "[00:11:16.840 --> 00:11:25.560]   I want to say that the sample of Q-learning and the vi.\n",
      "[00:11:25.560 --> 00:11:32.960]   So in terms of value iteration, we already say--\n",
      "[00:11:32.960 --> 00:11:36.080]   we'll talk about the sample complexity of value iteration\n",
      "[00:11:36.080 --> 00:11:41.640]   in the last lecture, which is the old tilde of h to the fourth\n",
      "[00:11:41.640 --> 00:11:46.400]   sa over epsilon square.\n",
      "[00:11:46.400 --> 00:11:49.800]   In terms of Q-learning, you can do a similar analysis\n",
      "[00:11:49.800 --> 00:11:56.840]   where we'll give a little bit sketch immediately.\n",
      "[00:11:56.840 --> 00:11:58.640]   But the result is you can essentially\n",
      "[00:11:58.640 --> 00:12:02.200]   prove the sample complexity is something like this.\n",
      "[00:12:02.200 --> 00:12:06.720]   It's h to the fifth of sa over epsilon square.\n",
      "[00:12:15.480 --> 00:12:18.040]   So we'll now talk about the formal proof about this result.\n",
      "[00:12:18.040 --> 00:12:19.960]   But I just want to say Q-learning can essentially\n",
      "[00:12:19.960 --> 00:12:22.600]   achieve a very similar result to value iteration,\n",
      "[00:12:22.600 --> 00:12:24.440]   but it was slightly worse.\n",
      "[00:12:24.440 --> 00:12:37.680]   H dependency.\n",
      "[00:12:37.680 --> 00:12:50.920]   In terms of sample-wise, actually,\n",
      "[00:12:50.920 --> 00:12:54.760]   by doing this incremental update, you can more or less imagine\n",
      "[00:12:54.760 --> 00:12:58.640]   this is not as efficient as a previous value iteration\n",
      "[00:12:58.640 --> 00:13:00.680]   algorithm, which you collect all the data\n",
      "[00:13:00.680 --> 00:13:02.640]   and use all the data simultaneously.\n",
      "[00:13:02.640 --> 00:13:05.600]   While here, you use the data in the beginning\n",
      "[00:13:05.600 --> 00:13:07.240]   and then you throw data out.\n",
      "[00:13:07.240 --> 00:13:08.800]   And then you collect new data.\n",
      "[00:13:08.800 --> 00:13:11.960]   So in that sense, you don't utilize\n",
      "[00:13:11.960 --> 00:13:15.760]   the power of each sample to its maximum.\n",
      "[00:13:15.760 --> 00:13:18.240]   So that's why it has a slightly worse h dependency.\n",
      "[00:13:18.240 --> 00:13:19.880]   And we'll also talk about the intuition\n",
      "[00:13:19.880 --> 00:13:22.240]   of why it has slightly worse.\n",
      "[00:13:22.240 --> 00:13:24.000]   But I just want to also say you can\n",
      "[00:13:24.000 --> 00:13:25.840]   think this is a trade-off.\n",
      "[00:13:25.840 --> 00:13:28.240]   I allow slightly worse simplicity,\n",
      "[00:13:28.240 --> 00:13:33.280]   but I have better space complexity where we talk about it.\n",
      "[00:13:34.280 --> 00:13:42.440]   And we will actually also have better computation\n",
      "[00:13:42.440 --> 00:13:44.160]   in the online setting.\n",
      "[00:13:44.160 --> 00:13:59.680]   So we haven't talked about the online setting yet,\n",
      "[00:13:59.680 --> 00:14:01.000]   but we will talk about it today.\n",
      "[00:14:01.000 --> 00:14:03.360]   And so with so far, we talk about only a simulator setting.\n",
      "[00:14:03.360 --> 00:14:05.120]   In a simulator setting, in vector iteration,\n",
      "[00:14:05.120 --> 00:14:06.720]   collect the data in the very beginning\n",
      "[00:14:06.720 --> 00:14:09.560]   and not only do one pass of dynamic programming,\n",
      "[00:14:09.560 --> 00:14:11.560]   so this is a computation is fine.\n",
      "[00:14:11.560 --> 00:14:13.720]   But in the online setting, a vectorization\n",
      "[00:14:13.720 --> 00:14:16.880]   needs to do multiple paths of bound and automatic equation.\n",
      "[00:14:16.880 --> 00:14:18.680]   And in that case, it's very slow.\n",
      "[00:14:18.680 --> 00:14:20.920]   So you can essentially think, every time\n",
      "[00:14:20.920 --> 00:14:23.320]   you're learning you need to do one incremental updates,\n",
      "[00:14:23.320 --> 00:14:27.160]   this VI needs to do whole paths of a binary programming.\n",
      "[00:14:27.160 --> 00:14:28.440]   So that's why in the online setting,\n",
      "[00:14:28.440 --> 00:14:31.320]   actually computation of kill learning is also better.\n",
      "[00:14:31.320 --> 00:14:49.160]   So I guess I will not formally go to all the proof\n",
      "[00:14:49.160 --> 00:14:51.880]   about the kill learning why it can also achieve this somehow.\n",
      "[00:14:51.880 --> 00:14:55.520]   But it will just point out some key steps to let you know\n",
      "[00:14:55.520 --> 00:14:58.880]   like how this thing happens.\n",
      "[00:14:58.880 --> 00:15:11.400]   So in order to do analysis for the kill learning,\n",
      "[00:15:11.400 --> 00:15:12.760]   I think the most important thing is\n",
      "[00:15:12.760 --> 00:15:16.000]   we look at this key steps we have.\n",
      "[00:15:16.000 --> 00:15:23.160]   So let's just copy the key update, which is a kill t h.\n",
      "[00:15:23.160 --> 00:15:32.040]   S a equal to 1 minus r for t kill h t minus 1.\n",
      "[00:15:32.040 --> 00:15:42.880]   S a plus r for t r h S a and plus v h minus 1,\n",
      "[00:15:42.880 --> 00:16:01.240]   a v h plus 1, t minus 1, s t prime.\n",
      "[00:16:01.240 --> 00:16:17.640]   This is like a previous kill value, and this is like the new update f\n",
      "[00:16:17.640 --> 00:16:28.880]   time t, or at her iteration t.\n",
      "[00:16:28.880 --> 00:16:33.080]   So this direct formula is a little bit difficult for analyze.\n",
      "[00:16:33.080 --> 00:16:35.840]   But you can think 1 very straightforward way\n",
      "[00:16:35.840 --> 00:16:40.920]   to analyze this update is you recursively expanding it out.\n",
      "[00:16:40.920 --> 00:16:44.600]   This kill value is again equal to 1 minus r for t minus 1\n",
      "[00:16:44.600 --> 00:16:47.400]   fraction of a kill t minus 2.\n",
      "[00:16:47.400 --> 00:16:52.200]   And plus this thing times r for t minus 1\n",
      "[00:16:52.200 --> 00:16:55.000]   of the update at a t minus 1 step.\n",
      "[00:16:55.000 --> 00:16:59.200]   So you can recursively expand this kill value out.\n",
      "[00:16:59.200 --> 00:17:05.400]   And you can actually equivalently have another form of this kill value.\n",
      "[00:17:05.400 --> 00:17:07.600]   You can return as something like this.\n",
      "[00:17:07.600 --> 00:17:22.320]   r h S a plus summation of i from 1 to t gamma t i\n",
      "[00:17:22.320 --> 00:17:28.640]   and v h plus 1 i minus 1 s i prime.\n",
      "[00:17:34.640 --> 00:17:39.040]   Essentially, it's weighted sum of all previous values\n",
      "[00:17:39.040 --> 00:17:43.480]   and previous samples.\n",
      "[00:17:43.480 --> 00:17:45.920]   So you can imagine, you just expanded everything.\n",
      "[00:17:45.920 --> 00:17:50.920]   Expanded this entire kill value out from t minus 1 to t minus 2, t minus 2, t minus 3.\n",
      "[00:17:50.920 --> 00:17:54.880]   So eventually, everything happens in the first term goes away.\n",
      "[00:17:54.880 --> 00:17:57.920]   And all you have is a bunch of summation of the second term.\n",
      "[00:17:57.920 --> 00:18:01.760]   And the second term, and then because of all the alpha,\n",
      "[00:18:01.760 --> 00:18:03.720]   and this reward is the same.\n",
      "[00:18:03.720 --> 00:18:06.240]   So reward summation together equal to 1.\n",
      "[00:18:06.240 --> 00:18:12.200]   And while this thing is what remains, it's like weighted sum of this value.\n",
      "[00:18:12.200 --> 00:18:13.720]   And we will see.\n",
      "[00:18:13.720 --> 00:18:21.160]   And by calculation, you can actually get that gamma i t i\n",
      "[00:18:21.160 --> 00:18:33.880]   is equal to r for i times the product j equal to i plus 1 to t 1 minus r for j.\n",
      "[00:18:33.880 --> 00:18:47.900]   And this summation i from 0 to t gamma t i is equal to 1.\n",
      "[00:18:47.900 --> 00:18:53.180]   OK.\n",
      "[00:18:53.180 --> 00:18:56.900]   So what I'm saying is just starting from this recursive equation,\n",
      "[00:18:56.900 --> 00:19:00.340]   you can prove-- you can do some calculation, essentially,\n",
      "[00:19:00.340 --> 00:19:03.460]   by expanding out this previous kill value.\n",
      "[00:19:03.460 --> 00:19:05.300]   And then eventually, you're getting this.\n",
      "[00:19:05.300 --> 00:19:10.540]   It can be viewed as a weighted sum of all the updates.\n",
      "[00:19:10.540 --> 00:19:14.460]   And while the weights, we can do the calculation of weights in this way,\n",
      "[00:19:14.460 --> 00:19:15.660]   and the weights sum to 1.\n",
      "[00:19:15.660 --> 00:19:37.100]  , so we'll compare this to the value iteration,\n",
      "[00:19:37.100 --> 00:19:39.940]   where we know the value iteration we're doing the following updates\n",
      "[00:19:39.940 --> 00:19:53.180]   on a Belmont automatic equation, that the kill value is equal to r plus p hat,\n",
      "[00:19:53.180 --> 00:19:55.300]   which is our estimate.\n",
      "[00:19:55.300 --> 00:20:00.300]   And v plus 1 hat sa.\n",
      "[00:20:00.300 --> 00:20:02.180]   And you can essentially expand out what\n",
      "[00:20:02.180 --> 00:20:05.260]   is the definition of p hat, which is using the empirical frequency\n",
      "[00:20:05.260 --> 00:20:14.860]   for estimated transition probability, which is equal to rh sa plus summation i\n",
      "[00:20:14.860 --> 00:20:24.060]   from 1 to n, and 1 over n v h plus 1 hat s i prime.\n",
      "[00:20:28.060 --> 00:20:41.660]   So this is like the value iteration.\n",
      "[00:20:41.660 --> 00:20:45.460]   So we notice if we compare those two update equation,\n",
      "[00:20:45.460 --> 00:20:49.220]   they are like two difference, I think.\n",
      "[00:20:49.220 --> 00:20:54.540]   So one difference is now we're doing a weighted sum.\n",
      "[00:20:54.540 --> 00:21:07.820]   The first difference we're doing, let's see, this is like summation with equal weight.\n",
      "[00:21:07.820 --> 00:21:11.300]   But here, we can potentially have different weights.\n",
      "[00:21:11.300 --> 00:21:14.540]   We can either favor in the later ones, favor in later updates,\n",
      "[00:21:14.540 --> 00:21:16.740]   or we can either favor in the previous updates.\n",
      "[00:21:16.740 --> 00:21:20.060]   Basically, we have the flexibility of choosing our learning weights,\n",
      "[00:21:20.060 --> 00:21:23.540]   which can tune what is the weights we put here.\n",
      "[00:21:24.540 --> 00:21:33.540]   And the second difference is we actually note here,\n",
      "[00:21:33.540 --> 00:21:40.540]   like this is just using v hat, which is our estimate of value,\n",
      "[00:21:40.540 --> 00:21:42.540]   after we're collecting all the data.\n",
      "[00:21:42.540 --> 00:21:46.540]   But here is the value at the ice iteration.\n",
      "[00:21:46.540 --> 00:22:00.540]   So Q-learning uses value at the previous iteration.\n",
      "[00:22:00.540 --> 00:22:12.540]   So in that sense, this v hat is basically calculated after you collect all the data.\n",
      "[00:22:12.540 --> 00:22:14.540]   So this v hat is reasonably accurate.\n",
      "[00:22:14.540 --> 00:22:17.540]   Well, here the value is used at a previous iteration.\n",
      "[00:22:17.540 --> 00:22:21.540]   So you can think of the v h plus 1, 1, for example.\n",
      "[00:22:21.540 --> 00:22:24.540]   This is like the value we calculated at the first iterate.\n",
      "[00:22:24.540 --> 00:22:28.540]   And in that case, you only observe one trajectory, or one data.\n",
      "[00:22:28.540 --> 00:22:32.540]   And in that kind of scenario, clearly, this is not very accurate.\n",
      "[00:22:32.540 --> 00:22:43.540]   So v h plus 1 i is not accurate for small i.\n",
      "[00:22:43.540 --> 00:23:09.540]   So this is where in terms of Q-learning, we have a little bit more complexity we need to deal with.\n",
      "[00:23:09.540 --> 00:23:19.540]   So we'll talk about the next thing is how we're going to choose gamma T.\n",
      "[00:23:19.540 --> 00:23:38.540]   Like, how to choose our weights.\n",
      "[00:23:38.540 --> 00:23:46.540]   Essentially, we have two conflicting parts we want to handle for this gamma T.\n",
      "[00:23:46.540 --> 00:24:02.540]   The first one is that we want to have gamma T to put more weights\n",
      "[00:24:02.540 --> 00:24:14.540]   on values of later updates.\n",
      "[00:24:14.540 --> 00:24:19.540]   This is because, as we said, the v h plus 1 i is not accurate for small i.\n",
      "[00:24:19.540 --> 00:24:24.540]   So we essentially want the gamma to put more weights on the value with, like,\n",
      "[00:24:24.540 --> 00:24:35.540]   at a later iteration, so to reduce the bias.\n",
      "[00:24:35.540 --> 00:24:41.540]   So ideally, for example, we just put a probability 1, or the weights 1, on the very last iterate.\n",
      "[00:24:41.540 --> 00:24:45.540]   And in this way, we have the smallest bias.\n",
      "[00:24:45.540 --> 00:25:00.540]   Remember, we should not do that, because the second thing is that we want to gamma i to be spread out.\n",
      "[00:25:00.540 --> 00:25:06.540]   We should now focus all the weights on one single sample, because a single sample is like a statistical sample,\n",
      "[00:25:06.540 --> 00:25:09.540]   and it has a lot of noise.\n",
      "[00:25:09.540 --> 00:25:29.540]   So we want the gamma T to be uniform, in some sense, to reduce the variance.\n",
      "[00:25:29.540 --> 00:25:39.540]   This is, like, the variance part.\n",
      "[00:25:39.540 --> 00:25:49.540]   So basically, we can think the ideal choice would be something which we will put a lot of weights on the later updates.\n",
      "[00:25:49.540 --> 00:25:55.540]   And so the previous iterates have smaller value, and later iterates have, like, higher weights.\n",
      "[00:25:55.540 --> 00:26:00.540]   But on the other hand, we should not just focus on one update, because it has a lot of noise.\n",
      "[00:26:00.540 --> 00:26:23.540]   So we should still, like, make it right how they spread out.\n",
      "[00:26:23.540 --> 00:26:29.540]   Any question about this, like, a bias variance trade-off principle in this Q value?\n",
      "[00:26:29.540 --> 00:26:34.540]   Q-learning part.\n",
      "[00:26:34.540 --> 00:26:43.540]   So now, like, we have some, like, intuitive idea of how we choose gamma T, and then we talk about some specific value.\n",
      "[00:26:43.540 --> 00:26:48.540]   So our baseline is we look at r of a T equal to 1 over T.\n",
      "[00:26:48.540 --> 00:26:54.540]   This is one very simple way you can compute, and you can actually use our formula for gamma IIT,\n",
      "[00:26:54.540 --> 00:26:57.540]   and you can compute the weights.\n",
      "[00:26:57.540 --> 00:27:05.540]   Gamma T1 is actually equal to gamma T2, instead of r, always equal to gamma T2.\n",
      "[00:27:05.540 --> 00:27:09.540]   And this is equal to 1 over T.\n",
      "[00:27:09.540 --> 00:27:17.540]   So this corresponds to the uniform weights.\n",
      "[00:27:17.540 --> 00:27:25.540]   That is, if I choose learning rate equal to 1 over T, we actually happens to be the uniform weights.\n",
      "[00:27:25.540 --> 00:27:36.540]   And this is the part, essentially, we are very good at variance, but somehow we are not very good at dealing with bias.\n",
      "[00:27:36.540 --> 00:27:50.540]   So actually, this ratio does not give good.\n",
      "[00:27:50.540 --> 00:27:58.540]   Good sample complexity because of bias.\n",
      "[00:27:58.540 --> 00:28:05.540]   So as we said, we want to tune this a little bit towards favoring the later part of the updates.\n",
      "[00:28:05.540 --> 00:28:13.540]   And by favoring later parts of the updates, that means we need to basically increase the learning rate.\n",
      "[00:28:13.540 --> 00:28:20.540]   By increasing the learning rate, that is, we put more emphasis on the later updates than the previous Q value.\n",
      "[00:28:20.540 --> 00:28:29.540]   So it turns out, theoretically, you can prove the learning rates which achieve this sample complexity\n",
      "[00:28:29.540 --> 00:28:51.540]   is something like r for T is equal to h plus 1 over h plus T, which you can think is roughly in the order of h over T.\n",
      "[00:28:51.540 --> 00:29:05.540]   This is like a better choice.\n",
      "[00:29:05.540 --> 00:29:18.540]   So you can think high level, and this is something like, essentially, we put more weights on the later transitions.\n",
      "[00:29:18.540 --> 00:29:41.540]   So this is roughly similar to put relatively uniform weights.\n",
      "[00:29:41.540 --> 00:30:01.540]   The latest one over h fraction of data.\n",
      "[00:30:01.540 --> 00:30:09.540]   And because we need to deal with this bias variance trade off, so that's why we essentially need to ignore some, to some extent,\n",
      "[00:30:09.540 --> 00:30:11.540]   ignore the earlier part of the data.\n",
      "[00:30:11.540 --> 00:30:14.540]   So that's why we're not that simple efficient.\n",
      "[00:30:14.540 --> 00:30:18.540]   We essentially pay some additional h factor because we're not doing uniform weighting.\n",
      "[00:30:18.540 --> 00:30:23.540]   We're doing some more aggressive weighting towards later updates.\n",
      "[00:30:23.540 --> 00:30:27.540]   This essentially is the reason why we have slightly worse h dependence.\n",
      "[00:30:27.540 --> 00:30:38.540]   But just remember, we still have a better space complexity and also the condition complexity.\n",
      "[00:30:38.540 --> 00:30:58.540]   Composition complexity, I think we said in a similar setting, we cannot see it.\n",
      "[00:30:58.540 --> 00:31:01.540]   But in our setting later, we'll actually see it.\n",
      "[00:31:01.540 --> 00:31:06.540]   In the export origin setting, I think the reason is because in the simulation setting,\n",
      "[00:31:06.540 --> 00:31:10.540]   we essentially just collect all the data, and then we do one path of planning.\n",
      "[00:31:10.540 --> 00:31:12.540]   That is a bone-optimistic equation.\n",
      "[00:31:12.540 --> 00:31:17.540]   Well, in the online setting, that is, we can no longer sweep over all the state action.\n",
      "[00:31:17.540 --> 00:31:20.540]   We actually will talk about immediately about what is the setting.\n",
      "[00:31:20.540 --> 00:31:26.540]   But the main takeaway is that when we do the value iteration, we actually need to collect one trajectories\n",
      "[00:31:26.540 --> 00:31:29.540]   and then do one path of bone-optimistic equation.\n",
      "[00:31:29.540 --> 00:31:33.540]   And then collect one trajectories, then we do another path of bone-optimistic equation.\n",
      "[00:31:33.540 --> 00:31:38.540]   So, in particular, we just collect one trajectories and do one path of these incremental updates.\n",
      "[00:31:38.540 --> 00:31:48.540]   So, that's why you can do a lot faster than that value iteration.\n",
      "[00:31:48.540 --> 00:31:56.540]   So, I just want to sum, before we move on to the next setting, we'll just do a slightly summary of the similar setting.\n",
      "[00:31:56.540 --> 00:32:11.540]   So, specifically, we'll talk about the non-stationary transition case.\n",
      "[00:32:11.540 --> 00:32:24.540]   We're by non-stationary, we mean we kind of assume p_y is not necessarily equal to p_2,\n",
      "[00:32:24.540 --> 00:32:27.540]   is not necessarily equal to p_n.\n",
      "[00:32:27.540 --> 00:32:30.540]   They can be equal, but we don't really require it to be equal.\n",
      "[00:32:30.540 --> 00:32:36.540]   So, it can be, like, different.\n",
      "[00:32:36.540 --> 00:32:44.540]   And then we just talk about 12 algorithm.\n",
      "[00:32:44.540 --> 00:32:46.540]   Essentially, we kind of already summarized there.\n",
      "[00:32:46.540 --> 00:32:51.540]   We talk about the model-based.\n",
      "[00:32:51.540 --> 00:32:58.540]   That is vi and model-free.\n",
      "[00:32:58.540 --> 00:33:08.540]   That is a q-learning, where the most important guarantees we talk about is a sample completely,\n",
      "[00:33:08.540 --> 00:33:15.540]   which this one is o_tilde, h_4 s_a over epsilon^2.\n",
      "[00:33:15.540 --> 00:33:22.540]   And space, this is h_s^2.\n",
      "[00:33:22.540 --> 00:33:33.540]   Well, in terms of q-learning, our sample combinations is o_tilde, h_5 s_a over epsilon^2, slightly worse.\n",
      "[00:33:33.540 --> 00:33:37.540]   And while the space, we kind of save some space by doing model-free.\n",
      "[00:33:37.540 --> 00:33:43.540]   That we no longer suffer s^2, depending on the way we only pay s dependency.\n",
      "[00:33:43.540 --> 00:33:51.540]   And we will also talk about later in the lower bound session or fundamental limit, there is a lower bound.\n",
      "[00:33:51.540 --> 00:33:58.540]   Or like, information theoretical lower bound.\n",
      "[00:33:58.540 --> 00:34:02.540]   Information theoretical lower bound basically says, because by our assumption of the model,\n",
      "[00:34:02.540 --> 00:34:08.540]   we kind of assume each state, next state, and the reward are, like, randomly generated.\n",
      "[00:34:08.540 --> 00:34:12.540]   So, there is always the statistical, like, noise in it.\n",
      "[00:34:12.540 --> 00:34:19.540]   And algorithm in the worst case, like, cannot really tell which one is the optimal policy because of those noise.\n",
      "[00:34:19.540 --> 00:34:28.540]   So, that is, we will see, like, in the worst case, like, essentially, any algorithm in the worst case\n",
      "[00:34:28.540 --> 00:34:42.540]   needs to pay at least this amount of samples to learn the optimal policy well.\n",
      "[00:34:42.540 --> 00:34:49.540]   So, we will discuss this topic after we talk about the explorations setting or online setting today.\n",
      "[00:35:02.540 --> 00:35:17.540]   So, this essentially says, no algorithm can do better than this\n",
      "[00:35:17.540 --> 00:35:29.540]   in the worst case.\n",
      "[00:35:29.540 --> 00:35:49.540]   So, this more or less conclude that VI almost achieve the, we call it a minimax optimal sample commodity.\n",
      "[00:35:49.540 --> 00:36:10.540]   That almost means up to some log vector.\n",
      "[00:36:10.540 --> 00:36:18.540]   Like, here we have the old choda here, while the lower bounds will prove in the class does not have the log vector.\n",
      "[00:36:18.540 --> 00:36:23.540]   But more or less, in this class, we don't really care about a log vector.\n",
      "[00:36:23.540 --> 00:36:30.540]   So, we will just say, VI can already achieve our goal of getting the best algorithm in this setting,\n",
      "[00:36:30.540 --> 00:36:58.540]   like, you cannot beat the value iteration in terms of some of the worst case.\n",
      "[00:36:58.540 --> 00:37:06.540]   Okay, so any questions so far about the similar setting?\n",
      "[00:37:06.540 --> 00:37:16.540]   Now that we will move on to talk about the next topic, which is the online reinforcement learning,\n",
      "[00:37:16.540 --> 00:37:26.540]   or we also call the exploration setting.\n",
      "[00:37:26.540 --> 00:37:47.540]   So basically, we talk about this is like more challenging setting that we no longer assume the simulator.\n",
      "[00:37:47.540 --> 00:37:51.540]   So first, what is the online reinforcement learning setting?\n",
      "[00:37:51.540 --> 00:37:56.540]   So, just remember how we are going to interact with the MDP.\n",
      "[00:37:56.540 --> 00:38:08.540]   The protocol of interacting with MDP is like, we have the S1, and we go to R1, sorry, we go to, we have A1,\n",
      "[00:38:08.540 --> 00:38:14.540]   the first action, and both of them simultaneously determines the reward you receive at the first step,\n",
      "[00:38:14.540 --> 00:38:29.540]   and all of them determine what is transition probability to go to the next states.\n",
      "[00:38:29.540 --> 00:38:46.540]   Something like this.\n",
      "[00:38:46.540 --> 00:38:55.540]   So, the protocol in the online reinforcement learning setting,\n",
      "[00:38:55.540 --> 00:39:08.540]   we will just observe the initial states\n",
      "[00:39:08.540 --> 00:39:26.540]   and then for h equal to 1 to H, agent takes action,\n",
      "[00:39:26.540 --> 00:39:44.540]   and then the environment transitions to the next states,\n",
      "[00:39:44.540 --> 00:40:00.540]   such plus one, and the standard reward, arch.\n",
      "[00:40:00.540 --> 00:40:06.540]   And the most important thing here is in the online reinforcement learning setting,\n",
      "[00:40:06.540 --> 00:40:19.540]   we will just basically reset to initial states.\n",
      "[00:40:19.540 --> 00:40:24.540]   And we only allow, like after we play the entire trajectory, then we can go back to initial states,\n",
      "[00:40:24.540 --> 00:40:27.540]   and then we can do everything again.\n",
      "[00:40:27.540 --> 00:40:34.540]   So, in the online reinforcement learning, it's like more challenging in the sense that we will play the entire trajectory.\n",
      "[00:40:34.540 --> 00:40:46.540]   So, this is like the biggest difference between the simulator and the online isle.\n",
      "[00:40:46.540 --> 00:40:54.540]   Is that in the simulator, you can basically choose both state and action.\n",
      "[00:40:54.540 --> 00:40:58.540]   Well, in the online reinforcement learning, you are essentially like playing a real game,\n",
      "[00:40:58.540 --> 00:41:02.540]   or like you're kind of doing a robot, you always like reset to initial states,\n",
      "[00:41:02.540 --> 00:41:06.540]   and essentially all you can do is you just choose the action sequence.\n",
      "[00:41:06.540 --> 00:41:19.540]   You no longer have the freedom to choose the states.\n",
      "[00:41:19.540 --> 00:41:21.540]   This is like the major difference.\n",
      "[00:41:21.540 --> 00:41:24.540]   You can think this is like more or less, it's also how we play the game.\n",
      "[00:41:24.540 --> 00:41:31.540]   Like when we die, we go back to the initial states and we play the game again.\n",
      "[00:41:31.540 --> 00:41:39.540]   So, this is also like mostly like a fact how we are going to collect the data.\n",
      "[00:41:39.540 --> 00:41:46.540]   So, one very important thing is in a simulator setting, we can just basically use a very simple strategy to collect the data.\n",
      "[00:41:46.540 --> 00:41:54.540]   That is we will sweep over our state action pair.\n",
      "[00:41:54.540 --> 00:42:06.540]   Essentially, in the better situation algorithm is we kind of query the simulator for all state action pair,\n",
      "[00:42:06.540 --> 00:42:09.540]   and then we get next states and this is basically done.\n",
      "[00:42:09.540 --> 00:42:13.540]   And our data collection from there is like a very good.\n",
      "[00:42:13.540 --> 00:42:23.540]   However, in online reinforcement, we can no longer do that because we can no longer like just for free set to any states.\n",
      "[00:42:23.540 --> 00:42:51.540]   So, in this case, just simply like a rich sum states by yourself may not be an easy task.\n",
      "[00:42:51.540 --> 00:42:56.540]   Imagine you're playing like Super Mario and some states is in like a last chapter.\n",
      "[00:42:56.540 --> 00:42:58.540]   And by reaching there, it's already not very easy.\n",
      "[00:42:58.540 --> 00:43:11.540]   Like you need to do some complicated strategy so that you can eventually go to the last chapter of the game.\n",
      "[00:43:11.540 --> 00:43:38.540]   So, one important question we will handle in our online reinforcement learning is how to efficiently\n",
      "[00:43:38.540 --> 00:43:45.540]   collect data.\n",
      "[00:43:45.540 --> 00:43:59.540]   In a sense, like we want to collect the informative data.\n",
      "[00:43:59.540 --> 00:44:05.540]   We're basically in a simulator setting as long as we sweep through all the state action, we're collecting informative data.\n",
      "[00:44:05.540 --> 00:44:10.540]   Well, in online reinforcement learning, this is no longer that clear.\n",
      "[00:44:10.540 --> 00:44:23.540]   And we will actually give an example showing why this is actually a non-trivial question or this is like an interesting question.\n",
      "[00:44:23.540 --> 00:44:49.540]   So, we'll talk about the failure of random exploration in MDP.\n",
      "[00:44:49.540 --> 00:44:52.540]   So, what is a random exploration?\n",
      "[00:44:52.540 --> 00:44:57.540]   So, the first idea to collect data is here in a similar setting because we can choose all the state action.\n",
      "[00:44:57.540 --> 00:45:00.540]   So, we just sweep through all the state action pair.\n",
      "[00:45:00.540 --> 00:45:09.540]   Well, in the online reinforcement learning, although I cannot choose state, but like by random exploration, that means like basically every time I just choose action randomly.\n",
      "[00:45:09.540 --> 00:45:29.540]   Like every time, every time when I take action, this means like every step.\n",
      "[00:45:29.540 --> 00:45:37.540]   So, this would be some naive idea I would try to see if the data we collected in this way is like a comprehensive.\n",
      "[00:45:37.540 --> 00:45:41.540]   Like it will give me a lot of information.\n",
      "[00:45:41.540 --> 00:45:47.540]   So, it turns out this random exploration strategy in MDP is like highly ineffective.\n",
      "[00:45:47.540 --> 00:45:58.540]   It may suffer from some like this is highly effective.\n",
      "[00:45:58.540 --> 00:46:14.540]   This may suffer from some A to the power of H some velocity.\n",
      "[00:46:14.540 --> 00:46:18.540]   So, this thing might suffer something like that is exponential in a horizon.\n",
      "[00:46:18.540 --> 00:46:27.540]   So, basically for anything that has a long horizon, like for example 20 or something, this entire method is really good.\n",
      "[00:46:27.540 --> 00:46:31.540]   You cannot use this way to do collected very effectively.\n",
      "[00:46:31.540 --> 00:46:33.540]   So, we will talk about why this is true.\n",
      "[00:46:33.540 --> 00:46:42.540]   We basically just construct some hard instance so that we show for this hard instance, this algorithm is like very bad.\n",
      "[00:46:42.540 --> 00:47:00.540]   So, to show this we construct a hard instance.\n",
      "[00:47:00.540 --> 00:47:08.540]   Hard instance in our literature is actually pretty famous a lot of later like hard instances like based on this.\n",
      "[00:47:08.540 --> 00:47:18.540]   It's called a complete hardware log.\n",
      "[00:47:18.540 --> 00:47:24.540]   By the way, I also want to emphasize this is like hard instance only for random exploration.\n",
      "[00:47:24.540 --> 00:47:29.540]   So, later we will actually have better algorithm so that it can basically, this is no longer hard.\n",
      "[00:47:29.540 --> 00:47:31.540]   We can basically solve this problem.\n",
      "[00:47:31.540 --> 00:47:34.540]   So, this is like different from this infomini theoretical lower bounds.\n",
      "[00:47:34.540 --> 00:47:37.540]   This lower bound states like no algorithm can do better than this.\n",
      "[00:47:37.540 --> 00:47:41.540]   But this lower bound essentially says this algorithm is not good.\n",
      "[00:47:41.540 --> 00:47:44.540]   But other algorithm can be good.\n",
      "[00:47:44.540 --> 00:47:48.540]   So, we look at this coming to our log.\n",
      "[00:47:48.540 --> 00:47:53.540]   Coming to our log is an MDP is a two-state MDP.\n",
      "[00:47:53.540 --> 00:48:00.540]   And let's say with A actions.\n",
      "[00:48:00.540 --> 00:48:09.540]   So, the MDP is actually a deterministic MDP.\n",
      "[00:48:09.540 --> 00:48:20.540]   We don't even need randomness to like essentially break this random exploration out with it.\n",
      "[00:48:20.540 --> 00:48:25.540]   So, we look at the MDP only has two states.\n",
      "[00:48:25.540 --> 00:48:30.540]   So, at every step we have like S0 and S1, those are the two states.\n",
      "[00:48:30.540 --> 00:48:37.540]   Maybe let's just superscript so that to make it different.\n",
      "[00:48:37.540 --> 00:48:39.540]   This is not talking about a step.\n",
      "[00:48:39.540 --> 00:48:43.540]   This is like basically in the first step, I have two states that S0 and S1.\n",
      "[00:48:43.540 --> 00:48:47.540]   This is like t equal to 1.\n",
      "[00:48:47.540 --> 00:48:50.540]   And then the transition goes as follows.\n",
      "[00:48:50.540 --> 00:48:56.540]   That is if I'm at S0 states, at first states, then no matter what I did.\n",
      "[00:48:56.540 --> 00:49:01.540]   So, basically whatever action I take, I'm always going to land in S0 in a second step.\n",
      "[00:49:01.540 --> 00:49:05.540]   This is like t equal to 2.\n",
      "[00:49:05.540 --> 00:49:16.540]   While in the S1 states, there's only one particular action that's going to land me into the S1\n",
      "[00:49:16.540 --> 00:49:18.540]   in the next step.\n",
      "[00:49:18.540 --> 00:49:20.540]   That's called A1 star.\n",
      "[00:49:20.540 --> 00:49:29.540]   And for whatever the action that's not here, that is not A1 star.\n",
      "[00:49:29.540 --> 00:49:35.540]   So, we just write this as A minus A1 star.\n",
      "[00:49:35.540 --> 00:49:42.540]   So, that is all the remaining action, I'm going to go to the S0.\n",
      "[00:49:42.540 --> 00:49:48.540]   So, essentially, we're going to duplicate this structure for t times, for h times.\n",
      "[00:49:48.540 --> 00:49:54.540]   So, when we add these states, whatever action I take, I'm going to land in this state as well.\n",
      "[00:49:54.540 --> 00:50:01.540]   And if I'm at this S1 states, and there's another action I take, a 2 star,\n",
      "[00:50:01.540 --> 00:50:06.540]   that will land me here, and whatever other action I take will go back to S0.\n",
      "[00:50:06.540 --> 00:50:18.540]   So, we essentially have this structure again and again,\n",
      "[00:50:18.540 --> 00:50:34.540]   till the very last step, t equal to 3, until at the very end that is t equal to h,\n",
      "[00:50:34.540 --> 00:50:38.540]   a h minus 1 star.\n",
      "[00:50:38.540 --> 00:50:42.540]   And this is A subject to a h minus 1 star.\n",
      "[00:50:42.540 --> 00:51:11.540]   [silence]\n",
      "[00:51:11.540 --> 00:51:16.540]   So, this is like the transition, and then we will set up the reward.\n",
      "[00:51:16.540 --> 00:51:26.540]   The reward is basically everywhere reward is 0, and only if I reach these states, we will receive reward of 1.\n",
      "[00:51:26.540 --> 00:51:48.540]   [silence]\n",
      "[00:51:48.540 --> 00:51:52.540]   S1 at t equal to h.\n",
      "[00:51:52.540 --> 00:51:55.540]   So, only at the very last step, if I go to S1, I got a reward of 1.\n",
      "[00:51:55.540 --> 00:52:00.540]   Otherwise, I always get a reward of 0.\n",
      "[00:52:00.540 --> 00:52:04.540]   This basically can be realized, if you are very strict about the reward,\n",
      "[00:52:04.540 --> 00:52:06.540]   it has to be a state action function.\n",
      "[00:52:06.540 --> 00:52:10.540]   And basically, it's seen at the very last step, whatever action you take,\n",
      "[00:52:10.540 --> 00:52:18.540]   you get a reward of 1. Otherwise, you get reward of 0.\n",
      "[00:52:18.540 --> 00:52:21.540]   So, we can see this transition has a very special structure.\n",
      "[00:52:21.540 --> 00:52:33.540]   That is only if you take precisely a 1 star, a 2 star, a 3 star.\n",
      "[00:52:33.540 --> 00:52:40.540]   So, this is basically the property of this commutary log.\n",
      "[00:52:40.540 --> 00:52:52.540]   So, we have the optimal policy.\n",
      "[00:52:52.540 --> 00:52:59.540]   So, the optimal policy is pi h, S0, which can be anything.\n",
      "[00:52:59.540 --> 00:53:02.540]   It doesn't really matter.\n",
      "[00:53:02.540 --> 00:53:11.540]   An important thing is pi h, S1, must be a h star.\n",
      "[00:53:11.540 --> 00:53:13.540]   And this is the optimal policy.\n",
      "[00:53:13.540 --> 00:53:24.540]   The one which is this optimal policy, what you will get is the value of pi star.\n",
      "[00:53:24.540 --> 00:53:30.540]   You will get it equal to 1.\n",
      "[00:53:30.540 --> 00:53:33.540]   So, this is the only case, you will get one reward.\n",
      "[00:53:33.540 --> 00:53:37.540]   And if you precisely choose the action sequence, that is a 1 star, a 2 star,\n",
      "[00:53:37.540 --> 00:53:41.540]   a 3 star, till the very end.\n",
      "[00:53:41.540 --> 00:54:08.540]   For whatever, every order sequence, you get a reward of 0.\n",
      "[00:54:08.540 --> 00:54:23.540]   So, in that case, it basically has no signal.\n",
      "[00:54:23.540 --> 00:54:38.540]   So, you don't really know what is optimal, optimal should actually in that case.\n",
      "[00:54:38.540 --> 00:54:54.540]   So, let's talk about the random exploration, what random exploration will do.\n",
      "[00:54:54.540 --> 00:55:01.540]   So, random exploration essentially try h, as we said by different definition, we will try\n",
      "[00:55:01.540 --> 00:55:10.540]   h uniformly at random.\n",
      "[00:55:10.540 --> 00:55:18.540]   So, basically, this algorithm will not be very good until it kind of try precisely this action sequence.\n",
      "[00:55:18.540 --> 00:55:26.540]   Try this a 1 star, a 2 star, and dot dot dot till a h star.\n",
      "[00:55:26.540 --> 00:55:33.540]   And until you feel like basically sampled this precise sequence, then you will realize\n",
      "[00:55:33.540 --> 00:55:38.540]   this is like the optimal policy, and then you find optimal policy.\n",
      "[00:55:38.540 --> 00:55:43.540]   Like every time before that, you doesn't really know what is the optimal policy.\n",
      "[00:55:43.540 --> 00:55:49.540]   So, it's not very difficult to compute that the probability of precisely sampled this action sequence\n",
      "[00:55:49.540 --> 00:56:15.540]   is something like a 1 over a 2 to the h.\n",
      "[00:56:15.540 --> 00:56:29.540]   So, this is essentially the whole reason that is basically we need at least a to the omega\n",
      "[00:56:29.540 --> 00:56:50.540]   h samples, or by samples here we means trajectories to find the optimal policy, which you kind\n",
      "[00:56:50.540 --> 00:56:58.540]   of even make an optimal policy, because here is just the difference between 0 and 1.\n",
      "[00:56:58.540 --> 00:57:27.540]   I think this is like a slightly weaker,\n",
      "[00:57:27.540 --> 00:57:31.540]   I'm just claiming some weaker statement.\n",
      "[00:57:31.540 --> 00:57:35.540]   Omega A to the h, you are essentially saying exponent has to be h, but here I'm saying\n",
      "[00:57:35.540 --> 00:57:50.540]   exponent can be h over 2 or something.\n",
      "[00:57:50.540 --> 00:57:57.540]   Basically, this is the key idea that is reinforcement because it has temporal correlation.\n",
      "[00:57:57.540 --> 00:58:02.540]   So, if you do random exploration, especially for some long horizon, the probability of\n",
      "[00:58:02.540 --> 00:58:05.540]   sample exactly one of the very good policies is like very, very low.\n",
      "[00:58:05.540 --> 00:58:11.540]   So, that's why if you just do some random exploration without using any smart algorithm\n",
      "[00:58:11.540 --> 00:58:16.540]   or smart information, then essentially you will pay something exponential in the horizon,\n",
      "[00:58:16.540 --> 00:58:25.540]   which is like people originally consider like a curse of horizon or something like that.\n",
      "[00:58:25.540 --> 00:58:31.540]   And you can go back to convince yourself this algorithm is, and this claim is essentially\n",
      "[00:58:31.540 --> 00:58:34.540]   also true.\n",
      "[00:58:34.540 --> 00:58:39.540]   So, we're talking about this is like a purely random exploration, but you can also think\n",
      "[00:58:39.540 --> 00:58:56.540]   that you can do like a essentially random exploration.\n",
      "[00:58:56.540 --> 00:59:04.540]   Then, random exploration first, and then do like a vector iteration, or like you do something\n",
      "[00:59:04.540 --> 00:59:13.540]   like an epsilon greedy version of a vector iteration.\n",
      "[00:59:13.540 --> 00:59:18.540]   Like by epsilon greedy means with one minus, with one minus epsilon probability, you do\n",
      "[00:59:18.540 --> 00:59:22.540]   a vector iteration with epsilon probability, you do some random action.\n",
      "[00:59:22.540 --> 00:59:26.540]   The whole idea is the same, as long as you don't use any information to do this exploration,\n",
      "[00:59:26.540 --> 00:59:31.540]   like as long as you try to action randomly, like epsilon greedy essentially with epsilon\n",
      "[00:59:31.540 --> 00:59:33.540]   probability, you try action randomly.\n",
      "[00:59:33.540 --> 00:59:38.540]   As long as you try action randomly, you will suffer from this like hard instance, essentially\n",
      "[00:59:38.540 --> 00:59:56.340]   that will not work.\n",
      "[00:59:56.340 --> 01:00:01.740]   So basically this is also one of the very important distinctions between theory.\n",
      "[01:00:01.740 --> 01:00:06.660]   And I would say like in practice a lot of people just blindly use the epsilon greedy\n",
      "[01:00:06.660 --> 01:00:11.940]   algorithm, but you already see it's like epsilon greedy is not a principle of the algorithm\n",
      "[01:00:11.940 --> 01:00:14.620]   that can solve any challenging explorations scenario.\n",
      "[01:00:14.620 --> 01:00:17.620]   At least even for tabular settings, this is like a bad algorithm.\n",
      "[01:00:17.620 --> 01:00:23.300]   This can only deal with those games where like basically it doesn't really require any\n",
      "[01:00:23.300 --> 01:00:24.300]   exploration.\n",
      "[01:00:24.300 --> 01:00:27.780]   It cannot deal with this hard instance, they can only deal with probably some easy instance\n",
      "[01:00:27.780 --> 01:00:36.460]   of MDP that does not require much exploration.\n",
      "[01:00:36.460 --> 01:00:41.700]   So as a side note, it turns out like people did a lot of experiments on Atari games and\n",
      "[01:00:41.700 --> 01:00:46.540]   they actually showed that epsilon greedy seems like performed reasonably well compared\n",
      "[01:00:46.540 --> 01:00:48.780]   to other sophisticated exploration algorithm.\n",
      "[01:00:48.780 --> 01:00:55.220]   But later there are also some analysis, basically it says a large set of Atari games does not\n",
      "[01:00:55.220 --> 01:00:59.820]   require like complicated exploration, basically you can just do random action and that still\n",
      "[01:00:59.820 --> 01:01:00.820]   work.\n",
      "[01:01:00.820 --> 01:01:04.980]   But what we hear is like we want to solve for all the MDP so that there are some like challenging\n",
      "[01:01:04.980 --> 01:01:17.980]   setting it won't work and we'll talk about how to make it work.\n",
      "[01:01:17.980 --> 01:01:42.140]   So essentially we need some like a smarter algorithm to do exploration.\n",
      "[01:01:42.140 --> 01:01:45.980]   Exploration is essentially collected data and whenever we talk about exploration people\n",
      "[01:01:45.980 --> 01:02:05.300]   also talk about essentially the exploration versus exploitation trade off.\n",
      "[01:02:05.300 --> 01:02:11.700]   So it turns out the key idea in reinforcement learning of handling the exploration is called\n",
      "[01:02:11.700 --> 01:02:24.940]   the optimism so actually introduce this most important idea or introduce this most important\n",
      "[01:02:24.940 --> 01:02:30.420]   idea in the next in this lecture and the next lecture.\n",
      "[01:02:30.420 --> 01:02:37.180]   So if we really want to talk about the exploration and exploitation trade off, it's better we\n",
      "[01:02:37.180 --> 01:02:43.180]   started with some like a simpler model so that we can talk about what is the algorithm\n",
      "[01:02:43.180 --> 01:02:46.820]   and what is phenomena there.\n",
      "[01:02:46.820 --> 01:02:50.620]   So it turns out in the literature there's a field which kind of study this trade off\n",
      "[01:02:50.620 --> 01:02:59.140]   for a very long time that is the multi unbended essentially the first model we introduce in\n",
      "[01:02:59.140 --> 01:03:08.780]   this lecture.\n",
      "[01:03:08.780 --> 01:03:14.060]   And this entire literature kind of really focus on this trade off and the optimism is\n",
      "[01:03:14.060 --> 01:03:24.260]   a key idea which we borrow from, the key idea we borrow from this literature.\n",
      "[01:03:24.260 --> 01:03:34.020]   Let's still recall what is the multi unbended that is we have like a bended machine with\n",
      "[01:03:34.020 --> 01:04:02.220]   eight different arms, one, two, tail eight.\n",
      "[01:04:02.220 --> 01:04:16.900]   The protocol for like a multi unbended goes follows that for t from one to capital T.\n",
      "[01:04:16.900 --> 01:04:37.540]   The learner picks an arm IT, IT is in basically the action set and then learner observed a\n",
      "[01:04:37.540 --> 01:05:00.340]   stochastic reward, alright.\n",
      "[01:05:00.340 --> 01:05:05.260]   We noted this here, this the reward stochastic is important because we no longer have the\n",
      "[01:05:05.260 --> 01:05:06.260]   transition.\n",
      "[01:05:06.260 --> 01:05:09.540]   The previous algorithm in EMDP we can study the tremendously reward is because usually\n",
      "[01:05:09.540 --> 01:05:13.220]   the transition is much harder to learn than the stochastic reward but here because we\n",
      "[01:05:13.220 --> 01:05:21.820]   have no transition so we have to like again say the reward is stochastic.\n",
      "[01:05:21.820 --> 01:05:29.620]   So we say have some normalization condition where we say I is always in zero to one that\n",
      "[01:05:29.620 --> 01:05:35.860]   is stochastic reward is always bounded in some zero to one and we have some expected\n",
      "[01:05:35.860 --> 01:05:45.140]   reward of I, expectation of I that is a random reward are going to receive for ice arm is\n",
      "[01:05:45.140 --> 01:06:10.140]   equal to little I, this is expected reward.\n",
      "[01:06:10.140 --> 01:06:33.420]   So, we will talk about two slightly different goals, but both of them are like standard\n",
      "[01:06:33.420 --> 01:06:40.300]   goals in the this multi arm balance the literature first let us first talk what is best arm best\n",
      "[01:06:40.300 --> 01:06:53.220]   arm is just define as a star is equal to arg max I of R essentially this is the arm that\n",
      "[01:06:53.220 --> 01:06:58.820]   achieved the highest expected reward and this is the arm we eventually want to find.\n",
      "[01:06:58.820 --> 01:07:08.060]   So there are two kinds of goals the first goal or sometimes we call it pack or sample\n",
      "[01:07:08.060 --> 01:07:25.620]   complexity or some other critical we essentially only want to find the the epsilon optimal\n",
      "[01:07:25.620 --> 01:07:42.260]   path epsilon optimal arm I hat so that by defining the epsilon optimal arm we say the\n",
      "[01:07:42.260 --> 01:07:50.660]   reward expected reward of I hat need to be greater equal to I star the best arm subtracted\n",
      "[01:07:50.660 --> 01:07:51.660]   by epsilon.\n",
      "[01:07:51.660 --> 01:08:05.260]   So, we at most tolerate some epsilon difference and sometimes we also okay with not only just\n",
      "[01:08:05.260 --> 01:08:12.420]   identifying an arm that we can also okay with like a identify a distribution D epsilon optimal\n",
      "[01:08:12.420 --> 01:08:34.780]   distribution D. So, that is the expectation of this reward the I I sample from D is greater\n",
      "[01:08:34.780 --> 01:08:41.060]   equal to I star subtracted by epsilon this is we also find like if even if you do not\n",
      "[01:08:41.060 --> 01:08:45.300]   identify a single arm but if you can say distribution and we say expectation this is\n",
      "[01:08:45.300 --> 01:08:52.100]   good this is I am also okay with this.\n",
      "[01:08:52.100 --> 01:09:04.540]   And there is a second type of goal in multi arm bunches which you can think this is like\n",
      "[01:09:04.540 --> 01:09:30.500]   a stronger objective which we want to achieve low regret where the regret is defined as\n",
      "[01:09:30.500 --> 01:09:52.300]   follows where the regret up to iteration t this is the number of iteration t is equal\n",
      "[01:09:52.300 --> 01:10:03.660]   to the best reward you can achieve t times I star this is like the best expected reward\n",
      "[01:10:03.660 --> 01:10:11.020]   you can achieve and subtracted the expected reward you receive by picking your own sequence\n",
      "[01:10:11.020 --> 01:10:30.420]   okay so this is essentially the expected reward\n",
      "[01:10:30.420 --> 01:10:53.220]   if always playing the optimal arm\n",
      "[01:10:53.220 --> 01:11:14.180]   and this is the expected reward to receive by the learner\n",
      "[01:11:14.180 --> 01:11:18.940]   and the regret is very intuitive by its name basically it says if regret is very large\n",
      "[01:11:18.940 --> 01:11:23.820]   you have a lot of regret if regret is small then then basically you do not regret much\n",
      "[01:11:23.820 --> 01:11:36.940]   of not playing optimally so I just want to say this because we have normalization reward\n",
      "[01:11:36.940 --> 01:11:52.060]   is in 0 to 1 so the regret is always like in 0 to t so regret is never going to be greater\n",
      "[01:11:52.060 --> 01:11:56.980]   than t so basically whatever you if you achieve some linear regret and that's already like\n",
      "[01:11:56.980 --> 01:12:02.820]   a trivial regret that doesn't say anything that's like very bad so the non trivial regret\n",
      "[01:12:02.820 --> 01:12:30.700]   is like we want to say sublinear or like square root t or something like that\n",
      "[01:12:30.700 --> 01:12:36.540]   so we make we kind of noted the most important difference between go one and go two go one\n",
      "[01:12:36.540 --> 01:13:03.220]   essentially it says player can play anything as long as you see eventually identify the\n",
      "[01:13:03.220 --> 01:13:26.020]   Epsilon optimal arm doesn't really care about in the middle like what you are playing you\n",
      "[01:13:26.020 --> 01:13:37.360]   can play poorly but as long as in the end you identify the Epsilon optimal arm this is\n",
      "[01:13:37.360 --> 01:13:46.760]   however in the go two you can think the second goal is like more difficult because the learner\n",
      "[01:13:46.760 --> 01:14:05.000]   needs to find the good arm which is similar to the best arm identification this but also\n",
      "[01:14:05.000 --> 01:14:27.480]   needs to do the play reasonably well in most steps so you cannot just play some like terrible\n",
      "[01:14:27.480 --> 01:14:32.320]   strategy in previous steps and in the end you realize what if I start in that case you\n",
      "[01:14:32.320 --> 01:14:40.240]   kind of pay some other t regret which is like not what you want so you can think this is\n",
      "[01:14:40.240 --> 01:14:55.320]   like what do we want to do in exploration and this is like what do we do in exploitation\n",
      "[01:14:55.320 --> 01:15:05.520]   so the goal of regret actually naturally reflects the tradeoff between exploration versus exploitation\n",
      "[01:15:05.520 --> 01:15:10.140]   so in reinforcement learning although so far we only talk about exploration we never\n",
      "[01:15:10.140 --> 01:15:14.280]   talk about regret we are okay with like the sample capacity but turns out because this\n",
      "[01:15:14.280 --> 01:15:18.960]   is like a sequential structure and we already talk about random exploration doesn't work\n",
      "[01:15:18.960 --> 01:15:24.320]   so it turns out the idea developed for this balancing exploration exploitation the optimism\n",
      "[01:15:24.320 --> 01:15:30.760]   actually is very useful in reinforcement learning so that is in the multi arm bandit we will\n",
      "[01:15:30.760 --> 01:15:34.840]   talk more about how we're going to balance exploration exploitation and motivates the\n",
      "[01:15:34.840 --> 01:15:40.440]   optimism and then we will talk about how to apply the algorithm the optimism principle\n",
      "[01:15:40.440 --> 01:15:44.840]   to reinforcement learning.\n",
      "[01:15:44.840 --> 01:15:47.420]   (upbeat music)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/739Ddvx1OIU.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/739Ddvx1OIU.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/739Ddvx1OIU.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/739Ddvx1OIU.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1290.06 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   2 h\n",
      "whisper_print_timings:      mel time =  2570.50 ms\n",
      "whisper_print_timings:   sample time = 18754.69 ms / 49257 runs (    0.38 ms per run)\n",
      "whisper_print_timings:   encode time =   359.46 ms /   196 runs (    1.83 ms per run)\n",
      "whisper_print_timings:   decode time =   679.78 ms /   358 runs (    1.90 ms per run)\n",
      "whisper_print_timings:   batchd time = 25163.24 ms / 47921 runs (    0.53 ms per run)\n",
      "whisper_print_timings:   prompt time =  9961.80 ms / 43672 runs (    0.23 ms per run)\n",
      "whisper_print_timings:    total time = 59213.90 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=72QWY6ubS14 started\n",
      "72QWY6ubS14\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/72QWY6ubS14.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/72QWY6ubS14.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:19:29.82, start: 0.000000, bitrate: 254 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 154 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/72QWY6ubS14.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  149057kB time=01:19:29.81 bitrate= 256.0kbits/s speed=1.41e+03x    \n",
      "video:0kB audio:149057kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000051%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/72QWY6ubS14.wav' (76317118 samples, 4769.8 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.240]   [MUSIC PLAYING]\n",
      "[00:00:03.240 --> 00:00:05.880]   So in the last Thursday, we talk about--\n",
      "[00:00:05.880 --> 00:00:08.280]   we end with the value-tuation algorithm\n",
      "[00:00:08.280 --> 00:00:10.080]   in the simulator set setting.\n",
      "[00:00:10.080 --> 00:00:12.320]   So we just recap what is the algorithm this.\n",
      "[00:00:12.320 --> 00:00:21.800]   This is like simulator setting, or we also\n",
      "[00:00:21.800 --> 00:00:23.520]   call this like generated model.\n",
      "[00:00:29.680 --> 00:00:32.000]   So essentially, we have already talking about the value-tuation\n",
      "[00:00:32.000 --> 00:00:35.760]   algorithm in the planning task, where\n",
      "[00:00:35.760 --> 00:00:38.280]   we know the transition and the reward.\n",
      "[00:00:38.280 --> 00:00:40.680]   The only difference here is now in the simulator setting,\n",
      "[00:00:40.680 --> 00:00:42.200]   we don't know the transition.\n",
      "[00:00:42.200 --> 00:00:44.780]   So the first step is we need to kind of\n",
      "[00:00:44.780 --> 00:00:46.520]   estimate the transition.\n",
      "[00:00:46.520 --> 00:00:51.400]   So the way we estimate transition is for all states\n",
      "[00:00:51.400 --> 00:01:03.640]   action horizon pair triple in all states action on H.\n",
      "[00:01:03.640 --> 00:01:04.560]   We do the following.\n",
      "[00:01:04.560 --> 00:01:09.560]   We kind of query the simulator.\n",
      "[00:01:09.560 --> 00:01:13.680]   Just remember, in a simulator, we take a state action\n",
      "[00:01:13.680 --> 00:01:16.640]   and step as an input.\n",
      "[00:01:16.640 --> 00:01:21.560]   And we kind of query the simulator n times.\n",
      "[00:01:21.560 --> 00:01:24.360]   So we will collect n samples of the next states.\n",
      "[00:01:24.360 --> 00:01:33.920]   Let us suppose we click with S1 prime, S2 prime, et cetera,\n",
      "[00:01:33.920 --> 00:01:34.960]   to SM prime.\n",
      "[00:01:34.960 --> 00:01:41.160]   And we will just do a very simple estimation\n",
      "[00:01:41.160 --> 00:01:42.880]   of the transition probability using\n",
      "[00:01:42.880 --> 00:01:46.600]   the empirical frequency of some state, some next state\n",
      "[00:01:46.600 --> 00:01:47.100]   appears.\n",
      "[00:01:47.100 --> 00:01:59.320]   So we'll estimate the transition by this.\n",
      "[00:01:59.320 --> 00:02:05.920]   P hat H, S prime, and the general SA,\n",
      "[00:02:05.920 --> 00:02:08.360]   is equal to 1 over n.\n",
      "[00:02:08.360 --> 00:02:12.920]   That is, total samples we kind of will query this simulator.\n",
      "[00:02:12.920 --> 00:02:18.480]   And times the indicator of how many times\n",
      "[00:02:18.480 --> 00:02:31.000]   I have seen S i prime to be exactly S prime, i from 1 to n,\n",
      "[00:02:31.000 --> 00:02:41.480]   for all S prime, n S. And essentially, this\n",
      "[00:02:41.480 --> 00:02:44.080]   is the only thing new in the simulator setting.\n",
      "[00:02:44.080 --> 00:02:47.640]   And the remaining setting will be the same as the valetration\n",
      "[00:02:47.640 --> 00:02:48.520]   in planning.\n",
      "[00:02:48.520 --> 00:02:52.080]   That is, we just do bound by automatic equation for H equal\n",
      "[00:02:52.080 --> 00:03:00.040]   to 1, et cetera, H to 1.\n",
      "[00:03:00.040 --> 00:03:14.080]   To the following, this will compute Q star hat H, S A\n",
      "[00:03:14.080 --> 00:03:26.280]   equal to R H, S A, and plus P hat, V H\n",
      "[00:03:26.280 --> 00:03:33.520]   minus 1 hat, star, S A. This is like the first step\n",
      "[00:03:33.520 --> 00:03:35.440]   of the valent-optimal equation, where\n",
      "[00:03:35.440 --> 00:03:38.160]   we replace all the Q star V star by the hat version.\n",
      "[00:03:38.160 --> 00:03:39.840]   That is our estimated version.\n",
      "[00:03:39.840 --> 00:03:42.360]   And the only important difference is\n",
      "[00:03:42.360 --> 00:03:44.840]   because we no longer have access to the transition.\n",
      "[00:03:44.840 --> 00:03:47.200]   So we use our estimate of the transition\n",
      "[00:03:47.200 --> 00:03:52.120]   to replace the true one in the valent-optimal equation.\n",
      "[00:03:52.120 --> 00:03:55.080]   And we will also do the same for the V value.\n",
      "[00:03:55.080 --> 00:04:11.720]   V star H is equal to max A in A, Q hat star, S A.\n",
      "[00:04:11.720 --> 00:04:14.240]   So essentially, we talk about this in the very end\n",
      "[00:04:14.240 --> 00:04:15.560]   of the last lecture.\n",
      "[00:04:15.560 --> 00:04:18.800]   We're saying the VI algorithm in a simulator setting\n",
      "[00:04:18.800 --> 00:04:21.440]   could be composed of two components.\n",
      "[00:04:21.440 --> 00:04:22.680]   The first component is essentially\n",
      "[00:04:22.680 --> 00:04:27.160]   we use the first for loop to estimate the transition P.\n",
      "[00:04:27.160 --> 00:04:30.360]   Essentially, we get the P hat is our estimate.\n",
      "[00:04:30.360 --> 00:04:32.560]   And the second for loop is just a standard\n",
      "[00:04:32.560 --> 00:04:33.960]   value-optimal equation.\n",
      "[00:04:33.960 --> 00:04:37.960]   And we use dynamic programming to compute the optimal value.\n",
      "[00:04:37.960 --> 00:04:40.160]   And finally, we just output the policy.\n",
      "[00:04:47.560 --> 00:04:56.880]   That is V hat S. equal to the-- essentially,\n",
      "[00:04:56.880 --> 00:04:59.480]   the up-policy we output is just the greedy policy\n",
      "[00:04:59.480 --> 00:05:00.800]   induced by Q hat star.\n",
      "[00:05:00.800 --> 00:05:07.600]   So the second phase, essentially, we have an estimated P hat.\n",
      "[00:05:07.600 --> 00:05:11.160]   We know this is not necessarily exactly equal to the P,\n",
      "[00:05:11.160 --> 00:05:16.640]   but we consider it to be close to the up-true transition\n",
      "[00:05:16.640 --> 00:05:17.480]   in some sense.\n",
      "[00:05:17.480 --> 00:05:20.240]   So essentially, we just do optimum-optimal equation\n",
      "[00:05:20.240 --> 00:05:22.520]   using this P hat, treat it as a P,\n",
      "[00:05:22.520 --> 00:05:24.800]   and also do the output as the same.\n",
      "[00:05:24.800 --> 00:05:26.560]   Essentially, we just do everything treated\n",
      "[00:05:26.560 --> 00:05:32.240]   this P hat as a true transition.\n",
      "[00:05:32.240 --> 00:05:33.300]   So this is the algorithm.\n",
      "[00:05:33.300 --> 00:05:54.380]   So we clearly think this algorithm kind of makes sense,\n",
      "[00:05:54.380 --> 00:05:57.620]   especially-- let's convince ourselves, when\n",
      "[00:05:57.620 --> 00:06:01.980]   angle to infinity, we know by law of large number,\n",
      "[00:06:01.980 --> 00:06:06.940]   this P hat will converge to the true transition, P.\n",
      "[00:06:06.940 --> 00:06:08.860]   In that sense, essentially, in the first step,\n",
      "[00:06:08.860 --> 00:06:10.820]   we will learn accurate transition.\n",
      "[00:06:10.820 --> 00:06:12.840]   And the second step, we just do optimum-optimal equation.\n",
      "[00:06:12.840 --> 00:06:16.020]   So this is clearly an algorithm, or a synthetically correct\n",
      "[00:06:16.020 --> 00:06:19.380]   algorithm, when angle to infinity.\n",
      "[00:06:19.380 --> 00:06:22.820]   But since we have learned a lot of tools in the last week\n",
      "[00:06:22.820 --> 00:06:25.940]   or the week before on my concentration inequality,\n",
      "[00:06:25.940 --> 00:06:28.100]   so we'll actually give a much stronger result.\n",
      "[00:06:28.100 --> 00:06:32.500]   We're not only talking that when angle to infinity,\n",
      "[00:06:32.500 --> 00:06:33.580]   we get the correct result.\n",
      "[00:06:33.580 --> 00:06:37.060]   But we will also talk about precisely how many samples\n",
      "[00:06:37.060 --> 00:06:41.740]   we need to find some like, epsilon-optimal policy.\n",
      "[00:06:41.740 --> 00:06:46.660]   So today, we'll go to talk about our first statistical\n",
      "[00:06:46.660 --> 00:06:49.420]   guarantees, like first guarantees for statistical\n",
      "[00:06:49.420 --> 00:06:50.340]   reinforcement learning.\n",
      "[00:06:54.460 --> 00:07:04.060]   So theorem one, we say there exists an absolute constant,\n",
      "[00:07:04.060 --> 00:07:24.540]   C, if we choose this sample size n,\n",
      "[00:07:24.540 --> 00:07:26.420]   where n is the number of samples we\n",
      "[00:07:26.420 --> 00:07:30.940]   need to query simulator for each sah pair, which\n",
      "[00:07:30.940 --> 00:07:36.020]   is this n to be greater or equal to c times h to the power\n",
      "[00:07:36.020 --> 00:07:40.660]   fourth, and some yota over epsilon square,\n",
      "[00:07:40.660 --> 00:07:43.660]   where yota will essentially throughout this course,\n",
      "[00:07:43.660 --> 00:07:47.980]   we'll use yota to just essentially mean some log factor.\n",
      "[00:07:47.980 --> 00:07:57.620]   Yota is equal to the log h sa over p,\n",
      "[00:07:57.620 --> 00:08:08.860]   then with probability at least 1 minus p,\n",
      "[00:08:08.860 --> 00:08:22.100]   the output pi hat of vi will be epsilon-optimal.\n",
      "[00:08:22.100 --> 00:08:36.820]   So we can think this theorem actually not only says\n",
      "[00:08:36.820 --> 00:08:38.740]   like when n goes to infinity, this is accurate.\n",
      "[00:08:38.740 --> 00:08:40.500]   We actually give a precisely bound.\n",
      "[00:08:40.500 --> 00:08:43.740]   We essentially says when n is greater than some absolute\n",
      "[00:08:43.740 --> 00:08:47.260]   constant times this amount, then we\n",
      "[00:08:47.260 --> 00:08:50.540]   are guaranteed this output of the value iteration algorithm\n",
      "[00:08:50.540 --> 00:08:52.380]   is actually epsilon-optimal.\n",
      "[00:08:52.380 --> 00:08:55.580]   And just to recall that by definition of epsilon-optimal,\n",
      "[00:08:55.580 --> 00:09:01.180]   last time we say we define this in terms of a v star 1\n",
      "[00:09:01.180 --> 00:09:10.500]   s1 minus v pi hat 1 s1, this is less than epsilon.\n",
      "[00:09:10.500 --> 00:09:17.220]   That is the pi hat policy.\n",
      "[00:09:17.220 --> 00:09:20.300]   When we evaluate the value at the first step in the initial state,\n",
      "[00:09:20.300 --> 00:09:22.300]   it is very, very close to the optimal value.\n",
      "[00:09:22.300 --> 00:09:51.100]   [INAUDIBLE]\n",
      "[00:09:51.100 --> 00:09:53.060]   What, why is it constant?\n",
      "[00:09:53.060 --> 00:09:54.740]   [INAUDIBLE]\n",
      "[00:09:54.740 --> 00:09:56.860]   Like, yeah, you can just put it there.\n",
      "[00:09:56.860 --> 00:10:01.120]   But I think in this course, we always separate [INAUDIBLE]\n",
      "[00:10:01.120 --> 00:10:03.780]   because a lot of times the log term can be complicated.\n",
      "[00:10:03.780 --> 00:10:05.620]   But for this course, I think usually we\n",
      "[00:10:05.620 --> 00:10:07.420]   don't really care about a log factor.\n",
      "[00:10:07.420 --> 00:10:09.980]   So we can just think this is like some small multiplicative\n",
      "[00:10:09.980 --> 00:10:11.060]   factor.\n",
      "[00:10:11.060 --> 00:10:13.340]   Or sometimes when you write some o tilde,\n",
      "[00:10:13.340 --> 00:10:15.780]   you can just get a read of this.\n",
      "[00:10:15.780 --> 00:10:20.940]   [INAUDIBLE]\n",
      "[00:10:20.940 --> 00:10:21.440]   Yes.\n",
      "[00:10:21.440 --> 00:10:21.940]   Yes.\n",
      "[00:10:21.940 --> 00:10:33.440]   [INAUDIBLE]\n",
      "[00:10:33.440 --> 00:10:35.300]   OK, the question is, where the epsilon came from?\n",
      "[00:10:35.300 --> 00:10:38.260]   Epsilon is in the guarantee.\n",
      "[00:10:38.260 --> 00:10:40.580]   That is, if we want to guarantee the epsilon-optimal\n",
      "[00:10:40.580 --> 00:10:42.980]   policy, we need to choose the sample sign status\n",
      "[00:10:42.980 --> 00:10:44.900]   scale with 1 or epsilon square.\n",
      "[00:10:44.900 --> 00:10:46.900]   That is, if we want to be more accurate,\n",
      "[00:10:46.900 --> 00:10:48.940]   then we need more samples.\n",
      "[00:10:48.940 --> 00:10:51.460]   [INAUDIBLE]\n",
      "[00:10:51.460 --> 00:10:54.200]   Yeah, can be set set by us.\n",
      "[00:10:54.200 --> 00:10:54.700]   Yes.\n",
      "[00:10:54.700 --> 00:11:02.380]   [INAUDIBLE]\n",
      "[00:11:02.380 --> 00:11:06.420]   I think for this course, we will just--\n",
      "[00:11:06.420 --> 00:11:09.580]   like we said, for simplicity, we always make s1\n",
      "[00:11:09.580 --> 00:11:11.380]   to be like the fixed initial states.\n",
      "[00:11:11.380 --> 00:11:13.940]   So we always assume we have any fixed initial states.\n",
      "[00:11:13.940 --> 00:11:16.300]   But if you say your initial state is sample from some\n",
      "[00:11:16.300 --> 00:11:18.180]   distribution, you can also say this\n",
      "[00:11:18.180 --> 00:11:21.100]   is an expectation of s1 and sample\n",
      "[00:11:21.100 --> 00:11:23.660]   from some distribution.\n",
      "[00:11:23.660 --> 00:11:24.500]   This is true.\n",
      "[00:11:24.500 --> 00:11:26.260]   You can also guarantee the same thing\n",
      "[00:11:26.260 --> 00:11:27.780]   with the same number of samples.\n",
      "[00:11:27.780 --> 00:11:36.860]   P is the probability with probability of 1, 1 minus p.\n",
      "[00:11:36.860 --> 00:11:52.740]   [INAUDIBLE]\n",
      "[00:11:52.740 --> 00:11:53.260]   OK.\n",
      "[00:11:53.260 --> 00:11:57.100]   So we will first make some comments about theorem 1,\n",
      "[00:11:57.100 --> 00:11:58.860]   and then we will spend essentially\n",
      "[00:11:58.860 --> 00:12:03.380]   a lot of time in this lecture to prove the theorem.\n",
      "[00:12:03.380 --> 00:12:05.900]   So first, we will talk about what is the sample\n",
      "[00:12:05.900 --> 00:12:10.100]   capacity this theorem gives us.\n",
      "[00:12:10.100 --> 00:12:12.740]   Sample capacity, as we discussed last time,\n",
      "[00:12:12.740 --> 00:12:15.300]   is essentially we will define it as how many times\n",
      "[00:12:15.300 --> 00:12:18.300]   we need to query the simulator in total\n",
      "[00:12:18.300 --> 00:12:21.020]   to find this epsilon optimal policy.\n",
      "[00:12:21.020 --> 00:12:22.740]   So in this case, somehow capacity\n",
      "[00:12:22.740 --> 00:12:29.940]   is we have n samples for each state action edge pair.\n",
      "[00:12:29.940 --> 00:12:32.660]   So that means we need n samples times the number\n",
      "[00:12:32.660 --> 00:12:36.020]   of states, times the number of action, times the number\n",
      "[00:12:36.020 --> 00:12:36.620]   of steps.\n",
      "[00:12:36.620 --> 00:12:38.740]   So this is a total amount of samples\n",
      "[00:12:38.740 --> 00:12:42.900]   we need to get the epsilon accurate policy.\n",
      "[00:12:42.900 --> 00:12:46.700]   And we can essentially just multiply them together.\n",
      "[00:12:46.700 --> 00:12:50.340]   And it becomes something like, oh, tilde.\n",
      "[00:12:50.340 --> 00:12:51.820]   Oh, tilde, as we said, essentially\n",
      "[00:12:51.820 --> 00:12:56.060]   we will hide some constant factor and logarithmic factor.\n",
      "[00:12:56.060 --> 00:12:58.540]   And it will become some H to the fifths,\n",
      "[00:12:58.540 --> 00:13:02.500]   sA over epsilon squared.\n",
      "[00:13:02.500 --> 00:13:13.180]   So essentially, we're claiming that the value iteration\n",
      "[00:13:13.180 --> 00:13:17.020]   will use some polynomial for the horizon,\n",
      "[00:13:17.020 --> 00:13:20.020]   and linear in states, linear in action,\n",
      "[00:13:20.020 --> 00:13:23.980]   and over epsilon squared samples to learn the optimal policy,\n",
      "[00:13:23.980 --> 00:13:25.300]   to learn the epsilon optimal policy.\n",
      "[00:13:25.300 --> 00:13:31.460]   One very important thing I want to claim\n",
      "[00:13:31.460 --> 00:13:36.220]   is let's just remember in P, P is a transition.\n",
      "[00:13:36.220 --> 00:13:44.380]   And if you remember, what is number of parameter,\n",
      "[00:13:44.380 --> 00:13:46.780]   or number of degree of freedom in this transition,\n",
      "[00:13:46.780 --> 00:13:49.780]   the transition essentially depends on the next states\n",
      "[00:13:49.780 --> 00:13:53.860]   and the previous states and the action.\n",
      "[00:13:53.860 --> 00:14:00.140]   So essentially, you can think this P is a matrix, or any pH.\n",
      "[00:14:00.140 --> 00:14:07.980]   pH is a matrix that is in S by S A.\n",
      "[00:14:07.980 --> 00:14:12.060]   So the transition actually has S squared a parameters.\n",
      "[00:14:12.060 --> 00:14:23.180]   So the transition at each step has S squared a parameters.\n",
      "[00:14:23.180 --> 00:14:25.300]   And if you have H transition at the time,\n",
      "[00:14:25.300 --> 00:14:28.980]   you kind of multiply the entire thing by H.\n",
      "[00:14:28.980 --> 00:14:31.060]   But let's ignore the H dependence here.\n",
      "[00:14:31.060 --> 00:14:33.300]   The important thing is, although the transition actually\n",
      "[00:14:33.300 --> 00:14:36.700]   has S squared a dependence, we actually\n",
      "[00:14:36.700 --> 00:14:41.220]   don't need to pay S squared in the sample complexity.\n",
      "[00:14:41.220 --> 00:14:43.020]   It turns out the entire samples we\n",
      "[00:14:43.020 --> 00:14:47.300]   need to learn the MDP or learn the accurate policy\n",
      "[00:14:47.300 --> 00:14:49.340]   is only depends on linear in S.\n",
      "[00:14:49.340 --> 00:15:04.300]   So we'll actually talk about this fact later.\n",
      "[00:15:04.300 --> 00:15:06.560]   So it turns out the intuitive answer is like,\n",
      "[00:15:06.560 --> 00:15:17.260]   we don't need to learn this transition\n",
      "[00:15:17.260 --> 00:15:20.100]   pH point-wise accurately.\n",
      "[00:15:20.100 --> 00:15:35.300]   We'll talk about in what sense is this point-wise\n",
      "[00:15:35.300 --> 00:15:36.780]   and that kind of thing later.\n",
      "[00:15:36.780 --> 00:15:42.180]   This is the first comment.\n",
      "[00:15:42.180 --> 00:15:54.500]   The second comment is, we actually\n",
      "[00:15:54.500 --> 00:16:09.140]   see the sample complexity scales with one of the epsilon\n",
      "[00:16:09.140 --> 00:16:10.260]   squared.\n",
      "[00:16:10.260 --> 00:16:13.060]   This is another important kind of dependency\n",
      "[00:16:13.060 --> 00:16:17.660]   we need to know how it depends on the epsilon.\n",
      "[00:16:17.660 --> 00:16:20.140]   So this is in terms of a number of sample\n",
      "[00:16:20.140 --> 00:16:21.460]   scale with one of epsilon.\n",
      "[00:16:21.460 --> 00:16:23.900]   So let's just pretend like this is the entire time of the set.\n",
      "[00:16:23.900 --> 00:16:26.940]   If we make this equal to n, this essentially\n",
      "[00:16:26.940 --> 00:16:38.640]   says that arrow epsilon is going to scale with one\n",
      "[00:16:38.640 --> 00:16:39.520]   over square root of n.\n",
      "[00:16:39.520 --> 00:16:47.480]   And we see this is like a typical arrow rate\n",
      "[00:16:47.480 --> 00:16:49.000]   we see from concentration.\n",
      "[00:16:49.000 --> 00:16:54.320]   Typical statistical arrow.\n",
      "[00:17:07.800 --> 00:17:11.120]   So no matter in happening or in Princeton,\n",
      "[00:17:11.120 --> 00:17:13.080]   the leading order term in terms of arrow\n",
      "[00:17:13.080 --> 00:17:15.880]   is always like decreases one over square root of n.\n",
      "[00:17:15.880 --> 00:17:18.840]   So this is essentially where it comes from.\n",
      "[00:17:18.840 --> 00:17:32.160]   So in the first comment, we talk about this assay dependence.\n",
      "[00:17:32.160 --> 00:17:34.320]   The second comment talk about the epsilon dependence.\n",
      "[00:17:34.320 --> 00:17:38.600]   So the final, I want to also comment a bit about the H dependence.\n",
      "[00:17:38.600 --> 00:17:42.040]   So it turns out-- so in this class,\n",
      "[00:17:42.040 --> 00:17:44.720]   we will just introduce a slightly simpler proof.\n",
      "[00:17:44.720 --> 00:17:47.680]   We will essentially get the H to the fifth sample\n",
      "[00:17:47.680 --> 00:17:48.800]   complexity.\n",
      "[00:17:48.800 --> 00:17:51.880]   But this H to the fifth can actually\n",
      "[00:17:51.880 --> 00:18:03.600]   be improved to H to the fourth by using slightly tighter\n",
      "[00:18:03.600 --> 00:18:04.320]   analysis.\n",
      "[00:18:04.320 --> 00:18:15.120]   So we will actually introduce the course analysis\n",
      "[00:18:15.120 --> 00:18:21.440]   as like what we-- like in this class to make it simple.\n",
      "[00:18:21.440 --> 00:18:25.600]   And we will leave this analysis in the homework too.\n",
      "[00:18:25.600 --> 00:18:28.880]   And we will talk about what is their key steps to achieve\n",
      "[00:18:28.880 --> 00:18:30.680]   in this tighter analysis.\n",
      "[00:18:30.680 --> 00:18:41.200]  , and later, we will actually also\n",
      "[00:18:41.200 --> 00:18:46.520]   have lower bound to say like this is essentially\n",
      "[00:18:46.520 --> 00:18:50.040]   information optimal, information theoretical optimal.\n",
      "[00:18:50.040 --> 00:18:54.400]   So you cannot improve to H cube or something.\n",
      "[00:18:54.400 --> 00:18:59.840]   This H force is already a shock.\n",
      "[00:18:59.840 --> 00:19:02.640]   There is no way to get better than H to the fourth\n",
      "[00:19:02.640 --> 00:19:03.960]   by any algorithm.\n",
      "[00:19:03.960 --> 00:19:06.600]   There is just no way to-- like no matter what algorithm you use,\n",
      "[00:19:06.600 --> 00:19:08.480]   you cannot get a better than H to the fourth.\n",
      "[00:19:08.480 --> 00:19:12.360]   This is information theoretical look.\n",
      "[00:19:12.360 --> 00:19:14.440]   And you might think this H to the fourth\n",
      "[00:19:14.440 --> 00:19:17.600]   is kind of like to some higher part on your hand.\n",
      "[00:19:17.600 --> 00:19:20.800]   You don't know like why we have H to the fourth there.\n",
      "[00:19:20.800 --> 00:19:23.160]   But it turns out to--\n",
      "[00:19:23.160 --> 00:19:26.360]   I think some of the H is actually\n",
      "[00:19:26.360 --> 00:19:32.200]   because of our characterization of H, epsilon.\n",
      "[00:19:32.200 --> 00:19:35.160]   Because you can imagine in our MDP,\n",
      "[00:19:35.160 --> 00:19:37.920]   we cannot essentially assume the reward at every step\n",
      "[00:19:37.920 --> 00:19:39.840]   is only from 0 to 1.\n",
      "[00:19:39.840 --> 00:19:42.800]   That means the value itself is from 0 to H.\n",
      "[00:19:42.800 --> 00:19:45.960]   So the value itself has the H scaling it.\n",
      "[00:19:45.960 --> 00:19:47.760]   So you can think epsilon is also something\n",
      "[00:19:47.760 --> 00:19:51.320]   should be in 0 to H. So in that sense, although you\n",
      "[00:19:51.320 --> 00:19:55.160]   have H to the fourth, but essentially two of the H\n",
      "[00:19:55.160 --> 00:19:56.640]   has the same scaling as epsilon.\n",
      "[00:19:56.640 --> 00:20:00.120]   So you can think epsilon cancels with some H scaling.\n",
      "[00:20:00.120 --> 00:20:03.800]   So some intrinsic H dependency here is just H squared.\n",
      "[00:20:03.800 --> 00:20:12.480]   So essentially, you can decompose this into H squared\n",
      "[00:20:12.480 --> 00:20:14.120]   times H squared.\n",
      "[00:20:14.120 --> 00:20:15.120]   And this is intrinsic.\n",
      "[00:20:19.120 --> 00:20:25.600]   This is the essentially scaling cancels\n",
      "[00:20:25.600 --> 00:20:34.200]   with-- so what I mean cancels with epsilon\n",
      "[00:20:34.200 --> 00:20:36.200]   is, instead of assuming this epsilon,\n",
      "[00:20:36.200 --> 00:20:38.240]   you can assume this epsilon H. Then you just\n",
      "[00:20:38.240 --> 00:20:39.680]   get a read off H squared.\n",
      "[00:20:39.680 --> 00:20:41.640]   This is just because some scaling we choose.\n",
      "[00:20:41.640 --> 00:20:53.880]   OK.\n",
      "[00:20:53.880 --> 00:20:57.920]   Any questions about the theorem?\n",
      "[00:20:57.920 --> 00:20:59.520]   So I think this is our first theorem,\n",
      "[00:20:59.520 --> 00:21:01.360]   talking about some sample complexity.\n",
      "[00:21:01.360 --> 00:21:03.920]   So it's kind of I have a lot of statistical flavor.\n",
      "[00:21:03.920 --> 00:21:07.720]   And we go to talk about what is exactly a sample complexity.\n",
      "[00:21:07.720 --> 00:21:10.560]   And we talk about how each dependency--\n",
      "[00:21:10.560 --> 00:21:11.920]   why each dependency makes sense?\n",
      "[00:21:11.920 --> 00:21:20.240]   If no questions, we'll go to prove the theorem 1.\n",
      "[00:21:20.240 --> 00:21:26.200]   So the theorem 1 is actually not very hard to prove.\n",
      "[00:21:26.200 --> 00:21:33.800]   So I first introduced the first lemma.\n",
      "[00:21:33.800 --> 00:21:35.760]   We only needed two lemmas.\n",
      "[00:21:35.760 --> 00:21:37.160]   Actually, one of them, you already\n",
      "[00:21:37.160 --> 00:21:40.520]   did it in the homework 1.\n",
      "[00:21:40.520 --> 00:21:42.480]   So the first lemma--\n",
      "[00:21:42.480 --> 00:21:43.960]   I think the first lemma is essentially\n",
      "[00:21:43.960 --> 00:21:47.760]   we want to guarantee the value has the absolute arrow.\n",
      "[00:21:47.760 --> 00:21:51.640]   But all we have is we have some estimating the transition.\n",
      "[00:21:51.640 --> 00:21:52.960]   So the first lemma, essentially, we\n",
      "[00:21:52.960 --> 00:21:56.000]   want to relate the difference in terms\n",
      "[00:21:56.000 --> 00:21:58.680]   of value to the difference in terms of transition.\n",
      "[00:21:58.680 --> 00:22:00.360]   And so later, we can essentially bound\n",
      "[00:22:00.360 --> 00:22:02.880]   the difference in terms of transition by concentration.\n",
      "[00:22:02.880 --> 00:22:06.320]   And we kind of get the result down.\n",
      "[00:22:06.320 --> 00:22:18.840]   So lemma 1 is about a relation of value difference\n",
      "[00:22:18.840 --> 00:22:20.440]   and the transition difference.\n",
      "[00:22:30.960 --> 00:22:41.480]   So the lemma state as follows, that for any policy pi,\n",
      "[00:22:41.480 --> 00:22:54.720]   and any state horizon pair in S and H,\n",
      "[00:22:54.720 --> 00:22:59.000]   we have the following equation holds.\n",
      "[00:22:59.000 --> 00:23:20.320]   That is the v pi S minus v hat pi H S\n",
      "[00:23:20.320 --> 00:23:25.480]   equal to expectation.\n",
      "[00:23:25.480 --> 00:23:27.640]   We'll talk about what's this notation later.\n",
      "[00:23:27.640 --> 00:23:35.280]   M hat pi and the summation of the difference\n",
      "[00:23:35.280 --> 00:23:50.960]   i from H to capital H p i minus p i hat v i plus 1 pi S i\n",
      "[00:23:50.960 --> 00:23:59.520]   a i conditional SH equal to S. So we\n",
      "[00:23:59.520 --> 00:24:07.080]   have the following holds for any H and S. I already\n",
      "[00:24:07.080 --> 00:24:18.960]   says this for any H. So first, we introduce\n",
      "[00:24:18.960 --> 00:24:21.560]   what is this notation of M hat?\n",
      "[00:24:21.560 --> 00:24:26.960]   M hat, I don't remember if I say this in the last lecture.\n",
      "[00:24:26.960 --> 00:24:29.960]   But M hat, essentially, you can think\n",
      "[00:24:29.960 --> 00:24:34.360]   this is a new MDP, where you use the P hat as your transition\n",
      "[00:24:34.360 --> 00:24:35.640]   and R as your reward.\n",
      "[00:24:35.640 --> 00:24:37.800]   This is the M hat.\n",
      "[00:24:37.800 --> 00:24:41.440]   So taking expectation over M hat means essentially we're\n",
      "[00:24:41.440 --> 00:24:44.680]   saying here, just remember the random thing here\n",
      "[00:24:44.680 --> 00:24:47.160]   is the future trajectory.\n",
      "[00:24:47.160 --> 00:24:54.240]   That is, we'll have A H, SH plus 1, A H plus 1,\n",
      "[00:24:54.240 --> 00:25:02.120]   and R to SH H. So all those things\n",
      "[00:25:02.120 --> 00:25:04.400]   are random in this expression.\n",
      "[00:25:04.400 --> 00:25:07.400]   So taking expectation by M hat and pi\n",
      "[00:25:07.400 --> 00:25:11.600]   means A, all of the action A are sampled from pi.\n",
      "[00:25:11.600 --> 00:25:14.400]   And all of the next states, SH plus 1\n",
      "[00:25:14.400 --> 00:25:19.680]   until S capital H is sampled from this P hat.\n",
      "[00:25:19.680 --> 00:25:30.960]   So there's a sample, according to pi hat, a P hat and pi.\n",
      "[00:25:30.960 --> 00:25:43.640]   So this is the lemma 1.\n",
      "[00:25:43.640 --> 00:25:45.720]   Although the formula is a little bit complicated,\n",
      "[00:25:45.720 --> 00:25:50.320]   but essentially, we will see very soon that is essentially\n",
      "[00:25:50.320 --> 00:25:53.400]   if you just expand the value by Bellman equation,\n",
      "[00:25:53.400 --> 00:25:56.720]   and you immediately see why this thing comes in.\n",
      "[00:25:56.720 --> 00:25:58.560]   It's just by calculation.\n",
      "[00:25:58.560 --> 00:26:02.200]   And we know this is the value in difference, difference in value.\n",
      "[00:26:09.400 --> 00:26:18.000]   And this is a difference in transition.\n",
      "[00:26:18.000 --> 00:26:27.600]   So we achieve our goal, where essentially we\n",
      "[00:26:27.600 --> 00:26:30.240]   want to establish a relation between the value difference\n",
      "[00:26:30.240 --> 00:26:32.360]   and the transition difference.\n",
      "[00:26:32.360 --> 00:26:34.640]   And we can essentially use this as our bound.\n",
      "[00:26:34.640 --> 00:26:38.680]   Say if the transition has no arrow or this arrow is very small,\n",
      "[00:26:38.680 --> 00:26:40.280]   then you can expect it.\n",
      "[00:26:40.280 --> 00:26:44.760]   So this value is small just by looking at this formula.\n",
      "[00:26:44.760 --> 00:26:49.760]   So we'll prove the lemma 1.\n",
      "[00:26:49.760 --> 00:26:57.800]   Any questions about the first lemma?\n",
      "[00:26:57.800 --> 00:27:14.720]   You may also wonder why this is like asymmetric.\n",
      "[00:27:14.720 --> 00:27:20.160]   Like we have n hat here and instead of n.\n",
      "[00:27:20.160 --> 00:27:24.880]   So actually, you can also make-- this is like can be swapped.\n",
      "[00:27:24.880 --> 00:27:27.320]   You can also make n here.\n",
      "[00:27:27.320 --> 00:27:30.320]   But instead of v here, you will have v hat here.\n",
      "[00:27:30.320 --> 00:27:32.160]   So in some sense, this is symmetric.\n",
      "[00:27:32.160 --> 00:27:35.480]   So you need to have hat here and now hat here.\n",
      "[00:27:35.480 --> 00:27:38.160]   Or you have no hat here, but hat here.\n",
      "[00:27:38.160 --> 00:27:40.680]   And then your hat need to appear to one of the place.\n",
      "[00:27:40.680 --> 00:27:46.080]   So we'll talk about proof of lemma 1.\n",
      "[00:27:46.080 --> 00:27:53.080]   As we said, proof is essentially just\n",
      "[00:27:53.080 --> 00:28:02.440]   follow the Bellman equation, so by Bellman's equation,\n",
      "[00:28:02.440 --> 00:28:15.040]   we have following that as we look at the Q hat, or QH,\n",
      "[00:28:15.040 --> 00:28:26.520]   let's say, subtract by Q hat pi H, and this\n",
      "[00:28:26.520 --> 00:28:35.440]   is equal to pH vH plus 1 pi, let's say,\n",
      "[00:28:35.440 --> 00:28:47.480]   subtracted by p hat H v hat H plus 1 pi, let's say.\n",
      "[00:28:47.480 --> 00:28:59.320]   I think I also should also say this v hat notation\n",
      "[00:28:59.320 --> 00:29:04.040]   is defined as a value of this MDP.\n",
      "[00:29:04.040 --> 00:29:07.200]   So essentially, we have this modified MDP\n",
      "[00:29:07.200 --> 00:29:10.280]   and the value of pi on this modified MDP\n",
      "[00:29:10.280 --> 00:29:11.840]   is denoted as v hat.\n",
      "[00:29:11.840 --> 00:29:13.880]   So this Q hat is also different and denoted\n",
      "[00:29:13.880 --> 00:29:20.000]   as the value of a policy pi on this modified version of MDP.\n",
      "[00:29:20.000 --> 00:29:21.920]   So both just by Bellman equation.\n",
      "[00:29:21.920 --> 00:29:25.440]   The first is by original MDP, so we expanded by PV.\n",
      "[00:29:25.440 --> 00:29:27.400]   And the second is the modified MDP,\n",
      "[00:29:27.400 --> 00:29:28.720]   so we have a p hat v hat.\n",
      "[00:29:33.960 --> 00:29:35.920]   This is very standard.\n",
      "[00:29:35.920 --> 00:29:40.320]   And then we just add a subtractor term.\n",
      "[00:29:40.320 --> 00:29:46.400]   So we essentially have a PV subtracted p hat v hat,\n",
      "[00:29:46.400 --> 00:29:50.440]   and we will add a p hat v, subtracted p hat v,\n",
      "[00:29:50.440 --> 00:29:53.840]   and plus p hat v back.\n",
      "[00:29:53.840 --> 00:30:03.760]   So this is equal to p subtracted by p hat times v H plus 1 pi,\n",
      "[00:30:03.760 --> 00:30:21.840]   let's say, plus p hat H times v H plus 1 pi,\n",
      "[00:30:21.840 --> 00:30:25.400]   subtracted by v H plus 1 pi hat.\n",
      "[00:30:25.400 --> 00:30:32.080]   So all we are doing is that we subtract this term,\n",
      "[00:30:32.080 --> 00:30:36.840]   subtract the p hat v, and we add it back, add the p hat v back.\n",
      "[00:30:36.840 --> 00:30:38.200]   So we didn't change anything.\n",
      "[00:30:38.200 --> 00:30:41.760]   We just subtracted and add back the same term.\n",
      "[00:30:41.760 --> 00:30:49.560]   So this essentially gives us some structure.\n",
      "[00:30:49.560 --> 00:30:52.440]   We know the difference in a Q value can decompose\n",
      "[00:30:52.440 --> 00:30:56.080]   as some difference in a p in a transition\n",
      "[00:30:56.080 --> 00:30:58.840]   times the value of the true MDP.\n",
      "[00:30:58.840 --> 00:31:02.880]   And plus, essentially, the value difference\n",
      "[00:31:02.880 --> 00:31:06.600]   of the next step after we apply the transition\n",
      "[00:31:06.600 --> 00:31:08.760]   of the modified MDP.\n",
      "[00:31:08.760 --> 00:31:13.960]   So you can see this is why we have that kind of formula there.\n",
      "[00:31:13.960 --> 00:31:17.840]   So all we need to do is just we do one more step.\n",
      "[00:31:17.840 --> 00:31:20.320]   This is like a VQ value.\n",
      "[00:31:20.320 --> 00:31:26.560]   So we can, by Bellman equation, we cancel the V value,\n",
      "[00:31:26.560 --> 00:31:32.640]   subtracted by V value of this H.\n",
      "[00:31:32.640 --> 00:31:34.920]   So in terms of V value, all we need to do\n",
      "[00:31:34.920 --> 00:31:38.480]   is we're just taking the action, like taking action,\n",
      "[00:31:38.480 --> 00:31:42.560]   taking over expectation over this policy pi.\n",
      "[00:31:42.560 --> 00:31:46.920]   So we just write it as expectation,\n",
      "[00:31:46.920 --> 00:31:52.520]   taking expectation over action through policy pi,\n",
      "[00:31:52.520 --> 00:32:03.560]   and the pH, subtracted by pH hat, V H plus 1 pi,\n",
      "[00:32:03.560 --> 00:32:13.680]   SHAH, conditional SH equal to S. This is the first term,\n",
      "[00:32:13.680 --> 00:32:15.560]   and we can further write the second term.\n",
      "[00:32:21.080 --> 00:32:24.160]   It turns out, the second term is precisely,\n",
      "[00:32:24.160 --> 00:32:27.640]   we can take expectation over M hat and pi.\n",
      "[00:32:27.640 --> 00:32:29.720]   So pi is something we need to take expectation\n",
      "[00:32:29.720 --> 00:32:31.240]   because we do the V value.\n",
      "[00:32:31.240 --> 00:32:34.000]   And we can essentially absorb this transition P hat\n",
      "[00:32:34.000 --> 00:32:37.160]   into taking samples according to P hat.\n",
      "[00:32:37.160 --> 00:32:41.320]   So sample according to this modified MDP.\n",
      "[00:32:41.320 --> 00:32:44.560]   And we look at the value difference at next step.\n",
      "[00:32:44.560 --> 00:33:07.400]   [INAUDIBLE]\n",
      "[00:33:07.400 --> 00:33:17.640]   The second term again.\n",
      "[00:33:17.640 --> 00:33:21.520]   I think we can also just quickly write this out.\n",
      "[00:33:21.520 --> 00:33:26.120]   By notation of this notation, we\n",
      "[00:33:26.120 --> 00:33:33.280]   know this is equal to taking expectation over S prime sample\n",
      "[00:33:33.280 --> 00:33:42.400]   from P hat dot SA, and this is S prime.\n",
      "[00:33:42.400 --> 00:33:47.400]   So we're essentially just writing this thing as an M hat.\n",
      "[00:33:47.400 --> 00:33:50.040]   And then we take another pi over the action.\n",
      "[00:33:50.040 --> 00:33:52.120]   So this is all we need, all we did.\n",
      "[00:33:52.120 --> 00:33:53.540]   We didn't do anything else.\n",
      "[00:33:53.540 --> 00:34:01.560]   I think the important thing is to observe this is precisely\n",
      "[00:34:01.560 --> 00:34:05.400]   corresponding to we take a next step according\n",
      "[00:34:05.400 --> 00:34:07.200]   to this modified MDP.\n",
      "[00:34:07.200 --> 00:34:09.520]   So that's why we can write in this compact form.\n",
      "[00:34:09.520 --> 00:34:18.480]   So we didn't do anything complicated.\n",
      "[00:34:18.480 --> 00:34:22.080]   And we're writing some compact notation\n",
      "[00:34:22.080 --> 00:34:26.960]   so that we can see things through a recursive fashion.\n",
      "[00:34:26.960 --> 00:34:32.920]   So we can think this is essentially relates the value\n",
      "[00:34:32.920 --> 00:34:39.480]   difference in H step to the transition difference in H step\n",
      "[00:34:39.480 --> 00:34:43.880]   and the value difference in the next step in H plus 1 step.\n",
      "[00:34:43.880 --> 00:34:46.320]   So we can essentially expand the recursion.\n",
      "[00:34:55.920 --> 00:34:58.840]   Expanding recursion means for the value in the next step,\n",
      "[00:34:58.840 --> 00:35:01.080]   we can, again, apply the same argument.\n",
      "[00:35:01.080 --> 00:35:04.720]   And we can decompose into the transition difference\n",
      "[00:35:04.720 --> 00:35:07.560]   in the next step, H plus 1 step, and the value step\n",
      "[00:35:07.560 --> 00:35:10.440]   in value difference in H plus 2 step.\n",
      "[00:35:10.440 --> 00:35:11.920]   And we can go on and on.\n",
      "[00:35:11.920 --> 00:35:15.100]   And eventually, we prove this let them.\n",
      "[00:35:15.100 --> 00:35:18.100]   [INAUDIBLE]\n",
      "[00:35:18.100 --> 00:35:46.360]   [INAUDIBLE]\n",
      "[00:35:46.360 --> 00:35:48.520]   And in question so far about this proof.\n",
      "[00:35:48.520 --> 00:35:58.520]   [BLANK_AUDIO]\n",
      "[00:35:58.520 --> 00:36:08.520]   [BLANK_AUDIO]\n",
      "[00:36:08.520 --> 00:36:18.520]   [BLANK_AUDIO]\n",
      "[00:36:18.520 --> 00:36:28.520]   [BLANK_AUDIO]\n",
      "[00:36:28.520 --> 00:36:36.520]   [BLANK_AUDIO]\n",
      "[00:36:36.520 --> 00:36:45.520]   [BLANK_AUDIO]\n",
      "[00:36:45.520 --> 00:36:50.520]   [BLANK_AUDIO]\n",
      "[00:36:50.520 --> 00:36:52.440]   And yes, please.\n",
      "[00:36:52.440 --> 00:37:02.440]   [BLANK_AUDIO]\n",
      "[00:37:02.440 --> 00:37:06.440]   Sorry, could you say the question again, second part of your question?\n",
      "[00:37:06.440 --> 00:37:12.440]   [BLANK_AUDIO]\n",
      "[00:37:12.440 --> 00:37:20.440]   Right, first term only has pi, but I think this is also, this is only H and this is H plus 1.\n",
      "[00:37:20.440 --> 00:37:30.440]   And when you expand this again, like a VH plus 1, that will again equal to the first term has this, but then you have m hat outside.\n",
      "[00:37:30.440 --> 00:37:39.440]   So this is where you're getting, you can think this VH plus 1 again will be equal to pH plus 1 minus pH plus 1 hat.\n",
      "[00:37:39.440 --> 00:37:45.440]   And then value difference in the H plus 2 step, where the pH plus 1 minus pH plus 1 hat.\n",
      "[00:37:45.440 --> 00:37:47.440]   Now it's inside this expectation.\n",
      "[00:37:47.440 --> 00:37:50.440]   So that's why you have m hat here.\n",
      "[00:37:50.440 --> 00:37:53.440]   [BLANK_AUDIO]\n",
      "[00:37:53.440 --> 00:37:56.440]   Okay, I would just, I would just do a bit more.\n",
      "[00:37:56.440 --> 00:37:59.440]   [BLANK_AUDIO]\n",
      "[00:37:59.440 --> 00:38:05.440]   What I'm saying is you will have the, let's say in the high level, you will have like a delta V.\n",
      "[00:38:05.440 --> 00:38:07.440]   This is like a value difference.\n",
      "[00:38:07.440 --> 00:38:15.440]   In the first step is equal to expectation of pi as called delta p.\n",
      "[00:38:15.440 --> 00:38:20.440]   And delta p is essentially a lot of things there.\n",
      "[00:38:20.440 --> 00:38:29.440]   And plus E pi hat, E pi m hat and delta V in 2.\n",
      "[00:38:29.440 --> 00:38:42.440]   So delta V in 2, you will also have expectation of pi of delta p2 and plus expectation of pi and hat and delta V3.\n",
      "[00:38:42.440 --> 00:38:46.440]   So what I'm saying is you kind of plug it in inside here.\n",
      "[00:38:46.440 --> 00:38:51.440]   Then the p2 term will actually not only have this expectation in pi.\n",
      "[00:38:51.440 --> 00:38:53.440]   That is taking expectation of A2.\n",
      "[00:38:53.440 --> 00:38:57.440]   But you also have this expectation of E pi and m hat.\n",
      "[00:38:57.440 --> 00:39:02.440]   That is taking expectation of the first, the second state and the first action.\n",
      "[00:39:02.440 --> 00:39:07.440]   So eventually you will, eventually when you kind of expand this outside,\n",
      "[00:39:07.440 --> 00:39:19.440]   eventually you will have like delta V1 is equal to some summation of E m hat pi and delta p h from 1 to capital H.\n",
      "[00:39:19.440 --> 00:39:25.440]   Where this expectation is actually like a concatenation of a lot of expectation,\n",
      "[00:39:25.440 --> 00:39:28.440]   of like state action at the different steps.\n",
      "[00:39:28.440 --> 00:39:31.440]   [INAUDIBLE]\n",
      "[00:39:31.440 --> 00:39:34.440]   P1 actually doesn't have m hat, you're right.\n",
      "[00:39:34.440 --> 00:39:38.440]   But if P1 essentially has no next state, so you don't.\n",
      "[00:39:38.440 --> 00:39:44.440]   Yeah, P1 is like first step, so you don't have m hat there.\n",
      "[00:39:44.440 --> 00:40:13.440]   [BLANK_AUDIO]\n",
      "[00:40:13.440 --> 00:40:16.440]   Okay, so we will talk about the second lemma.\n",
      "[00:40:16.440 --> 00:40:18.440]   So the first lemma is pretty straightforward.\n",
      "[00:40:18.440 --> 00:40:20.440]   It's just by directly by Bellman optimization.\n",
      "[00:40:20.440 --> 00:40:24.440]   Essentially we decompose the value difference by the transition difference.\n",
      "[00:40:24.440 --> 00:40:29.440]   You can expect of cross, like if transition are very close, then two values should be very close.\n",
      "[00:40:29.440 --> 00:40:33.440]   Essentially we just have the mathematical formula to that kind of decomposition.\n",
      "[00:40:33.440 --> 00:40:40.440]   And the second step essentially now we can say we can bound the value difference by transition difference.\n",
      "[00:40:40.440 --> 00:40:45.440]   The second step essentially will use concentration to say how much value can be different.\n",
      "[00:40:45.440 --> 00:40:46.440]   [BLANK_AUDIO]\n",
      "[00:40:46.440 --> 00:40:48.440]   So we'll do lemma two.\n",
      "[00:40:48.440 --> 00:40:51.440]   [BLANK_AUDIO]\n",
      "[00:40:51.440 --> 00:40:55.440]   Lemma two is essentially what we have already proved in homework one.\n",
      "[00:40:55.440 --> 00:40:57.440]   So for fixed.\n",
      "[00:40:57.440 --> 00:41:01.440]   [BLANK_AUDIO]\n",
      "[00:41:01.440 --> 00:41:06.440]   S, A, H and fixed value function.\n",
      "[00:41:06.440 --> 00:41:09.440]   [BLANK_AUDIO]\n",
      "[00:41:09.440 --> 00:41:14.440]   V, let's suppose in zero to the H to the power of S.\n",
      "[00:41:14.440 --> 00:41:19.440]   That is a S dimensional vector with value taken from zero to H.\n",
      "[00:41:19.440 --> 00:41:21.440]   [BLANK_AUDIO]\n",
      "[00:41:21.440 --> 00:41:30.440]   We say with probability greater equal to one minus P over HSA.\n",
      "[00:41:30.440 --> 00:41:34.440]   [BLANK_AUDIO]\n",
      "[00:41:34.440 --> 00:41:36.440]   We have that so.\n",
      "[00:41:36.440 --> 00:41:46.440]   [BLANK_AUDIO]\n",
      "[00:41:46.440 --> 00:41:56.440]   Let's say we have, we have that the, this P H, subject by P hat H.\n",
      "[00:41:56.440 --> 00:42:00.440]   Where P hat H is our estimated in value iteration algorithm.\n",
      "[00:42:00.440 --> 00:42:07.440]   Times the, this value at S A is less or equal to.\n",
      "[00:42:07.440 --> 00:42:10.440]   [BLANK_AUDIO]\n",
      "[00:42:10.440 --> 00:42:17.440]   C times H Yota of N.\n",
      "[00:42:17.440 --> 00:42:20.440]   Where this Yota is equal to.\n",
      "[00:42:20.440 --> 00:42:25.440]   [BLANK_AUDIO]\n",
      "[00:42:25.440 --> 00:42:29.440]   Log HSA over P.\n",
      "[00:42:29.440 --> 00:42:38.440]   [BLANK_AUDIO]\n",
      "[00:42:38.440 --> 00:42:43.440]   So this is actually precisely the very last question of the homework one.\n",
      "[00:42:43.440 --> 00:42:54.440]   [BLANK_AUDIO]\n",
      "[00:42:54.440 --> 00:43:00.440]   With slightly rescaling where in the homework one, we kind of say the value is in minus one, one, two H to S.\n",
      "[00:43:00.440 --> 00:43:06.440]   Here we say zero H to the S, so we kind of have additional H factor here just because of scaling.\n",
      "[00:43:06.440 --> 00:43:13.440]   And also I think there, we don't precisely get this, like this probability, you can just say one minus data.\n",
      "[00:43:13.440 --> 00:43:18.440]   But you can easily replace this by like this, this entire quantity as data.\n",
      "[00:43:18.440 --> 00:43:22.440]   And we choose this specific P over HSA because later we want to do a union bound.\n",
      "[00:43:22.440 --> 00:43:26.440]   But in the homework, essentially you prove this with probability one minus data.\n",
      "[00:43:26.440 --> 00:43:30.440]   And you can do everything with the C one and log one over data over N.\n",
      "[00:43:30.440 --> 00:43:48.440]   [BLANK_AUDIO]\n",
      "[00:43:48.440 --> 00:43:56.440]   And I don't know how many of you have done the homework, but essentially this is down by applying half-dins concentration in quality.\n",
      "[00:43:56.440 --> 00:44:06.440]   [BLANK_AUDIO]\n",
      "[00:44:06.440 --> 00:44:28.440]   [BLANK_AUDIO]\n",
      "[00:44:28.440 --> 00:44:38.440]   Which is a very good exercise after we learn all the concentration in your quality.\n",
      "[00:44:38.440 --> 00:44:48.440]   [BLANK_AUDIO]\n",
      "[00:44:48.440 --> 00:45:10.440]   [BLANK_AUDIO]\n",
      "[00:45:10.440 --> 00:45:12.440]   Okay, so any questions?\n",
      "[00:45:12.440 --> 00:45:17.440]   I guess the proof will be left as a homework, already said in homework one.\n",
      "[00:45:17.440 --> 00:45:22.440]   And I think I want to emphasize something a bit more like I think in the earlier phase,\n",
      "[00:45:22.440 --> 00:45:28.440]   we talk about the sample complexity where we think one very non-trivial factor of this like tabular MDP.\n",
      "[00:45:28.440 --> 00:45:33.440]   We don't actually require the S square in number of samples to learn the transition.\n",
      "[00:45:33.440 --> 00:45:35.440]   We actually only require SA.\n",
      "[00:45:35.440 --> 00:45:42.440]   So I think the key thing here is we actually only do this for fix value function.\n",
      "[00:45:42.440 --> 00:45:45.440]   Or we can extend this.\n",
      "[00:45:45.440 --> 00:45:53.440]   This can be extended to random value function.\n",
      "[00:45:53.440 --> 00:46:00.440]   [BLANK_AUDIO]\n",
      "[00:46:00.440 --> 00:46:10.440]   But the randomness need to be independent.\n",
      "[00:46:10.440 --> 00:46:17.440]   [BLANK_AUDIO]\n",
      "[00:46:17.440 --> 00:46:19.440]   Of p hat.\n",
      "[00:46:19.440 --> 00:46:22.440]   Okay.\n",
      "[00:46:22.440 --> 00:46:26.440]   So we need to know what is the probability statement here.\n",
      "[00:46:26.440 --> 00:46:31.440]   So this is a probability statement, that is with the probability of 1 minus p over SA.\n",
      "[00:46:31.440 --> 00:46:36.440]   So the only thing random in this quantity, like this is a fixed value function,\n",
      "[00:46:36.440 --> 00:46:38.440]   and this is an original transition.\n",
      "[00:46:38.440 --> 00:46:39.440]   There's no randomness here.\n",
      "[00:46:39.440 --> 00:46:43.440]   The only thing random here is the p hat, where the samples we collect.\n",
      "[00:46:43.440 --> 00:46:48.440]   Remember this p hat, we use empirical frequency to estimate this transition.\n",
      "[00:46:48.440 --> 00:46:53.440]   So the sample we collected as one.\n",
      "[00:46:53.440 --> 00:46:58.440]   So this is with probability over the sample we collected as one prime.\n",
      "[00:46:58.440 --> 00:47:02.440]   So those samples will essentially form the p hat.\n",
      "[00:47:02.440 --> 00:47:04.440]   Okay, so those things are only random things.\n",
      "[00:47:04.440 --> 00:47:09.440]   And we're saying, we actually, this does not only works for\n",
      "[00:47:09.440 --> 00:47:12.440]   fixed value function, but also works for random value function.\n",
      "[00:47:12.440 --> 00:47:18.440]   As long as this randomness is independent of p hat, or independent of this S1 to Sn prime.\n",
      "[00:47:18.440 --> 00:47:28.440]   So the reason, like we can handle this independent thing is, in that case,\n",
      "[00:47:28.440 --> 00:47:31.440]   we can first just draw this random function.\n",
      "[00:47:31.440 --> 00:47:36.440]   And then apply this lemma tool, because after we draw the random function first,\n",
      "[00:47:36.440 --> 00:47:41.440]   and because this p hat is completely independent of this random value function.\n",
      "[00:47:41.440 --> 00:47:47.440]   So we can still basically apply the same thing, and we get the same result.\n",
      "[00:47:47.440 --> 00:47:52.440]   Okay, and later here, it's actually very important.\n",
      "[00:47:52.440 --> 00:47:57.440]   We will use this fact.\n",
      "[00:48:10.440 --> 00:48:21.440]   We also want to make some comments that lemma tool essentially says,\n",
      "[00:48:21.440 --> 00:48:31.440]   essentially says as long as n\n",
      "[00:48:31.440 --> 00:48:39.440]   equal to some theta polyage, or the theta polyage.\n",
      "[00:48:39.440 --> 00:48:43.440]   So let's say H is just some scaling, so let's not care about H.\n",
      "[00:48:43.440 --> 00:48:47.440]   So we already have concentration.\n",
      "[00:49:05.440 --> 00:49:12.440]   It's an all state action pair, or an S8.\n",
      "[00:49:12.440 --> 00:49:24.440]   So that means the total number of samples is like, again, n times HSA,\n",
      "[00:49:24.440 --> 00:49:29.440]   which will be scaling as S8 instead of S2.\n",
      "[00:49:29.440 --> 00:49:34.440]   So this lemma tool essentially already kind of contains the most important factor.\n",
      "[00:49:34.440 --> 00:49:40.440]   We say we don't require S2, a number of samples to learn a value function well.\n",
      "[00:49:40.440 --> 00:49:43.440]   We actually only require S8 to learn a function value well.\n",
      "[00:49:43.440 --> 00:49:48.440]   Because for each S8 pair, we actually doesn't need to pay anything in terms of S.\n",
      "[00:49:48.440 --> 00:49:52.440]   We just pay as long as this n equal to some polyage,\n",
      "[00:49:52.440 --> 00:50:02.440]   we can push this arrow to be something very, very small, and we kind of already in some very good shape.\n",
      "[00:50:02.440 --> 00:50:09.440]   I just want to say this in general very important thing is we actually consider this as a fixed value function,\n",
      "[00:50:09.440 --> 00:50:12.440]   or like an independent value function.\n",
      "[00:50:12.440 --> 00:50:16.440]   So I just want to say in general if this is not fixed value function,\n",
      "[00:50:16.440 --> 00:50:20.440]   if this is like some adversarial value function, this is not true.\n",
      "[00:50:20.440 --> 00:50:25.440]   So we don't have the sharp guarantees.\n",
      "[00:50:25.440 --> 00:50:34.440]   So in PSAT 1, we also kind of have another results.\n",
      "[00:50:34.440 --> 00:50:38.440]   We will ask you to prove this.\n",
      "[00:50:38.440 --> 00:50:51.440]   P hat minus P is an infinite norm, or TV norm.\n",
      "[00:50:51.440 --> 00:51:00.440]   TV distance is less or equal to 0 tilde, square root S over n.\n",
      "[00:51:00.440 --> 00:51:04.440]   This has some logarithmic term.\n",
      "[00:51:04.440 --> 00:51:08.440]   So this is what we traditionally in the sense,\n",
      "[00:51:08.440 --> 00:51:14.440]   if we really require some TV distance probability to learn accurately,\n",
      "[00:51:14.440 --> 00:51:19.440]   we typically measure in terms of total variance distance.\n",
      "[00:51:19.440 --> 00:51:24.440]   In this way, we actually see we actually require n to be something linear in S.\n",
      "[00:51:24.440 --> 00:51:38.440]   This is also equivalent to saying we need the max S,\n",
      "[00:51:38.440 --> 00:51:52.440]   P hat, P and this V value.\n",
      "[00:51:52.440 --> 00:51:58.440]   This is less than 0 tilde, square root S over n.\n",
      "[00:51:58.440 --> 00:52:08.440]   So here, this is like 0 tilde 1 over square root of n.\n",
      "[00:52:08.440 --> 00:52:19.440]   If ignoring the H dependence in here.\n",
      "[00:52:19.440 --> 00:52:22.440]   So you can see if we use this type of guarantees,\n",
      "[00:52:22.440 --> 00:52:25.440]   then this will actually lose by a factor of S.\n",
      "[00:52:25.440 --> 00:52:30.440]   For every state of action, we already need to pay some number of S samples\n",
      "[00:52:30.440 --> 00:52:34.440]   to push this arrow to be something very small.\n",
      "[00:52:34.440 --> 00:52:40.440]   In that case, we will pay S square A samples.\n",
      "[00:52:40.440 --> 00:52:45.440]   So the major difference between here and there is this is like for fixed value\n",
      "[00:52:45.440 --> 00:52:48.440]   or for random value independent of this P hat.\n",
      "[00:52:48.440 --> 00:52:51.440]   Well, this can be for a zero value.\n",
      "[00:52:51.440 --> 00:52:55.440]   This is essentially because we are taking max over P of the value,\n",
      "[00:52:55.440 --> 00:53:05.440]   so this is a adversarial for P hat.\n",
      "[00:53:05.440 --> 00:53:08.440]   Now, this we can first draw some randomness P hat,\n",
      "[00:53:08.440 --> 00:53:14.440]   and then we can essentially pick some V that is like the worst case against the P hat.\n",
      "[00:53:14.440 --> 00:53:27.440]   So of course, this is like a very loose.\n",
      "[00:53:27.440 --> 00:53:38.440]   So the key observation here is for reinforcement learning.\n",
      "[00:53:38.440 --> 00:53:43.440]   So our task is just want to learn some absolute optimal policy.\n",
      "[00:53:43.440 --> 00:54:03.440]   Now, it's not necessary to learn P\n",
      "[00:54:03.440 --> 00:54:16.440]   that is accurate enough for all V.\n",
      "[00:54:16.440 --> 00:54:25.440]   So this is not necessary, which i is in like in TV distance or something.\n",
      "[00:54:25.440 --> 00:54:48.440]   So we only need to make P minus P hat\n",
      "[00:54:48.440 --> 00:55:12.440]   accurate, concentrate for certain these with good property.\n",
      "[00:55:12.440 --> 00:55:17.440]   Good property in terms of either is fixed or is like random,\n",
      "[00:55:17.440 --> 00:55:41.440]   which saves as sample complexity as factor sample complexity.\n",
      "[00:55:41.440 --> 00:55:44.440]   So the key takeaway is essentially in short for reinforcement learning,\n",
      "[00:55:44.440 --> 00:55:46.440]   all we care is the policy to be great.\n",
      "[00:55:46.440 --> 00:55:51.440]   So in that case, it turns out we don't actually need to learn the transition very\n",
      "[00:55:51.440 --> 00:55:53.440]   accurate in terms of in this sense.\n",
      "[00:55:53.440 --> 00:55:57.440]   Like we don't need to learn transition so accurate so that it works for any V,\n",
      "[00:55:57.440 --> 00:55:58.440]   this is accurate.\n",
      "[00:55:58.440 --> 00:56:03.440]   We only need for some specific type of V, like either fixed or not like random,\n",
      "[00:56:03.440 --> 00:56:07.440]   as long as this expectation is accurate and then we are good.\n",
      "[00:56:07.440 --> 00:56:12.440]   So that's why we kind of bypass the number of parameters in the reinforcement learning.\n",
      "[00:56:12.440 --> 00:56:17.440]   We don't pay a square, we only pay a sec.\n",
      "[00:56:17.440 --> 00:56:23.440]   So we will see this in the final proof for the theorem very soon.\n",
      "[00:56:23.440 --> 00:56:50.440]   Okay, any questions about this part of discussion?\n",
      "[00:56:50.440 --> 00:57:00.440]   If no questions we will just directly to prove the theorem one,\n",
      "[00:57:00.440 --> 00:57:13.440]   we will immediately see where it comes in mathematically about this key observation.\n",
      "[00:57:13.440 --> 00:57:17.440]   Okay, so now we are at the final stuff.\n",
      "[00:57:17.440 --> 00:57:22.440]   So final stuff we will actually look at what we want to bound.\n",
      "[00:57:22.440 --> 00:57:27.440]   The thing we want to bound is we say the sub-optimality of pi hat,\n",
      "[00:57:27.440 --> 00:57:33.440]   which by definition we need to look at this V1 of pi star as one,\n",
      "[00:57:33.440 --> 00:57:39.440]   subtract V1 of pi hat as one.\n",
      "[00:57:39.440 --> 00:57:43.440]   Okay?\n",
      "[00:57:43.440 --> 00:57:47.440]   And remember in the, like we first, we want to bridge this to,\n",
      "[00:57:47.440 --> 00:57:53.440]   we want to like link this to the transition difference so that we can use the concentration.\n",
      "[00:57:53.440 --> 00:57:58.440]   However, just remember our lemma one, we actually, we actually use something different.\n",
      "[00:57:58.440 --> 00:58:04.440]   We actually calculate the value difference in terms of the same policy but different MDP.\n",
      "[00:58:04.440 --> 00:58:10.440]   So we are doing V minus V hat, but we need the same policy.\n",
      "[00:58:10.440 --> 00:58:15.440]   So here we essentially use the lemma by just add a term and subtract a term,\n",
      "[00:58:15.440 --> 00:58:25.440]   where we do V1 of pi star as one, subtract by V1 hat, pi star as one.\n",
      "[00:58:25.440 --> 00:58:36.440]   And plus V1 hat, pi star as one, subtract by V1 hat, pi hat as one.\n",
      "[00:58:36.440 --> 00:58:50.440]   And plus V1 hat, pi hat as one, subtract by V1, pi hat as one.\n",
      "[00:58:50.440 --> 00:58:54.440]   So we subtract term and add it back, and subtract a term and add it back.\n",
      "[00:58:54.440 --> 00:58:57.440]   So this is like precisely equal.\n",
      "[00:58:57.440 --> 00:59:00.440]   And now we have three terms essentially.\n",
      "[00:59:00.440 --> 00:59:03.440]   We will group the first two together as a term A.\n",
      "[00:59:03.440 --> 00:59:08.440]   And group the last two together as a term B.\n",
      "[00:59:08.440 --> 00:59:31.440]   And for this term, we know this is always smaller than zero because pi hat is actually optimal\n",
      "[00:59:31.440 --> 00:59:36.440]   for M hat.\n",
      "[00:59:36.440 --> 00:59:41.440]   Just remember how we computed this pi hat in this value iteration algorithm.\n",
      "[00:59:41.440 --> 00:59:45.440]   We directly do the Bellman automatic equation on this M hat.\n",
      "[00:59:45.440 --> 00:59:47.440]   And this like modified MDP.\n",
      "[00:59:47.440 --> 00:59:51.440]   So that's why on this modified MDP pi hat is actually the optimal policy\n",
      "[00:59:51.440 --> 00:59:54.440]   because it's satisfied the Bellman automatic equation there.\n",
      "[00:59:54.440 --> 00:59:57.440]   Well, pi star, although it's optimal policy of the original MDP,\n",
      "[00:59:57.440 --> 01:00:00.440]   is suboptimal in this M hat.\n",
      "[01:00:00.440 --> 01:00:03.440]   So that's why this value is actually greater than this value.\n",
      "[01:00:03.440 --> 01:00:06.440]   So this is smaller equal to zero.\n",
      "[01:00:06.440 --> 01:00:08.440]   So in order to upper bound this value difference,\n",
      "[01:00:08.440 --> 01:00:11.440]   all we need to upper bound is the term A and term B,\n",
      "[01:00:11.440 --> 01:00:14.440]   which is precisely goes back to the lemma one case\n",
      "[01:00:14.440 --> 01:00:18.440]   where we look at the value on different MDP on different transition\n",
      "[01:00:18.440 --> 01:00:22.440]   but on same policy.\n",
      "[01:00:22.440 --> 01:00:23.440]   OK.\n",
      "[01:00:23.440 --> 01:00:27.440]   So now let's bound the term one and term A and term B.\n",
      "[01:00:27.440 --> 01:00:36.440]   This is equal to directly by lemma one.\n",
      "[01:00:36.440 --> 01:00:56.440]   M hat pi and summation h from one to capital H.\n",
      "[01:00:56.440 --> 01:01:01.440]   What is this P?\n",
      "[01:01:01.440 --> 01:01:05.440]   Subtract P hat.\n",
      "[01:01:05.440 --> 01:01:10.440]   V h plus one star.\n",
      "[01:01:10.440 --> 01:01:13.440]   S-I-A-I.\n",
      "[01:01:13.440 --> 01:01:15.440]   Conditioner S one.\n",
      "[01:01:15.440 --> 01:01:34.440]   S-I-A-I.\n",
      "[01:01:34.440 --> 01:01:39.440]   So this is just directly by lemma one.\n",
      "[01:01:39.440 --> 01:01:43.440]   But the most important thing we observe here is,\n",
      "[01:01:43.440 --> 01:01:45.440]   if we want to use the lemma two,\n",
      "[01:01:45.440 --> 01:01:48.440]   we actually need to argue about whether V being fixed\n",
      "[01:01:48.440 --> 01:01:51.440]   or V being independent random thing.\n",
      "[01:01:51.440 --> 01:01:55.440]   But this is the optimal value function of the true MDP,\n",
      "[01:01:55.440 --> 01:01:58.440]   where true MDP is not anything random.\n",
      "[01:01:58.440 --> 01:02:02.440]   True MDP exists on a line P and R, so it's not random at all.\n",
      "[01:02:02.440 --> 01:02:07.440]   So this is actually a fixed value.\n",
      "[01:02:07.440 --> 01:02:13.440]   So in the sense it doesn't depend on S one prime, S two prime to SM prime at all.\n",
      "[01:02:13.440 --> 01:02:23.440]   So where we can safely apply the lemma two.\n",
      "[01:02:23.440 --> 01:02:27.440]   We'll actually say by lemma two,\n",
      "[01:02:27.440 --> 01:02:29.440]   with probability one minus P.\n",
      "[01:02:29.440 --> 01:02:32.440]   Because lemma two essentially say for each state of action,\n",
      "[01:02:32.440 --> 01:02:37.440]   this event holds with a probability one minus P over HSA.\n",
      "[01:02:37.440 --> 01:02:39.440]   And then we can just use a union bound.\n",
      "[01:02:39.440 --> 01:02:42.440]   So we say for one minus P probability,\n",
      "[01:02:42.440 --> 01:02:46.440]   this thing actually holds simultaneously for all S-A-H pairs.\n",
      "[01:02:46.440 --> 01:02:49.440]   Essentially the last time we talk about the union bound.\n",
      "[01:02:49.440 --> 01:02:53.440]   So we can just apply the same concentration for all state of action.\n",
      "[01:02:53.440 --> 01:03:21.440]   The union bound will apply lemma two to all S-A-H simultaneously.\n",
      "[01:03:21.440 --> 01:03:34.440]   So we say this is smaller than C, just times...\n",
      "[01:03:34.440 --> 01:03:37.440]   So each term, if we push expectation inside,\n",
      "[01:03:37.440 --> 01:03:42.440]   each term is smaller than C times H and Eta over N.\n",
      "[01:03:42.440 --> 01:03:44.440]   But we have an additional summation there,\n",
      "[01:03:44.440 --> 01:03:54.440]   so we just say summation H from one to capital H.\n",
      "[01:03:54.440 --> 01:04:04.440]   And this is equal to C times H squared and Eta over N.\n",
      "[01:04:04.440 --> 01:04:08.440]   And eventually just remember in our...\n",
      "[01:04:08.440 --> 01:04:17.440]   In the theorem one, we actually choose N to be greater or equal to C prime\n",
      "[01:04:17.440 --> 01:04:21.440]   for some large constant C times H to the fourth,\n",
      "[01:04:21.440 --> 01:04:24.440]   Eta over epsilon squared.\n",
      "[01:04:24.440 --> 01:04:27.440]   So essentially we'll use Eta to cancel Eta and H to the fourths\n",
      "[01:04:27.440 --> 01:04:29.440]   to cancel H squared here.\n",
      "[01:04:29.440 --> 01:04:33.440]   And we choose constant and large enough so that it absorbs anything constant here.\n",
      "[01:04:33.440 --> 01:04:37.440]   And then we say this is less than or equal to epsilon over true.\n",
      "[01:04:37.440 --> 01:04:40.440]   Which is a lot constant and large enough.\n",
      "[01:04:40.440 --> 01:04:47.440]   So it absorbs this constant but also has an additional factor of two here.\n",
      "[01:04:47.440 --> 01:04:48.440]   Yes.\n",
      "[01:04:48.440 --> 01:04:52.440]   Where does the small H use?\n",
      "[01:04:52.440 --> 01:04:54.440]   Small H use?\n",
      "[01:04:54.440 --> 01:04:55.440]   Oh, so...\n",
      "[01:04:55.440 --> 01:04:56.440]   Where is this used?\n",
      "[01:04:56.440 --> 01:04:57.440]   This small H.\n",
      "[01:04:57.440 --> 01:05:00.440]   So essentially in this concentration we actually,\n",
      "[01:05:00.440 --> 01:05:05.440]   although this is applied for the transition difference for small H,\n",
      "[01:05:05.440 --> 01:05:07.440]   and because of all we have N samples.\n",
      "[01:05:07.440 --> 01:05:10.440]   So the upper bound actually doesn't depend on H.\n",
      "[01:05:10.440 --> 01:05:12.440]   Although the transition difference is like at H staff,\n",
      "[01:05:12.440 --> 01:05:14.440]   but upper bound doesn't depend on H.\n",
      "[01:05:14.440 --> 01:05:19.440]   So what we are doing is we push the summation outside.\n",
      "[01:05:19.440 --> 01:05:25.440]   And for each staff, we apply the lemma two and so is this bound.\n",
      "[01:05:25.440 --> 01:05:28.440]   The upper bound actually doesn't depend on small H.\n",
      "[01:05:28.440 --> 01:05:33.440]   So that's why when you do the summation you just simply have a multiple factor of H.\n",
      "[01:05:33.440 --> 01:05:38.440]   Okay.\n",
      "[01:05:38.440 --> 01:05:47.440]   So this is one term.\n",
      "[01:05:47.440 --> 01:05:50.440]   We essentially just need to argue the similar thing for the second term,\n",
      "[01:05:50.440 --> 01:05:54.440]   and then we're done.\n",
      "[01:05:54.440 --> 01:05:56.440]   Second term is slightly more complicated to argue,\n",
      "[01:05:56.440 --> 01:05:58.440]   but we can manage that.\n",
      "[01:05:58.440 --> 01:06:02.440]   So we look at the P term.\n",
      "[01:06:02.440 --> 01:06:08.440]   Again by lemma two, a lemma one, we do the decomposition,\n",
      "[01:06:08.440 --> 01:06:14.440]   that is equal to expectation over m hat and pi.\n",
      "[01:06:14.440 --> 01:06:22.440]   We again have the summation H from one to capital H.\n",
      "[01:06:22.440 --> 01:06:38.440]   So we have the P H sub check by P H hat, V H plus one, pi hat.\n",
      "[01:06:38.440 --> 01:06:51.440]   S E, S R A I, conditional S Y.\n",
      "[01:06:51.440 --> 01:06:57.440]   This case is definitely trickier because this value here,\n",
      "[01:06:57.440 --> 01:07:02.440]   in the first argument we just argue this value V star is fixed.\n",
      "[01:07:02.440 --> 01:07:04.440]   But here this is clearly random.\n",
      "[01:07:04.440 --> 01:07:08.440]   This depends on pi hat, where pi hat actually depends on the sample of simulator.\n",
      "[01:07:08.440 --> 01:07:11.440]   So this is random.\n",
      "[01:07:11.440 --> 01:07:18.440]   But the key observation here is that we note this value, V H plus one,\n",
      "[01:07:18.440 --> 01:07:33.440]   pi hat only depends on--\n",
      "[01:07:33.440 --> 01:07:35.440]   since this is a value at H plus one step.\n",
      "[01:07:35.440 --> 01:07:40.440]   So it only depends on the policy at a later stage, V H plus one,\n",
      "[01:07:40.440 --> 01:07:45.440]   pi hat dot dot dot, till V capital H, pi hat.\n",
      "[01:07:45.440 --> 01:07:49.440]   It actually doesn't depends on the policy in the earlier step.\n",
      "[01:07:49.440 --> 01:07:52.440]   So we didn't really care about what are the policy on the earlier step.\n",
      "[01:07:52.440 --> 01:07:56.440]   We can already fully determine what are those values.\n",
      "[01:07:56.440 --> 01:07:59.440]   And the next thing is we observe the value iteration.\n",
      "[01:07:59.440 --> 01:08:03.440]   It's a dynamic programming from back to the front.\n",
      "[01:08:03.440 --> 01:08:10.440]   So we actually calculated this pi hat H only use the data that happens like after H.\n",
      "[01:08:10.440 --> 01:08:23.440]   So that means the computation of those policy only depends on the randomness.\n",
      "[01:08:23.440 --> 01:08:27.440]   This pi hat H only depends on the randomness in this P hat H.\n",
      "[01:08:27.440 --> 01:08:30.440]   It only depends on the randomness in the very last step.\n",
      "[01:08:30.440 --> 01:08:37.440]   While the pi hat H plus one, H minus one, depends on the randomness in the last two step.\n",
      "[01:08:37.440 --> 01:08:41.440]   And da, da, da, until the very first policy--\n",
      "[01:08:41.440 --> 01:08:54.440]   this pi hat H plus one depends on the randomness in all the steps after H plus one.\n",
      "[01:08:54.440 --> 01:08:57.440]   The important thing is it only depends on data in the later steps,\n",
      "[01:08:57.440 --> 01:08:59.440]   the next stage in the later steps.\n",
      "[01:08:59.440 --> 01:09:06.440]   It doesn't depends on the next stage in the previous step.\n",
      "[01:09:06.440 --> 01:09:09.440]   So this actually creates an independence here.\n",
      "[01:09:09.440 --> 01:09:16.440]   Because remember in this value iteration, we actually for each state action and for each\n",
      "[01:09:16.440 --> 01:09:19.440]   H pair, we actually draw samples independently.\n",
      "[01:09:19.440 --> 01:09:26.440]   So that means this randomness in this P hat H is actually independent to all of the later\n",
      "[01:09:26.440 --> 01:09:49.440]   independent and the transition randomness.\n",
      "[01:09:49.440 --> 01:09:51.440]   So this is actually the very key step.\n",
      "[01:09:51.440 --> 01:09:56.440]   That observe, this value only depends on the transition in the later steps,\n",
      "[01:09:56.440 --> 01:10:01.440]   which is actually independent of the transition samples in the later step,\n",
      "[01:10:01.440 --> 01:10:07.440]   which is actually independent of the samples you observe at H step.\n",
      "[01:10:07.440 --> 01:10:10.440]   So finally, we just apply the same argument again.\n",
      "[01:10:10.440 --> 01:10:15.440]   So this is less or equal to some C, M is H squared, Yota over N.\n",
      "[01:10:15.440 --> 01:10:27.440]   And we're by choosing N large enough, we can say this is also less or equal to epsilon over 2.\n",
      "[01:10:27.440 --> 01:10:29.440]   And this essentially finished up proof.\n",
      "[01:10:29.440 --> 01:10:33.440]   Because A is less than epsilon over 2, B is less than epsilon 2,\n",
      "[01:10:33.440 --> 01:10:35.440]   and the middle term is less than 0.\n",
      "[01:10:35.440 --> 01:10:38.440]   So that everything, like the sub-optimistic is less than epsilon.\n",
      "[01:10:38.440 --> 01:10:39.440]   Less or equal to epsilon.\n",
      "[01:10:39.440 --> 01:10:47.440]   And the word down.\n",
      "[01:10:47.440 --> 01:10:48.440]   Yes?\n",
      "[01:10:48.440 --> 01:10:52.440]   [INAUDIBLE]\n",
      "[01:10:52.440 --> 01:10:54.440]   Oh, the question is, what is the factor above the N?\n",
      "[01:10:54.440 --> 01:11:00.440]   This is the Yota, the logarithmic term, just whatever the logarithmic term.\n",
      "[01:11:00.440 --> 01:11:03.440]   The logarithmic here.\n",
      "[01:11:03.440 --> 01:11:10.440]   [INAUDIBLE]\n",
      "[01:11:10.440 --> 01:11:13.440]   Oh, this is like in a theorem.\n",
      "[01:11:13.440 --> 01:11:18.440]   Remember in the theorem 1, we talk about the sample complexity, right?\n",
      "[01:11:18.440 --> 01:11:21.440]   In the theorem 1, we say we need to choose N large enough,\n",
      "[01:11:21.440 --> 01:11:23.440]   larger than some threshold.\n",
      "[01:11:23.440 --> 01:11:27.440]   Where in the theorem 1, a threshold is exactly some absolute constants\n",
      "[01:11:27.440 --> 01:11:30.440]   times H to the 4, 0, tau over epsilon squared.\n",
      "[01:11:30.440 --> 01:11:35.440]   So this is just saying by our choice of a number of samples.\n",
      "[01:11:35.440 --> 01:11:41.440]   And then we can use this H4, cancel this H squared, and Yota, cancel Yota.\n",
      "[01:11:41.440 --> 01:11:44.440]   And epsilon, we put it here.\n",
      "[01:11:44.440 --> 01:11:47.440]   And all we need to do is make this constant large enough\n",
      "[01:11:47.440 --> 01:11:51.440]   so that it's equal to something like a 4c squared.\n",
      "[01:11:51.440 --> 01:11:56.440]   [INAUDIBLE]\n",
      "[01:11:56.440 --> 01:12:23.440]   [INAUDIBLE]\n",
      "[01:12:23.440 --> 01:12:35.440]   [INAUDIBLE]\n",
      "[01:12:35.440 --> 01:12:41.440]   Yeah, I think the way we written this is a little bit inaccurate.\n",
      "[01:12:41.440 --> 01:12:52.440]   And so we just think of just consider what do we mean by this independent thing.\n",
      "[01:12:52.440 --> 01:12:57.440]   [INAUDIBLE]\n",
      "[01:12:57.440 --> 01:13:04.440]   So the samples we collected for this p hat is we have like S-A-H,\n",
      "[01:13:04.440 --> 01:13:11.440]   and we collect a bunch of next states, S-1 prime to your S-N prime.\n",
      "[01:13:11.440 --> 01:13:16.440]   And then for the next step, S-A-H plus 1,\n",
      "[01:13:16.440 --> 01:13:21.440]   we also collect a bunch of next sets.\n",
      "[01:13:21.440 --> 01:13:23.440]   [INAUDIBLE]\n",
      "[01:13:23.440 --> 01:13:26.440]   Yeah, they're not on the thing, actually, they're just completely different.\n",
      "[01:13:26.440 --> 01:13:28.440]   Complete independent.\n",
      "[01:13:28.440 --> 01:13:43.440]   [INAUDIBLE]\n",
      "[01:13:43.440 --> 01:14:02.440]   [INAUDIBLE]\n",
      "[01:14:02.440 --> 01:14:05.440]   So a vector iteration is actually a very simple algorithm.\n",
      "[01:14:05.440 --> 01:14:14.440]   So we actually study a lot throughout the course on different settings, because it's basically just bi-dynamic programming.\n",
      "[01:14:14.440 --> 01:14:20.440]   But here, in this simulator setting, I also want to introduce one additional algorithm,\n",
      "[01:14:20.440 --> 01:14:24.440]   which is also very frequently used in the field.\n",
      "[01:14:24.440 --> 01:14:26.440]   That is called a cure learning.\n",
      "[01:14:26.440 --> 01:14:30.440]   Let's first recap, how do we do the value iteration?\n",
      "[01:14:30.440 --> 01:14:37.440]   Value iteration essentially follows the Bellman-optimal equation, that we do the cure hat.\n",
      "[01:14:37.440 --> 01:14:53.440]   S-A is equal to R-H-S-A, and plus p hat, v hat.\n",
      "[01:14:53.440 --> 01:14:59.440]   Essentially, this is the key updated equation, and where we know this p hat is used,\n",
      "[01:14:59.440 --> 01:15:04.440]   is we use the empirical frequency to estimate this transition probability.\n",
      "[01:15:04.440 --> 01:15:12.440]   Where it is, she had S-prime given conditional S-A is equal to summation i from 1 to n,\n",
      "[01:15:12.440 --> 01:15:21.440]   indicator of S-I-prime equal to S-prime over n.\n",
      "[01:15:21.440 --> 01:15:38.440]   Essentially, we use n data, we collect all the data, then we update.\n",
      "[01:15:38.440 --> 01:15:48.440]   This is kind of like the batch update.\n",
      "[01:15:48.440 --> 01:15:52.440]   In practice, there is another type of algorithm where we don't do the batch update.\n",
      "[01:15:52.440 --> 01:15:54.440]   We actually do some incremental update.\n",
      "[01:15:54.440 --> 01:15:55.440]   We do online update.\n",
      "[01:15:55.440 --> 01:15:59.440]   That is, we don't like to wait till we collect all the data, and we do this update.\n",
      "[01:15:59.440 --> 01:16:03.440]   But as soon as we observe a single data, we will do some update.\n",
      "[01:16:03.440 --> 01:16:07.440]   That is essentially the cure learning algorithm.\n",
      "[01:16:07.440 --> 01:16:29.440]   We will do online updates, which we will only use the most recent data.\n",
      "[01:16:29.440 --> 01:16:58.440]   Of course, in the statistical setting,\n",
      "[01:16:58.440 --> 01:17:03.440]   when we draw some random samples, if we just use the most recent data,\n",
      "[01:17:03.440 --> 01:17:07.440]   then in absolute, there are a lot of noise in it.\n",
      "[01:17:07.440 --> 01:17:13.440]   Because data is very noisy, that means we should not just use only the recent data,\n",
      "[01:17:13.440 --> 01:17:18.440]   but we somehow should have used some previous data to average the noise out.\n",
      "[01:17:18.440 --> 01:17:31.440]   What we actually will do is some incremental update.\n",
      "[01:17:31.440 --> 01:17:36.440]   We will do some updates based on the previous value we get.\n",
      "[01:17:36.440 --> 01:17:40.440]   It turns out the updated equation is something like this.\n",
      "[01:17:40.440 --> 01:17:51.440]   QHTSA is equal to 1 minus RFA, 1 minus RFA fraction of QHT minus 1 HSA.\n",
      "[01:17:51.440 --> 01:17:56.440]   We have some previous Q value, and we do the 1 minus RFA fraction.\n",
      "[01:17:56.440 --> 01:18:00.440]   We trust the previous Q value.\n",
      "[01:18:00.440 --> 01:18:04.440]   With RFA T fraction, we trust the new data.\n",
      "[01:18:04.440 --> 01:18:22.440]   Single data is single data, we look at the reward and the value at the next step.\n",
      "[01:18:22.440 --> 01:18:33.440]   You can think this is essentially some learning rates.\n",
      "[01:18:33.440 --> 01:18:54.440]   This is the T sample from simulator.\n",
      "[01:18:54.440 --> 01:18:55.440]   I think I will stop here.\n",
      "[01:18:55.440 --> 01:19:02.440]   In the next lecture, we will continue talking about the security and benefits advantage\n",
      "[01:19:02.440 --> 01:19:05.440]   compared to the value iteration.\n",
      "[01:19:05.440 --> 01:19:10.440]   We probably won't go into that detailed proof in terms of a sample complexity of\n",
      "[01:19:10.440 --> 01:19:15.440]   Q&A, but we will give the results and talk about a qualitative, like what's the\n",
      "[01:19:15.440 --> 01:19:19.440]   difference or what's the similarity, and then we will move on to the exploration\n",
      "[01:19:19.440 --> 01:19:24.440]   scenario, the online scenario, more challenging than the simulator.\n",
      "[01:19:24.440 --> 01:19:25.440]   Thank you.\n",
      "[01:19:25.440 --> 01:19:28.020]   (upbeat music)\n",
      "[01:19:28.020 --> 01:19:30.020]   [Music]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/72QWY6ubS14.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/72QWY6ubS14.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/72QWY6ubS14.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/72QWY6ubS14.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1281.49 ms\n",
      "whisper_print_timings:     fallbacks =   5 p /   1 h\n",
      "whisper_print_timings:      mel time =  2734.56 ms\n",
      "whisper_print_timings:   sample time = 22302.87 ms / 58967 runs (    0.38 ms per run)\n",
      "whisper_print_timings:   encode time =   348.59 ms /   196 runs (    1.78 ms per run)\n",
      "whisper_print_timings:   decode time =   676.45 ms /   330 runs (    2.05 ms per run)\n",
      "whisper_print_timings:   batchd time = 30043.44 ms / 57636 runs (    0.52 ms per run)\n",
      "whisper_print_timings:   prompt time =  9937.58 ms / 44292 runs (    0.22 ms per run)\n",
      "whisper_print_timings:    total time = 67775.59 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=Szae6pHKr60 started\n",
      "Szae6pHKr60\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/Szae6pHKr60.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/Szae6pHKr60.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:19:48.88, start: 0.000000, bitrate: 244 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 145 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/Szae6pHKr60.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  149653kB time=01:19:48.88 bitrate= 256.0kbits/s speed=1.39e+03x    \n",
      "video:0kB audio:149653kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000051%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/Szae6pHKr60.wav' (76622135 samples, 4788.9 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.100]   [MUSIC]\n",
      "[00:00:03.100 --> 00:00:05.240]   >> At the very end of the last lecture,\n",
      "[00:00:05.240 --> 00:00:08.240]   we wrap up with the concentration in\n",
      "[00:00:08.240 --> 00:00:10.840]   quality of independent random variables,\n",
      "[00:00:10.840 --> 00:00:14.120]   and we start talking about the random concentration\n",
      "[00:00:14.120 --> 00:00:17.560]   of random variables with under weaker condition.\n",
      "[00:00:17.560 --> 00:00:21.480]   That is no longer independent,\n",
      "[00:00:21.480 --> 00:00:24.560]   but it satisfies so-called like a martingale difference condition.\n",
      "[00:00:24.560 --> 00:00:27.320]   So just to recap, we formalize\n",
      "[00:00:27.320 --> 00:00:31.120]   there is a martingale difference thing.\n",
      "[00:00:31.120 --> 00:00:37.520]   In formal condition, we say this is a martingale,\n",
      "[00:00:37.520 --> 00:00:41.440]   is we look at the summation of some random variables.\n",
      "[00:00:41.440 --> 00:00:46.320]   Okay, let's say we look at C_i,\n",
      "[00:00:46.320 --> 00:00:48.360]   summation from i to n.\n",
      "[00:00:48.360 --> 00:00:55.840]   And so it's martingale because each C_i is no longer ID.\n",
      "[00:00:55.840 --> 00:00:59.520]   It's what we have been working on like for the last lecture.\n",
      "[00:00:59.520 --> 00:01:02.560]   But the martingale difference no longer required,\n",
      "[00:01:02.560 --> 00:01:03.840]   they are independent.\n",
      "[00:01:03.840 --> 00:01:07.800]   It only require you to have some condition expectation of\n",
      "[00:01:07.800 --> 00:01:12.280]   C_i conditional all the thing happens in the past,\n",
      "[00:01:12.280 --> 00:01:14.800]   where we say a lot of things happen in the past,\n",
      "[00:01:14.800 --> 00:01:19.420]   including like C_i minus 1, the value of C_i, okay?\n",
      "[00:01:19.420 --> 00:01:22.960]   So you can informally understand this past as like,\n",
      "[00:01:22.960 --> 00:01:25.000]   all the randomness happen in the past,\n",
      "[00:01:25.000 --> 00:01:27.880]   and we're saying this is equal to 0.\n",
      "[00:01:27.880 --> 00:01:29.900]   In this case, we call this C_i,\n",
      "[00:01:29.900 --> 00:01:32.560]   martingale difference sequence.\n",
      "[00:01:32.560 --> 00:01:37.520]   And in the last lecture, we kind of already see the example\n",
      "[00:01:37.520 --> 00:01:43.440]   where we have this probability matrix,\n",
      "[00:01:43.440 --> 00:01:59.440]   C_i, 2, C_i, and the minus 1, the 1, minus 1, 0, 1, 0.5, 0, 0.5, 0, 1, 0.\n",
      "[00:01:59.440 --> 00:02:03.720]   And this is the conditional probability of C_i, 2,\n",
      "[00:02:03.720 --> 00:02:07.440]   conditional C_i.\n",
      "[00:02:07.440 --> 00:02:10.440]   So in the last lecture, we're already verified.\n",
      "[00:02:10.440 --> 00:02:15.480]   So in this case, first C_i, 2 is definitely\n",
      "[00:02:15.480 --> 00:02:20.320]   not independent of C_i, because the table does not factorize.\n",
      "[00:02:20.320 --> 00:02:23.120]   But on the other hand, you have a C_i, 2,\n",
      "[00:02:23.120 --> 00:02:28.960]   conditional C_i equal to 1, that is equal to 0.\n",
      "[00:02:28.960 --> 00:02:34.760]   And the expectation of a C_i, conditional C_i equal to 0,\n",
      "[00:02:34.760 --> 00:02:38.640]   this is also equal to 0, sorry, minus 1.\n",
      "[00:02:38.640 --> 00:02:39.880]   This is also equal to 0.\n",
      "[00:02:39.880 --> 00:02:49.280]   So this essentially says this C_i, 1, C_i, 2\n",
      "[00:02:49.280 --> 00:03:04.400]   are not independent, but it's a martingale difference sequence.\n",
      "[00:03:04.400 --> 00:03:25.040]   And finally, we say the martingale difference would also\n",
      "[00:03:25.040 --> 00:03:27.320]   exhibit the concentration phenomena,\n",
      "[00:03:27.320 --> 00:03:31.160]   because the summation of the martingale difference,\n",
      "[00:03:31.160 --> 00:03:33.640]   essentially the martingale would also\n",
      "[00:03:33.640 --> 00:03:36.000]   have the concentration phenomena, because if we just\n",
      "[00:03:36.000 --> 00:03:40.800]   plot the summation to C_i over time, like this is i equal to 1,\n",
      "[00:03:40.800 --> 00:03:44.360]   i equal to 2, and we sum them together.\n",
      "[00:03:44.360 --> 00:03:48.880]   Sorry, this i summation from i to n, this is n.\n",
      "[00:03:48.880 --> 00:03:51.440]   And we see the first step, we're doing some random work.\n",
      "[00:03:51.440 --> 00:03:54.400]   And the second step, we're also doing some random work.\n",
      "[00:03:54.400 --> 00:03:59.280]   And eventually, it's something like this.\n",
      "[00:03:59.280 --> 00:04:01.560]   And the only difference here is, for example,\n",
      "[00:04:01.560 --> 00:04:08.560]   if I have another realization, it's something like here.\n",
      "[00:04:08.560 --> 00:04:12.800]   So those two are two different realization of the random work.\n",
      "[00:04:12.800 --> 00:04:14.840]   And the martingale difference sequence essentially\n",
      "[00:04:14.840 --> 00:04:16.760]   says they are no longer independent.\n",
      "[00:04:16.760 --> 00:04:20.040]   So for this trajectory and this trajectory,\n",
      "[00:04:20.040 --> 00:04:23.920]   this can see C plus 1.\n",
      "[00:04:23.920 --> 00:04:25.920]   The distribution can be different.\n",
      "[00:04:25.920 --> 00:04:28.560]   For example, this distribution is like this, 1/2, 1/2.\n",
      "[00:04:28.560 --> 00:04:31.280]   And this distribution is like this with probability 1\n",
      "[00:04:31.280 --> 00:04:32.920]   equal to 0.\n",
      "[00:04:32.920 --> 00:04:36.080]   But the important thing is the conditional expectation\n",
      "[00:04:36.080 --> 00:04:38.440]   that is conditional, all things happens.\n",
      "[00:04:38.440 --> 00:04:41.920]   In the past, the single step in the future\n",
      "[00:04:41.920 --> 00:04:44.960]   has an expectation that is equal to 0.\n",
      "[00:04:44.960 --> 00:04:47.040]   So in this case, you will also expect\n",
      "[00:04:47.040 --> 00:04:48.920]   a lot of cancellation happens.\n",
      "[00:04:48.920 --> 00:04:50.520]   And eventually, the summation will\n",
      "[00:04:50.520 --> 00:04:55.240]   grow like something like square root n, OK?\n",
      "[00:04:55.240 --> 00:04:57.560]   So this is the last time where we finished.\n",
      "[00:04:57.560 --> 00:05:00.920]   We'll talk about this high-level picture about what\n",
      "[00:05:00.920 --> 00:05:04.360]   is this martingale happening.\n",
      "[00:05:04.360 --> 00:05:07.080]   I think today we'll be more formally talking about,\n",
      "[00:05:07.080 --> 00:05:09.800]   mathematically, how we're going to define everything.\n",
      "[00:05:09.800 --> 00:05:12.800]   I think to understand intuition, really you should understand\n",
      "[00:05:12.800 --> 00:05:16.240]   is the major difference is we are no longer independent.\n",
      "[00:05:16.240 --> 00:05:18.840]   What we're seeing is we require weaker condition\n",
      "[00:05:18.840 --> 00:05:20.720]   that is conditioned on the past.\n",
      "[00:05:20.720 --> 00:05:23.600]   The one step future has a conditional expectation\n",
      "[00:05:23.600 --> 00:05:25.800]   equal to 0.\n",
      "[00:05:25.800 --> 00:05:28.000]   This is a key idea in the martingale.\n",
      "[00:05:28.000 --> 00:05:33.440]   So today, we'll mathematically define--\n",
      "[00:05:33.440 --> 00:05:34.960]   this is an informal definition.\n",
      "[00:05:34.960 --> 00:05:36.480]   We never talk about what is the past.\n",
      "[00:05:36.480 --> 00:05:38.880]   So today, we'll be just a bit more rigorous\n",
      "[00:05:38.880 --> 00:05:42.160]   on how we define the martingale and what\n",
      "[00:05:42.160 --> 00:05:44.200]   is this martingale concentration inequality.\n",
      "[00:05:44.200 --> 00:05:52.680]   So this is a little bit like terminology,\n",
      "[00:05:52.680 --> 00:05:55.720]   but you can just intuitively understand\n",
      "[00:05:55.720 --> 00:05:59.840]   how we'll talk about intuitive understanding later.\n",
      "[00:05:59.840 --> 00:06:07.440]   So let's say we suppose F1 to Fn are a few traditions.\n",
      "[00:06:07.440 --> 00:06:19.760]   Two traditions typically can be understand as a set set\n",
      "[00:06:19.760 --> 00:06:22.960]   of random events.\n",
      "[00:06:22.960 --> 00:06:27.200]   You can just think of a collection of a lot of randomness.\n",
      "[00:06:27.200 --> 00:06:33.920]   So that the filtration at i minus 1 time\n",
      "[00:06:33.920 --> 00:06:37.040]   is a subset of filtration at i's time.\n",
      "[00:06:37.040 --> 00:06:40.120]   So you can think here we kind of require the randomness\n",
      "[00:06:40.120 --> 00:06:41.480]   to be strictly increased.\n",
      "[00:06:41.480 --> 00:06:58.400]   So for now, you just understand like this sigma F_i\n",
      "[00:06:58.400 --> 00:07:02.880]   is like all the randomness up to time i from time step 1\n",
      "[00:07:02.880 --> 00:07:06.240]   to time step i so that you'll have this like inclusion\n",
      "[00:07:06.240 --> 00:07:26.120]   condition for any i in M. We say this could see i sequence\n",
      "[00:07:26.120 --> 00:07:43.880]   is a martingale difference sequence\n",
      "[00:07:43.880 --> 00:08:00.200]   with respect to filtration F_i i from 1 to N.\n",
      "[00:08:00.200 --> 00:08:07.840]   If the following two conditional holds, that is 1.\n",
      "[00:08:07.840 --> 00:08:12.440]   C_i needs to be inside F_i.\n",
      "[00:08:12.440 --> 00:08:13.440]   This is some conditioner.\n",
      "[00:08:13.440 --> 00:08:15.680]   We'll explain later what is the filtration\n",
      "[00:08:15.680 --> 00:08:17.760]   and what does this mean.\n",
      "[00:08:17.760 --> 00:08:23.160]   And the second thing is the expectation of C_i,\n",
      "[00:08:23.160 --> 00:08:27.400]   conditional on the past, which we denoted as the filtration\n",
      "[00:08:27.400 --> 00:08:46.600]   F_i minus 1 is equal to 0 for all i.\n",
      "[00:08:46.600 --> 00:08:49.280]   So this is the formal mathematical definition.\n",
      "[00:08:49.280 --> 00:08:52.680]   I think everything else is like pretty straightforward.\n",
      "[00:08:52.680 --> 00:08:55.160]   The only thing is a little bit opaque\n",
      "[00:08:55.160 --> 00:09:00.320]   if you haven't learned a formal graduate version of stochastic\n",
      "[00:09:00.320 --> 00:09:03.200]   process is what is the definition of filtration.\n",
      "[00:09:03.200 --> 00:09:05.920]   I think for now, for this class, we don't really necessarily\n",
      "[00:09:05.920 --> 00:09:08.520]   like you need a lot of rigorous understanding it.\n",
      "[00:09:08.520 --> 00:09:11.400]   But I think you can just usually understand this filtration\n",
      "[00:09:11.400 --> 00:09:14.400]   as a collection of all the randomness in the past.\n",
      "[00:09:14.400 --> 00:09:27.080]   So we will say the common choice of filtration.\n",
      "[00:09:27.080 --> 00:09:29.280]   So this is not necessarily always the choice,\n",
      "[00:09:29.280 --> 00:09:30.880]   but we say common choice.\n",
      "[00:09:30.880 --> 00:09:33.760]   Or intuitively, I understand this.\n",
      "[00:09:33.760 --> 00:09:38.200]   It's the F_i will be chosen as a sigma field,\n",
      "[00:09:38.200 --> 00:09:45.040]   so class sigma field of C_1 to C_n, C_i.\n",
      "[00:09:45.040 --> 00:09:51.440]   So the mathematical definition is the class sigma field.\n",
      "[00:09:51.440 --> 00:09:53.160]   But intuitive understanding, you can just\n",
      "[00:09:53.160 --> 00:10:10.320]   say this include all the randomness in C_1 to i.\n",
      "[00:10:10.320 --> 00:10:15.560]   Just whatever random events can be induced by C_1 to C_i,\n",
      "[00:10:15.560 --> 00:10:17.920]   this is included in this pass.\n",
      "[00:10:17.920 --> 00:10:22.320]   So this is why this is like all the past.\n",
      "[00:10:22.320 --> 00:10:24.920]   So you can think this is just a mathematical definition\n",
      "[00:10:24.920 --> 00:10:28.720]   of conditional all the possible paths as the sigma field.\n",
      "[00:10:28.720 --> 00:10:39.080]   But I also want to comment, if you only choose this filtration\n",
      "[00:10:39.080 --> 00:10:41.680]   to be like a sigma field of C_1 to C_i,\n",
      "[00:10:41.680 --> 00:10:46.320]   then there is really no need to write this filtration notation.\n",
      "[00:10:46.320 --> 00:10:50.680]   And you can just basically just say a conditional all the past thing.\n",
      "[00:10:50.680 --> 00:10:54.600]   So sometimes, in the later lecture,\n",
      "[00:10:54.600 --> 00:10:57.400]   we might need to use some filtration--\n",
      "[00:10:57.400 --> 00:10:58.440]   slightly different filtration.\n",
      "[00:10:58.440 --> 00:11:02.320]   I think the comments here is like the filtration can actually\n",
      "[00:11:02.320 --> 00:11:03.480]   add more randomness.\n",
      "[00:11:03.480 --> 00:11:13.360]   Two filtration.\n",
      "[00:11:17.560 --> 00:11:22.440]   So in this answer, all we need is we satisfy those two conditions.\n",
      "[00:11:22.440 --> 00:11:24.960]   So you can actually choose some other type of filtration.\n",
      "[00:11:24.960 --> 00:11:27.720]   For example, an alternative choice\n",
      "[00:11:27.720 --> 00:11:32.920]   would be if I have not only C_c, but also some zeta random variable.\n",
      "[00:11:32.920 --> 00:11:35.000]   So I can also choose this filtration i\n",
      "[00:11:35.000 --> 00:11:39.000]   to be the sigma field of C_1, zeta_1,\n",
      "[00:11:39.000 --> 00:11:45.480]   C_2, zeta_2, and over until C_i, zeta_1.\n",
      "[00:11:45.480 --> 00:11:54.120]   So in that case, I still satisfy, for example, the F_i,\n",
      "[00:11:54.120 --> 00:11:57.680]   the randomness strictly increased for this filtration.\n",
      "[00:11:57.680 --> 00:12:03.960]   And also, the random variable C_i is in this filtration\n",
      "[00:12:03.960 --> 00:12:04.800]   F_i.\n",
      "[00:12:04.800 --> 00:12:07.560]   So this is also another valid choice.\n",
      "[00:12:07.560 --> 00:12:10.440]   So sometimes we can include some more randomness\n",
      "[00:12:10.440 --> 00:12:11.440]   into this filtration.\n",
      "[00:12:11.440 --> 00:12:15.960]   And if we can prove the martingale condition hose,\n",
      "[00:12:15.960 --> 00:12:19.360]   random different condition hose for this different filtration.\n",
      "[00:12:19.360 --> 00:12:22.000]   And then we also say this is a martingale difference.\n",
      "[00:12:22.000 --> 00:12:24.240]   The only difference is the filtration definition\n",
      "[00:12:24.240 --> 00:12:25.960]   is slightly different.\n",
      "[00:12:25.960 --> 00:12:28.040]   Essentially, intuitive understanding\n",
      "[00:12:28.040 --> 00:12:29.960]   is like, what are you including in the past?\n",
      "[00:12:29.960 --> 00:12:32.400]   It's not only just C_1 to C_i minus 1.\n",
      "[00:12:32.400 --> 00:12:34.600]   Sometimes you can also include some other random variable,\n",
      "[00:12:34.600 --> 00:12:36.160]   other random events into the past.\n",
      "[00:12:36.160 --> 00:12:50.160]   OK, so finally, we already defined\n",
      "[00:12:50.160 --> 00:12:53.840]   what is so-called martingale difference sequence.\n",
      "[00:12:53.840 --> 00:12:59.160]   And then we can define what is so-called martingale sequence.\n",
      "[00:13:03.320 --> 00:13:10.040]   So s_i, i from 1 to n, is a martingale sequence,\n",
      "[00:13:10.040 --> 00:13:16.320]   or a line time, you just call it martingale,\n",
      "[00:13:16.320 --> 00:13:27.520]   if and only if this s_i is equal to some summation of C_j,\n",
      "[00:13:27.520 --> 00:13:40.360]   from 1 to i, and this C_j is a martingale difference\n",
      "[00:13:40.360 --> 00:13:59.600]   sequence. OK, so essentially, what we're saying\n",
      "[00:13:59.600 --> 00:14:03.640]   is summation of the martingale difference is the martingale.\n",
      "[00:14:03.640 --> 00:14:07.400]   So we call it some to be the martingale and the individual\n",
      "[00:14:07.400 --> 00:14:09.400]   summation of the called a martingale difference sequence.\n",
      "[00:14:09.400 --> 00:14:22.640]   So this is like a mathematical formalism,\n",
      "[00:14:22.640 --> 00:14:25.440]   but the intuitive understanding is really this informal.\n",
      "[00:14:25.440 --> 00:14:26.880]   Just fusion is just the past.\n",
      "[00:14:26.880 --> 00:14:29.360]   And all we have is just condition on the past.\n",
      "[00:14:29.360 --> 00:14:30.920]   The condition of expectation is 0.\n",
      "[00:14:30.920 --> 00:14:44.560]   So given the definition now, we're\n",
      "[00:14:44.560 --> 00:14:48.040]   ready to state the concentration inequality\n",
      "[00:14:48.040 --> 00:14:50.040]   for martingale difference sequence.\n",
      "[00:14:50.040 --> 00:14:57.280]   It turns out, what are we going to stay here\n",
      "[00:14:57.280 --> 00:14:58.120]   for martingale difference?\n",
      "[00:14:58.120 --> 00:15:01.520]   It's basically identical to a half-ding and sine-berenstant\n",
      "[00:15:01.520 --> 00:15:02.440]   inequality.\n",
      "[00:15:02.440 --> 00:15:04.000]   The only difference is precondition.\n",
      "[00:15:04.000 --> 00:15:08.520]   We kind of relax the precondition from requiring\n",
      "[00:15:08.520 --> 00:15:11.880]   all the random variable to be independent to now we only\n",
      "[00:15:11.880 --> 00:15:14.480]   require all the random variable to satisfy the martingale\n",
      "[00:15:14.480 --> 00:15:16.960]   difference condition.\n",
      "[00:15:16.960 --> 00:15:20.040]   So the theorem now is called azuma half-ding.\n",
      "[00:15:20.840 --> 00:15:23.340]   [APPLAUSE]\n",
      "[00:15:23.340 --> 00:15:36.540]   My intuitive understanding is like a half-ding is originally\n",
      "[00:15:36.540 --> 00:15:38.400]   for independent random variable.\n",
      "[00:15:38.400 --> 00:15:40.960]   And you can have azuma half-ding, which is basically\n",
      "[00:15:40.960 --> 00:15:43.880]   strengthening the tool like a martingale difference.\n",
      "[00:15:43.880 --> 00:15:46.260]   And later, we also have the azuma berenstant, essentially,\n",
      "[00:15:46.260 --> 00:15:48.560]   the berenstant version of martingale difference.\n",
      "[00:15:49.280 --> 00:15:52.760]   All right.\n",
      "[00:15:52.760 --> 00:15:56.500]   So the theorem is very simple.\n",
      "[00:15:56.500 --> 00:15:59.440]   It's pretty similar to the half-dinging inequality.\n",
      "[00:15:59.440 --> 00:16:09.320]   So we just suppose this cos ci, i from 1 to n,\n",
      "[00:16:09.320 --> 00:16:11.000]   is a martingale difference sequence.\n",
      "[00:16:11.000 --> 00:16:14.000]   [APPLAUSE]\n",
      "[00:16:14.000 --> 00:16:31.080]   With respect to filtration, I find.\n",
      "[00:16:31.800 --> 00:16:34.800]   [APPLAUSE]\n",
      "[00:16:34.800 --> 00:16:47.480]   And remember, for the half-dinging inequality,\n",
      "[00:16:47.480 --> 00:16:48.880]   the most important precondition is\n",
      "[00:16:48.880 --> 00:16:51.400]   to like the random variable need to be bounded.\n",
      "[00:16:51.400 --> 00:16:54.120]   So here, we also essentially assume the same thing.\n",
      "[00:16:54.120 --> 00:17:03.960]   We say for n, for all i in from 1 to n,\n",
      "[00:17:03.960 --> 00:17:22.680]   we have a cos ci is supported on an interval\n",
      "[00:17:22.680 --> 00:17:32.720]   with length ri, with the probability of 1.\n",
      "[00:17:32.720 --> 00:17:41.280]   For most cases, we just understand this cos ci is bounded.\n",
      "[00:17:41.280 --> 00:17:42.600]   This is fine.\n",
      "[00:17:42.600 --> 00:17:52.360]   And then we have essentially the same concentration inequality\n",
      "[00:17:52.360 --> 00:17:58.840]   that the probability of this summation, i from 1 to n,\n",
      "[00:17:58.840 --> 00:18:09.960]   cos ci, greater or equal to t, is less or equal to minus t\n",
      "[00:18:09.960 --> 00:18:15.880]   square e to the minus t square over 2 times summation 1\n",
      "[00:18:15.880 --> 00:18:20.760]   from summation i from 1 to n ri square.\n",
      "[00:18:20.760 --> 00:18:39.080]   So that's essentially the same thing.\n",
      "[00:18:39.080 --> 00:18:43.640]   We still have the bounded random variable,\n",
      "[00:18:43.640 --> 00:18:45.720]   still like our main precondition.\n",
      "[00:18:45.720 --> 00:18:48.880]   And the concentration is like the probability\n",
      "[00:18:48.880 --> 00:18:52.400]   of some tail probability that is the summation.\n",
      "[00:18:52.400 --> 00:18:54.920]   And here, we no longer subtract the expectation\n",
      "[00:18:54.920 --> 00:18:58.080]   because by assuming this marginal difference sequence,\n",
      "[00:18:58.080 --> 00:19:01.520]   we essentially already assume this expectation is equal to 0.\n",
      "[00:19:01.520 --> 00:19:06.600]   So you should think this cos c is something roughly\n",
      "[00:19:06.600 --> 00:19:10.680]   like something xi minus e xi or something like that.\n",
      "[00:19:10.680 --> 00:19:13.120]   So we already subtract the mean.\n",
      "[00:19:13.120 --> 00:19:15.280]   So that's why it's corresponding to the half-dinging\n",
      "[00:19:15.280 --> 00:19:17.440]   quality, and the remaining thing is basically the same.\n",
      "[00:19:17.440 --> 00:19:21.600]   We're seeing that tail probability is smaller than something\n",
      "[00:19:21.600 --> 00:19:26.120]   that exponentially smaller with spectral t.\n",
      "[00:19:26.120 --> 00:19:28.720]   And you can do essentially the same operation.\n",
      "[00:19:28.720 --> 00:19:33.400]   We say we make this delta, and we can solve the upper bounds\n",
      "[00:19:33.400 --> 00:19:39.320]   for this summation, cos ci, the martingale,\n",
      "[00:19:39.320 --> 00:19:41.360]   and we will not repeat that kind of part.\n",
      "[00:19:41.360 --> 00:20:03.840]   [NOISE]\n",
      "[00:20:03.840 --> 00:20:09.880]   So any question about theorem statements? Yes.\n",
      "[00:20:09.880 --> 00:20:19.040]   [inaudible]\n",
      "[00:20:19.040 --> 00:20:25.680]   Is it? No. Oh, I think probably I made a mistake.\n",
      "[00:20:25.680 --> 00:20:27.120]   I think it should be exactly the same.\n",
      "[00:20:27.120 --> 00:20:29.360]   So a half-dinging has a tool on the top, right?\n",
      "[00:20:29.360 --> 00:20:34.400]   Sorry. Just want to make sure a half-dinging\n",
      "[00:20:34.400 --> 00:20:38.280]   has a tool on the top. Yeah. Yeah, let's do it.\n",
      "[00:20:38.280 --> 00:20:44.280]   [NOISE]\n",
      "[00:20:44.280 --> 00:20:46.740]   Thanks for pointing out the typo.\n",
      "[00:20:46.740 --> 00:20:54.160]   [NOISE]\n",
      "[00:20:54.160 --> 00:21:02.320]   Okay. So if no question about the theorem,\n",
      "[00:21:02.320 --> 00:21:06.000]   we will just briefly talk about how we're going to prove this.\n",
      "[00:21:06.000 --> 00:21:09.080]   The most of the proof is like going to follow\n",
      "[00:21:09.080 --> 00:21:14.120]   the same arguments for the half-dinging quality.\n",
      "[00:21:14.120 --> 00:21:16.680]   So in order to do this proof,\n",
      "[00:21:16.680 --> 00:21:19.400]   let's just, we need to essentially record how we\n",
      "[00:21:19.400 --> 00:21:22.840]   prove the original half-dinging quality for independent random variable.\n",
      "[00:21:22.840 --> 00:21:26.120]   We have three lemma and the final theorem is just proved\n",
      "[00:21:26.120 --> 00:21:29.400]   by a combination of simple combination of three lemma.\n",
      "[00:21:29.400 --> 00:21:32.080]   So the lemma 1 is Markovian quality,\n",
      "[00:21:32.080 --> 00:21:37.520]   which, of course, we don't need to modify it all.\n",
      "[00:21:37.520 --> 00:21:40.280]   So at that time, lemma 2,\n",
      "[00:21:40.280 --> 00:21:46.840]   so just this is like a recall the proof of independent proof\n",
      "[00:21:46.840 --> 00:21:52.680]   for the original half-dinging quality.\n",
      "[00:21:52.680 --> 00:22:01.480]   [NOISE]\n",
      "[00:22:01.480 --> 00:22:06.800]   The second lemma, we're seeing the summation of independent Gaussian.\n",
      "[00:22:06.800 --> 00:22:18.440]   [NOISE]\n",
      "[00:22:18.440 --> 00:22:25.640]   It has good concentration.\n",
      "[00:22:25.640 --> 00:22:29.080]   [NOISE]\n",
      "[00:22:29.080 --> 00:22:35.640]   And the third step, we're seeing bounded random variables,\n",
      "[00:22:35.640 --> 00:22:39.200]   [NOISE]\n",
      "[00:22:39.200 --> 00:22:42.360]   is bounded random variable, is, is sub Gaussian.\n",
      "[00:22:42.360 --> 00:22:47.960]   [NOISE]\n",
      "[00:22:47.960 --> 00:22:53.320]   So those are three lemmas we use to prove the original half-dinging\n",
      "[00:22:53.320 --> 00:22:54.200]   quality.\n",
      "[00:22:54.200 --> 00:22:56.960]   And we can check first the Markovian quality, of course,\n",
      "[00:22:56.960 --> 00:22:59.880]   like we don't need to change anything, so this is like down.\n",
      "[00:22:59.880 --> 00:23:02.440]   We don't need to do any extra work.\n",
      "[00:23:02.440 --> 00:23:05.200]   And then this bounded random variable is sub Gaussian.\n",
      "[00:23:05.200 --> 00:23:10.280]   This is regarding a single random variable.\n",
      "[00:23:10.280 --> 00:23:20.320]   [NOISE]\n",
      "[00:23:20.320 --> 00:23:24.320]   So this lemma actually has nothing to do with whether it's independent\n",
      "[00:23:24.320 --> 00:23:26.440]   or whether it's like a martingale difference.\n",
      "[00:23:26.440 --> 00:23:29.040]   All you see is for a single bounded random variable,\n",
      "[00:23:29.040 --> 00:23:30.280]   it is sub Gaussian.\n",
      "[00:23:30.280 --> 00:23:33.920]   So this one, we actually also does not need any modification.\n",
      "[00:23:33.920 --> 00:23:36.080]   We just directly use the same lemma.\n",
      "[00:23:36.080 --> 00:23:39.400]   So the only thing that's going to related to whether it's independent\n",
      "[00:23:39.400 --> 00:23:42.680]   or it's a martingale difference is the second lemma,\n",
      "[00:23:42.680 --> 00:23:46.200]   where we see the summation of independent sub Gaussian\n",
      "[00:23:46.200 --> 00:23:47.480]   has good concentration.\n",
      "[00:23:47.480 --> 00:23:51.280]   And we essentially need to modify the proof for the second lemma.\n",
      "[00:23:51.280 --> 00:23:55.440]   We need to strengthen the second lemma to move from independent\n",
      "[00:23:55.440 --> 00:23:57.000]   to like the martingale difference.\n",
      "[00:23:57.000 --> 00:24:03.360]   [NOISE]\n",
      "[00:24:03.360 --> 00:24:06.440]   So this is like what we will do here.\n",
      "[00:24:06.440 --> 00:24:27.800]   [NOISE]\n",
      "[00:24:27.800 --> 00:24:33.120]   So the new lemma, let's call them a two star.\n",
      "[00:24:33.120 --> 00:24:34.560]   It's a new lemma we're going to prove.\n",
      "[00:24:34.560 --> 00:24:43.600]   We say the last C, i, i from 1 to n,\n",
      "[00:24:43.600 --> 00:25:04.160]   be a martingale difference sequence,\n",
      "[00:25:04.160 --> 00:25:16.680]   where for all lambda in R, we have the following precondition\n",
      "[00:25:16.680 --> 00:25:22.640]   that is the expectation of the e to the power of lambda\n",
      "[00:25:22.640 --> 00:25:24.400]   c_i.\n",
      "[00:25:24.400 --> 00:25:27.480]   So in a previous definition, it's just this like a moment generating\n",
      "[00:25:27.480 --> 00:25:28.000]   function.\n",
      "[00:25:28.000 --> 00:25:30.360]   We say it's bounded by some exponential.\n",
      "[00:25:30.360 --> 00:25:32.800]   And now the only difference is we essentially\n",
      "[00:25:32.800 --> 00:25:34.880]   need to say this expectation is, again,\n",
      "[00:25:34.880 --> 00:25:36.560]   the conditional expectation.\n",
      "[00:25:36.560 --> 00:25:39.600]   We conditional whatever happens in the past.\n",
      "[00:25:39.600 --> 00:25:42.040]   That is the filtration of i minus 1.\n",
      "[00:25:42.040 --> 00:25:47.040]   We say this expectation is upper bounded by e\n",
      "[00:25:47.040 --> 00:25:51.880]   to the lambda square, sigma i square, over 2.\n",
      "[00:25:58.000 --> 00:26:08.200]   Again, for all t greater than 0, greater equal to 0,\n",
      "[00:26:08.200 --> 00:26:13.400]   we have that to the probability of summation i\n",
      "[00:26:13.400 --> 00:26:22.000]   from 1 to n, cos c_i, greater equal to t,\n",
      "[00:26:22.000 --> 00:26:28.240]   is less or equal to e to the minus t square, over 2 summation\n",
      "[00:26:28.240 --> 00:26:32.320]   i from 1 to n, sigma i square.\n",
      "[00:26:32.320 --> 00:26:45.760]   So everything is the same.\n",
      "[00:26:45.760 --> 00:26:47.520]   So last time, the only difference is now\n",
      "[00:26:47.520 --> 00:26:50.880]   we kind of adding the condition, like change this\n",
      "[00:26:50.880 --> 00:26:52.720]   original, like, unconditional expectation\n",
      "[00:26:52.720 --> 00:26:54.600]   to now the conditional expectation.\n",
      "[00:26:54.600 --> 00:26:56.280]   Because no longer independent, it's\n",
      "[00:26:56.280 --> 00:26:57.680]   a multiple difference sequence.\n",
      "[00:26:57.680 --> 00:26:59.680]   [INAUDIBLE]\n",
      "[00:26:59.680 --> 00:27:25.000]   [INAUDIBLE]\n",
      "[00:27:25.000 --> 00:27:25.500]   OK.\n",
      "[00:27:25.500 --> 00:27:32.580]   [INAUDIBLE]\n",
      "[00:27:32.580 --> 00:27:34.940]   So let's prove this lemma_2.\n",
      "[00:27:34.940 --> 00:27:44.460]   So we will essentially follow a lot of steps.\n",
      "[00:27:44.460 --> 00:27:47.180]   Like in the last lecture, how we prove the lemma_2,\n",
      "[00:27:47.180 --> 00:27:51.940]   the first step is we first write out\n",
      "[00:27:51.940 --> 00:27:57.380]   what we want to prove as probability of summation, cos c_i,\n",
      "[00:27:57.380 --> 00:27:59.540]   greater or equal to t.\n",
      "[00:27:59.540 --> 00:28:03.900]   The first step, we raise everything to the exponent.\n",
      "[00:28:03.900 --> 00:28:06.620]   So we say this is equal to the probability\n",
      "[00:28:06.620 --> 00:28:13.780]   of e to the lambda summation i from 1 to n, cos c_i,\n",
      "[00:28:13.780 --> 00:28:16.940]   greater or equal to e to the lambda t.\n",
      "[00:28:16.940 --> 00:28:18.820]   This is the fact-- again, use the fact\n",
      "[00:28:18.820 --> 00:28:21.820]   that lambda is greater than 0, greater or equal to 0.\n",
      "[00:28:21.820 --> 00:28:24.940]   So basically, e to the lambda t is like monotonic function\n",
      "[00:28:24.940 --> 00:28:27.220]   with respect to t.\n",
      "[00:28:27.220 --> 00:28:29.980]   And again, we will use Markovian quality.\n",
      "[00:28:29.980 --> 00:28:35.060]   So we say this is less or equal to e to the minus lambda t\n",
      "[00:28:35.060 --> 00:28:41.860]   times the expectation of e to the lambda summation i from 1\n",
      "[00:28:41.860 --> 00:28:43.380]   to n, cos c_i.\n",
      "[00:28:43.380 --> 00:28:45.500]   This is using the Markovian quality.\n",
      "[00:28:45.500 --> 00:29:08.460]   So let's first recall what do we\n",
      "[00:29:08.460 --> 00:29:10.940]   use in the independent case?\n",
      "[00:29:10.940 --> 00:29:20.100]   Recall in the independent case-- so in the independent case,\n",
      "[00:29:20.100 --> 00:29:24.380]   we use a very important equality.\n",
      "[00:29:24.380 --> 00:29:27.060]   That is, for anything independent,\n",
      "[00:29:27.060 --> 00:29:31.100]   the expectation of the product of independent random variable\n",
      "[00:29:31.100 --> 00:29:34.900]   is equal to the expectation of the product of expectation\n",
      "[00:29:34.900 --> 00:29:38.220]   of each independent random variable.\n",
      "[00:29:38.220 --> 00:29:43.220]   So we have e of xy equal to e x times e y.\n",
      "[00:29:43.220 --> 00:29:45.020]   So in this case, we can essentially\n",
      "[00:29:45.020 --> 00:29:55.380]   make the second part to be the product of i from 1 to n\n",
      "[00:29:55.380 --> 00:29:58.740]   and the expectation of e to the lambda cos c_i.\n",
      "[00:29:58.740 --> 00:30:05.740]   Essentially, we pull the summation to be the product\n",
      "[00:30:05.740 --> 00:30:08.980]   outside the expectation and then we use the property\n",
      "[00:30:08.980 --> 00:30:10.060]   of independent random rule.\n",
      "[00:30:10.060 --> 00:30:13.620]   We can also pull the product outside the expectation.\n",
      "[00:30:13.620 --> 00:30:16.020]   And then for each expectation, we\n",
      "[00:30:16.020 --> 00:30:18.940]   can just directly use a sub-gouchen condition\n",
      "[00:30:18.940 --> 00:30:22.340]   and this finish the proof for independent case.\n",
      "[00:30:22.340 --> 00:30:24.380]   But for now, we can no longer do that.\n",
      "[00:30:24.380 --> 00:30:26.900]   So we realize for Markovian difference,\n",
      "[00:30:26.900 --> 00:30:30.700]   the key is essentially we can bound this term now\n",
      "[00:30:30.700 --> 00:30:33.740]   with the relaxed condition, with a weaker condition,\n",
      "[00:30:33.740 --> 00:30:34.900]   with a weaker condition.\n",
      "[00:30:34.900 --> 00:30:45.980]   So how are we going to do that?\n",
      "[00:30:45.980 --> 00:30:52.740]   Actually, it's also not that difficult.\n",
      "[00:30:52.740 --> 00:30:54.580]   So what we can do is we again look\n",
      "[00:30:54.580 --> 00:30:57.700]   at the important quantity, which we need to bound here.\n",
      "[00:30:57.700 --> 00:31:01.220]   That is the expectation of e to the lambda summation i\n",
      "[00:31:01.220 --> 00:31:02.820]   from 1 to n, cos c_i.\n",
      "[00:31:02.820 --> 00:31:09.900]   So what we will do here is we will do the expectation\n",
      "[00:31:09.900 --> 00:31:10.660]   in two steps.\n",
      "[00:31:10.660 --> 00:31:19.260]   So we will first take an expectation over only\n",
      "[00:31:19.260 --> 00:31:20.860]   the last randomness.\n",
      "[00:31:28.180 --> 00:31:33.180]   So essentially we will basically condition on f_i n minus 1,\n",
      "[00:31:33.180 --> 00:31:37.260]   condition on the history up to n minus 1 time.\n",
      "[00:31:37.260 --> 00:31:39.460]   And then we essentially only take expectation\n",
      "[00:31:39.460 --> 00:31:41.900]   over the very last randomness.\n",
      "[00:31:41.900 --> 00:31:45.580]   And then we take an expectation over the history.\n",
      "[00:31:45.580 --> 00:31:52.980]   So essentially, you should understand\n",
      "[00:31:52.980 --> 00:31:55.140]   like intuitively this understanding\n",
      "[00:31:55.140 --> 00:31:58.820]   is like this expectation is taking over c_n.\n",
      "[00:31:58.820 --> 00:32:02.020]   And this expectation is taking over c_1 to n minus 1.\n",
      "[00:32:02.020 --> 00:32:04.580]   This is what this equation really means.\n",
      "[00:32:04.580 --> 00:32:19.980]   So the good thing is if we look at the expectation inside,\n",
      "[00:32:19.980 --> 00:32:22.100]   that is we condition on the past.\n",
      "[00:32:22.100 --> 00:32:24.060]   When we condition on the past, a lot of things\n",
      "[00:32:24.060 --> 00:32:25.980]   on the exponent become deterministic\n",
      "[00:32:25.980 --> 00:32:28.020]   because we kind of condition on the past.\n",
      "[00:32:28.020 --> 00:32:31.860]   So we can move that outside the expectation.\n",
      "[00:32:31.860 --> 00:32:38.500]   So what we are saying is this is the expectation outside.\n",
      "[00:32:38.500 --> 00:32:46.380]   And what we can do is we move this e to the lambda summation\n",
      "[00:32:46.380 --> 00:32:51.420]   i from 1 to n minus 1, cos c_i.\n",
      "[00:32:51.420 --> 00:32:53.620]   Outside this conditional expectation.\n",
      "[00:32:53.620 --> 00:32:56.940]   And inside expectation, we only have expectation\n",
      "[00:32:56.940 --> 00:33:02.340]   of e to the lambda cos c_n of condition\n",
      "[00:33:02.340 --> 00:33:06.500]   expectation of f_i minus 1.\n",
      "[00:33:06.500 --> 00:33:09.500]   The reason is because we call the inside expectation,\n",
      "[00:33:09.500 --> 00:33:11.500]   we only take expectation of cos c_n.\n",
      "[00:33:11.500 --> 00:33:13.420]   So only cos c_n is random.\n",
      "[00:33:13.420 --> 00:33:17.300]   Everything else is essentially fixed given a history.\n",
      "[00:33:17.300 --> 00:33:19.380]   So that's why we can move everything else\n",
      "[00:33:19.380 --> 00:33:21.380]   fixed outside the expectation.\n",
      "[00:33:21.380 --> 00:33:25.740]   So inside, we only take an expectation over cos c_i.\n",
      "[00:33:25.740 --> 00:33:26.240]   Cos c_n.\n",
      "[00:33:26.240 --> 00:33:36.660]   So you already realize why we do this?\n",
      "[00:33:36.660 --> 00:33:39.020]   Because now we can use this precondition.\n",
      "[00:33:39.020 --> 00:33:43.020]   For the very last step, when we place this i equal to n--\n",
      "[00:33:43.020 --> 00:33:44.700]   so we say this is less or equal to--\n",
      "[00:33:48.220 --> 00:33:58.740]   expectation of e to the lambda summation\n",
      "[00:33:58.740 --> 00:34:02.340]   i from 1 to n minus 1, cos c_i.\n",
      "[00:34:07.780 --> 00:34:18.760]   And times e to the lambda square sigma n square over 2.\n",
      "[00:34:18.760 --> 00:34:20.380]   This essentially just uses a precondition.\n",
      "[00:34:20.380 --> 00:34:35.500]   We call like martingale sub-gautian or something.\n",
      "[00:34:35.500 --> 00:34:38.500]   Like it's a bit informal.\n",
      "[00:35:04.500 --> 00:35:07.980]   So by now, you will see how it goes.\n",
      "[00:35:07.980 --> 00:35:11.220]   Because we start with this moment generating function\n",
      "[00:35:11.220 --> 00:35:14.340]   that is summation from i to n.\n",
      "[00:35:14.340 --> 00:35:17.300]   Now we end up with this moment generating function\n",
      "[00:35:17.300 --> 00:35:19.660]   by summation from i to n minus 1.\n",
      "[00:35:19.660 --> 00:35:21.060]   The only interesting thing we get\n",
      "[00:35:21.060 --> 00:35:23.220]   is we time some scalar.\n",
      "[00:35:23.220 --> 00:35:23.820]   This is scalar.\n",
      "[00:35:23.820 --> 00:35:24.900]   This is not even random.\n",
      "[00:35:24.900 --> 00:35:26.500]   This is just some scalar.\n",
      "[00:35:26.500 --> 00:35:30.340]   So we can essentially repeat the whole process again and again.\n",
      "[00:35:30.340 --> 00:35:35.540]   So we say this is also less or equal to the expectation of--\n",
      "[00:35:35.540 --> 00:35:39.700]   for example, we can move n minus 1 outside.\n",
      "[00:35:39.700 --> 00:35:44.220]   So what we will end up with is summation from i to n minus 2,\n",
      "[00:35:44.220 --> 00:35:45.540]   cos c_i.\n",
      "[00:35:45.540 --> 00:35:51.500]   And times e to the lambda square sigma n minus 1 square over 2.\n",
      "[00:35:51.500 --> 00:35:56.340]   And times e to the lambda square sigma n square over 2.\n",
      "[00:35:56.340 --> 00:35:59.780]   So eventually, we can repeat the entire process again and again.\n",
      "[00:35:59.780 --> 00:36:02.660]   And we end up with a single scalar that\n",
      "[00:36:02.660 --> 00:36:09.300]   is e to the minus e to the lambda square.\n",
      "[00:36:09.300 --> 00:36:16.940]   And summation i from 1 to n sigma i square over 2.\n",
      "[00:36:16.940 --> 00:36:46.160]   [VIDEO PLAYBACK]\n",
      "[00:36:46.160 --> 00:36:48.760]   So the final step is basically the same.\n",
      "[00:36:48.760 --> 00:36:52.280]   We can just say-- we start from the very beginning\n",
      "[00:36:52.280 --> 00:36:56.780]   where we say probability of this summation greater or equal to t\n",
      "[00:36:56.780 --> 00:37:01.520]   is now less or equal to e to the minus lambda t.\n",
      "[00:37:01.520 --> 00:37:05.240]   And we plug in the results we have for this expectation.\n",
      "[00:37:05.240 --> 00:37:10.280]   That is e to the lambda square summation i from 1 to n sigma i\n",
      "[00:37:10.280 --> 00:37:12.680]   square over 2.\n",
      "[00:37:12.680 --> 00:37:16.080]   And we again, so for optimal lambda,\n",
      "[00:37:16.080 --> 00:37:18.400]   and to minimize the right-hand side-- and again,\n",
      "[00:37:18.400 --> 00:37:23.120]   this lambda is some quadratic formula on the exponent.\n",
      "[00:37:23.120 --> 00:37:29.520]   So what we can do is we just pick the optimal lambda\n",
      "[00:37:29.520 --> 00:37:33.760]   equal to t over summation sigma i square.\n",
      "[00:37:33.760 --> 00:37:35.680]   This is exactly the same as the last time\n",
      "[00:37:35.680 --> 00:37:37.840]   when we do the independence of Gaussian.\n",
      "[00:37:37.840 --> 00:37:43.320]   And this becomes less or equal to e to the minus t\n",
      "[00:37:43.320 --> 00:38:09.400]   square over 2 summation i from 1 to n sigma i square.\n",
      "[00:38:09.400 --> 00:38:32.680]   [VIDEO PLAYBACK]\n",
      "[00:38:32.680 --> 00:38:34.120]   So the two end up in the bottom, yes.\n",
      "[00:38:34.120 --> 00:38:36.400]   And this is like in the lambda.\n",
      "[00:38:36.400 --> 00:38:40.000]   So in this like lambda, about a sub-Gaussian\n",
      "[00:38:40.000 --> 00:38:41.400]   2 is in the bottom.\n",
      "[00:38:41.400 --> 00:38:43.600]   But in the original halving bound 2 is on the top,\n",
      "[00:38:43.600 --> 00:38:45.280]   because we still need the third step.\n",
      "[00:38:45.280 --> 00:38:47.400]   That boundary random variable is sub-Gaussian.\n",
      "[00:38:47.400 --> 00:38:51.160]   So eventually, we need to choose like a sigma equal to r over 2\n",
      "[00:38:51.160 --> 00:38:52.560]   or something like that.\n",
      "[00:38:52.560 --> 00:38:54.360]   Wait, so when we're doing lambda 3,\n",
      "[00:38:54.360 --> 00:38:56.720]   we have to get out that conditional expectation instead\n",
      "[00:38:56.720 --> 00:38:58.720]   of just like the normal non-disruptions.\n",
      "[00:38:58.720 --> 00:39:00.200]   That changes lambda 3 at all.\n",
      "[00:39:00.200 --> 00:39:02.160]   Yeah, so I think this is also a good question\n",
      "[00:39:02.160 --> 00:39:02.960]   if you're not convinced.\n",
      "[00:39:02.960 --> 00:39:05.800]   It's a good exercise to think about at home.\n",
      "[00:39:05.800 --> 00:39:11.800]   But basically, the main idea is in this like a azuma\n",
      "[00:39:11.800 --> 00:39:14.520]   halving inquiry, where essentially we're assuming\n",
      "[00:39:14.520 --> 00:39:19.800]   that no matter what is the past, the CI is always bounded.\n",
      "[00:39:19.800 --> 00:39:22.080]   So that bounded random variable is also\n",
      "[00:39:22.080 --> 00:39:25.240]   modified to be basically conditional on whatever\n",
      "[00:39:25.240 --> 00:39:27.120]   happening in the past is bounded.\n",
      "[00:39:27.120 --> 00:39:31.120]   And then it's just a single random variable thing.\n",
      "[00:39:31.120 --> 00:39:33.080]   So you can just repeat the same argument\n",
      "[00:39:33.080 --> 00:39:35.280]   and you prove that the same thing is true.\n",
      "[00:39:35.280 --> 00:39:56.520]   OK, so basically, everything is the same,\n",
      "[00:39:56.520 --> 00:39:59.800]   except now we make the conditional weaker,\n",
      "[00:39:59.800 --> 00:40:02.640]   which we will actually leverage this weaker condition\n",
      "[00:40:02.640 --> 00:40:05.400]   in the later when we do the reinforcement learning.\n",
      "[00:40:05.400 --> 00:40:07.400]   Because in the reinforcement setting,\n",
      "[00:40:07.400 --> 00:40:10.440]   we don't have the luxury to make everything independent.\n",
      "[00:40:10.440 --> 00:40:12.680]   We actually will encounter a lot of cases\n",
      "[00:40:12.680 --> 00:40:13.920]   that is like a margin goal.\n",
      "[00:40:13.920 --> 00:40:22.400]   And finally, after we introduce the azuma halving,\n",
      "[00:40:22.400 --> 00:40:26.480]   I will just quickly give the result for azuma brainstem.\n",
      "[00:40:26.480 --> 00:40:28.480]   I will not no longer do the discussion,\n",
      "[00:40:28.480 --> 00:40:30.400]   because everything is the same as a brainstem.\n",
      "[00:40:30.400 --> 00:40:32.600]   The only thing different is like the preconditions\n",
      "[00:40:32.600 --> 00:40:34.880]   slightly weaker.\n",
      "[00:40:34.880 --> 00:40:47.240]   So the azuma brainstem inequality,\n",
      "[00:40:47.240 --> 00:40:52.240]   we again suppose we have the marginal difference sequence.\n",
      "[00:40:52.240 --> 00:41:15.080]   [INAUDIBLE]\n",
      "[00:41:15.080 --> 00:41:26.200]   As we back to the filtration, that is f i, i from 1 to n.\n",
      "[00:41:26.200 --> 00:41:32.560]   And we have the following two condition.\n",
      "[00:41:32.560 --> 00:41:39.440]   Let's just for simplicity say this ci\n",
      "[00:41:39.440 --> 00:41:43.200]   is always bounded by r with probability 1.\n",
      "[00:41:43.200 --> 00:41:53.760]   [INAUDIBLE]\n",
      "[00:41:53.760 --> 00:41:57.320]   And the variance, I think the important part about the--\n",
      "[00:41:57.320 --> 00:42:00.640]   for instance, in quality is the variance of ci.\n",
      "[00:42:00.640 --> 00:42:05.960]   Conditional filtration i minus 1 is bounded by sigma i square.\n",
      "[00:42:05.960 --> 00:42:17.360]   [INAUDIBLE]\n",
      "[00:42:17.360 --> 00:42:28.040]   Then we will have that again, the probability of summation i\n",
      "[00:42:28.040 --> 00:42:35.440]   from 1 to n, cos ci greater or equal to t,\n",
      "[00:42:35.440 --> 00:42:46.680]   is less or equal to e to the minus t square over 2,\n",
      "[00:42:46.680 --> 00:42:54.640]   summation i from 1 to n, sigma i square, plus 2 over 3,\n",
      "[00:42:54.640 --> 00:43:06.480]   r t for any t, greater than 0.\n",
      "[00:43:06.480 --> 00:43:09.000]   This is essentially the same bound.\n",
      "[00:43:09.000 --> 00:43:12.240]   We have the summation of the variance,\n",
      "[00:43:12.240 --> 00:43:13.920]   and r, we no longer have the summation,\n",
      "[00:43:13.920 --> 00:43:16.280]   but we have actual t here.\n",
      "[00:43:16.280 --> 00:43:18.680]   And eventually, this will be the leading order term.\n",
      "[00:43:18.680 --> 00:43:19.960]   And we can still do the discussion\n",
      "[00:43:19.960 --> 00:43:22.920]   like this leading order term depends on summation of a variance.\n",
      "[00:43:22.920 --> 00:43:26.040]   Well, the previous halfting depends on the summation of the bound,\n",
      "[00:43:26.040 --> 00:43:28.200]   so this will be slightly sharper.\n",
      "[00:43:28.200 --> 00:43:55.920]   [INAUDIBLE]\n",
      "[00:43:55.920 --> 00:43:57.440]   So this will be basically everything.\n",
      "[00:43:57.440 --> 00:44:01.280]   We won't talk about any questions.\n",
      "[00:44:01.280 --> 00:44:03.520]   No, this is basically what we want\n",
      "[00:44:03.520 --> 00:44:06.440]   to talk about for the concentration in equality.\n",
      "[00:44:06.440 --> 00:44:09.560]   So we just want to wrap up and summarize\n",
      "[00:44:09.560 --> 00:44:10.560]   what we have learned.\n",
      "[00:44:10.560 --> 00:44:15.840]   And if you're kind of feel you're not very sure about the proof,\n",
      "[00:44:15.840 --> 00:44:16.960]   it's OK.\n",
      "[00:44:16.960 --> 00:44:20.840]   I think in the later class, we will never\n",
      "[00:44:20.840 --> 00:44:23.560]   use the proof for concentration in equality.\n",
      "[00:44:23.560 --> 00:44:25.280]   All we need is we need to use the results.\n",
      "[00:44:25.280 --> 00:44:38.580]   [INAUDIBLE]\n",
      "[00:44:38.580 --> 00:44:42.240]   So we basically talk about the four different inequalities\n",
      "[00:44:42.240 --> 00:44:50.280]   that is halfting in equality, and the second one\n",
      "[00:44:50.280 --> 00:45:05.360]   is for instance in equality, third one is azuma halfting,\n",
      "[00:45:05.360 --> 00:45:07.640]   and fourth one is azuma-branston.\n",
      "[00:45:07.640 --> 00:45:19.840]   So the halfting is just given for independent random variable.\n",
      "[00:45:19.840 --> 00:45:31.220]   [INAUDIBLE]\n",
      "[00:45:31.220 --> 00:45:36.200]   And plus independent and the bounded random variable.\n",
      "[00:45:36.200 --> 00:45:42.120]   So the branston is for independent\n",
      "[00:45:42.120 --> 00:45:48.160]   and bounded random variable, but with sharper variance\n",
      "[00:45:48.160 --> 00:45:51.120]   condition.\n",
      "[00:45:51.120 --> 00:45:54.880]   So in this case, when the variance is much smaller than the bounds,\n",
      "[00:45:54.880 --> 00:45:57.880]   so this will be sharper than the previous one.\n",
      "[00:45:57.880 --> 00:46:08.560]   And finally, we just relax this independence condition\n",
      "[00:46:08.560 --> 00:46:09.560]   from Martin Goh here.\n",
      "[00:46:09.560 --> 00:46:16.960]   And everything else is the same.\n",
      "[00:46:16.960 --> 00:46:18.200]   It's a halfting and variance condition.\n",
      "[00:46:18.200 --> 00:46:20.200]   It's corresponding, but it's the same.\n",
      "[00:46:20.200 --> 00:46:49.400]   So for now, I hope you now feel\n",
      "[00:46:49.400 --> 00:46:52.120]   like a very powerful-- you have been through already\n",
      "[00:46:52.120 --> 00:46:55.840]   the most difficult mathematical contents of the course.\n",
      "[00:46:55.840 --> 00:46:59.320]   And now, when we do the statistical estimation,\n",
      "[00:46:59.320 --> 00:47:03.440]   we can essentially just call this concentration in equality,\n",
      "[00:47:03.440 --> 00:47:06.680]   and we already have pretty sharp guarantees.\n",
      "[00:47:06.680 --> 00:47:07.920]   And you will do this in homework,\n",
      "[00:47:07.920 --> 00:47:11.000]   and we will also see more in the lecture.\n",
      "[00:47:11.000 --> 00:47:15.800]   So before we go to the talk about reinforcement again,\n",
      "[00:47:15.800 --> 00:47:18.600]   I will also talk about another mathematical bounds,\n",
      "[00:47:18.600 --> 00:47:21.280]   like very simple bounds, which-- but we will also use a lot.\n",
      "[00:47:21.280 --> 00:47:25.280]   It's easy to see it here in isolation.\n",
      "[00:47:25.280 --> 00:47:26.280]   It's very easy to see it.\n",
      "[00:47:26.280 --> 00:47:29.240]   It's called a union bound.\n",
      "[00:47:29.240 --> 00:47:32.320]   So the simple union bound is as easy as the following.\n",
      "[00:47:32.320 --> 00:47:34.960]   That is, let's call it a proposition of one.\n",
      "[00:47:34.960 --> 00:47:46.800]   So I'm sure you will learn this from every basic--\n",
      "[00:47:46.800 --> 00:47:49.160]   like probability and statistical course.\n",
      "[00:47:49.160 --> 00:48:00.320]   We're saying for any random events,\n",
      "[00:48:00.320 --> 00:48:07.520]   A and B are random variables, or just events.\n",
      "[00:48:07.520 --> 00:48:12.320]   So we say the probability of A union B\n",
      "[00:48:12.320 --> 00:48:16.000]   is that either A happens or B happens.\n",
      "[00:48:16.000 --> 00:48:22.440]   We notice this is less or equal to Pa plus Pb.\n",
      "[00:48:22.440 --> 00:48:26.400]   So it's very intuitive.\n",
      "[00:48:26.400 --> 00:48:28.880]   The probability of either A happens or B happens\n",
      "[00:48:28.880 --> 00:48:31.160]   is less or equal to the probability of A happens\n",
      "[00:48:31.160 --> 00:48:34.600]   plus probability of B happens.\n",
      "[00:48:34.600 --> 00:48:36.480]   In fact, we can see the right hand side\n",
      "[00:48:36.480 --> 00:48:42.720]   is equal to the probability of either A or B happens\n",
      "[00:48:42.720 --> 00:48:46.120]   plus the probability of both A and B happens.\n",
      "[00:49:09.320 --> 00:49:13.920]   So this basic form is very straightforward.\n",
      "[00:49:13.920 --> 00:49:16.320]   And I guess you will immediately see it\n",
      "[00:49:16.320 --> 00:49:19.320]   from writing out the equation.\n",
      "[00:49:19.320 --> 00:49:21.880]   I will write out a slightly transformed form\n",
      "[00:49:21.880 --> 00:49:24.800]   where we use a lot throughout the lecture.\n",
      "[00:49:24.800 --> 00:49:27.800]   I think it's easy to see it here, clary one.\n",
      "[00:49:27.800 --> 00:49:34.800]   So this is like an immediate clary of this proposition.\n",
      "[00:49:34.800 --> 00:49:38.040]   Let's say, for example, if we have a lot of events\n",
      "[00:49:38.040 --> 00:49:42.160]   and we know the probability of each individual event\n",
      "[00:49:42.160 --> 00:49:43.440]   as happens with high probability,\n",
      "[00:49:43.440 --> 00:49:47.280]   that is the probability of each individual event, Ai,\n",
      "[00:49:47.280 --> 00:49:51.400]   is greater or equal to 1 minus delta for all i in n.\n",
      "[00:49:51.400 --> 00:50:07.120]   And then we need to calculate the probability of--\n",
      "[00:50:07.120 --> 00:50:10.160]   so each individual event is happening with high probability.\n",
      "[00:50:10.160 --> 00:50:12.640]   And what I want to know, what is the probability of all\n",
      "[00:50:12.640 --> 00:50:16.080]   those high probability events happen simultaneously?\n",
      "[00:50:16.080 --> 00:50:20.160]   That is the union-- that is the intersection of Ai\n",
      "[00:50:20.160 --> 00:50:22.280]   from 1 to n.\n",
      "[00:50:22.280 --> 00:50:24.800]   So what is this probability?\n",
      "[00:50:24.800 --> 00:50:27.600]   All the high probability events happen simultaneously.\n",
      "[00:50:27.600 --> 00:50:30.240]   It turns out you can also prove this probability is greater\n",
      "[00:50:30.240 --> 00:50:34.440]   or equal to 1 minus n delta.\n",
      "[00:50:34.440 --> 00:50:37.240]   So essentially, you pay n factor here,\n",
      "[00:50:37.240 --> 00:50:38.680]   and it's the number of events you're\n",
      "[00:50:38.680 --> 00:50:40.200]   taking its intersection over.\n",
      "[00:50:40.200 --> 00:50:50.200]   And this clary is actually a directly\n",
      "[00:50:50.200 --> 00:50:52.480]   corollary of this union bounds.\n",
      "[00:50:52.480 --> 00:50:55.320]   We will immediately do some proof here.\n",
      "[00:50:55.320 --> 00:50:57.680]   So later on, we will essentially always--\n",
      "[00:50:57.680 --> 00:50:58.960]   like this concentration inequality\n",
      "[00:50:58.960 --> 00:51:01.860]   to prove for some events holds with high probability.\n",
      "[00:51:01.860 --> 00:51:03.800]   And then we want to say all the concentration\n",
      "[00:51:03.800 --> 00:51:04.960]   happens simultaneously.\n",
      "[00:51:04.960 --> 00:51:07.040]   And then we'll just essentially do an intersection\n",
      "[00:51:07.040 --> 00:51:11.160]   over all of them, and we pay some n factor there.\n",
      "[00:51:11.160 --> 00:51:15.320]   That's usually how we handle the math there.\n",
      "[00:51:15.320 --> 00:51:18.520]   So this corollary, why this is true,\n",
      "[00:51:18.520 --> 00:51:27.000]   and we essentially refers right out\n",
      "[00:51:27.000 --> 00:51:28.040]   of what we want to bound.\n",
      "[00:51:28.560 --> 00:51:35.480]   And the one way to calculate this\n",
      "[00:51:35.480 --> 00:51:38.200]   is we look at the complement of this set.\n",
      "[00:51:38.200 --> 00:51:44.280]   So the complement of the intersection of this ai\n",
      "[00:51:44.280 --> 00:51:47.080]   is actually equal to the probability\n",
      "[00:51:47.080 --> 00:51:52.760]   of a union of the complement of ai complement.\n",
      "[00:51:52.760 --> 00:52:02.920]   OK, so the events of all ai happens simultaneously\n",
      "[00:52:02.920 --> 00:52:07.120]   is the complement of at least one of the ai bars happening.\n",
      "[00:52:07.120 --> 00:52:15.440]   So that is in order to do a lower bound for this.\n",
      "[00:52:15.440 --> 00:52:19.800]   We only need to do a upper bound for this complement.\n",
      "[00:52:19.800 --> 00:52:23.240]   And we just look at the probability of the union i\n",
      "[00:52:23.240 --> 00:52:25.620]   from 1 to n ai bar.\n",
      "[00:52:25.620 --> 00:52:32.640]   And now we can immediately apply the union bound.\n",
      "[00:52:32.640 --> 00:52:38.360]   This is less or equal to the probability of a1 bar\n",
      "[00:52:38.360 --> 00:52:44.880]   plus a2 bar into the tau-like probability of an bar,\n",
      "[00:52:44.880 --> 00:52:48.480]   which we know each ai bar is upper bounded by delta.\n",
      "[00:52:48.480 --> 00:52:50.920]   So this is upper bounded by n delta.\n",
      "[00:52:50.920 --> 00:52:52.080]   And we finish the proof.\n",
      "[00:52:52.080 --> 00:52:55.060]   [INAUDIBLE]\n",
      "[00:52:55.060 --> 00:52:58.040]   [SIDE CONVERSATION]\n",
      "[00:52:58.040 --> 00:53:01.020]   [SIDE CONVERSATION]\n",
      "[00:53:01.020 --> 00:53:04.000]   [SIDE CONVERSATION]\n",
      "[00:53:04.000 --> 00:53:06.500]   [SIDE CONVERSATION]\n",
      "[00:53:06.500 --> 00:53:09.480]   [SIDE CONVERSATION]\n",
      "[00:53:09.480 --> 00:53:12.580]   [SIDE CONVERSATION]\n",
      "[00:53:12.580 --> 00:53:15.560]   [SIDE CONVERSATION]\n",
      "[00:53:15.560 --> 00:53:18.560]   [SIDE CONVERSATION]\n",
      "[00:53:18.560 --> 00:53:21.560]   [SIDE CONVERSATION]\n",
      "[00:53:21.560 --> 00:53:24.560]   [SIDE CONVERSATION]\n",
      "[00:53:24.560 --> 00:53:28.540]   [SIDE CONVERSATION]\n",
      "[00:53:28.540 --> 00:53:32.540]   [SIDE CONVERSATION]\n",
      "[00:53:32.540 --> 00:53:36.540]   [SIDE CONVERSATION]\n",
      "[00:53:36.540 --> 00:53:40.540]   [SIDE CONVERSATION]\n",
      "[00:53:40.540 --> 00:53:44.540]   [SIDE CONVERSATION]\n",
      "[00:53:44.540 --> 00:53:47.540]   [SIDE CONVERSATION]\n",
      "[00:53:47.540 --> 00:53:51.540]   [SIDE CONVERSATION]\n",
      "[00:53:51.540 --> 00:53:54.540]   [SIDE CONVERSATION]\n",
      "[00:53:54.540 --> 00:53:57.540]   [SIDE CONVERSATION]\n",
      "[00:53:57.540 --> 00:54:00.540]   [SIDE CONVERSATION]\n",
      "[00:54:00.540 --> 00:54:01.540]   [SIDE CONVERSATION]\n",
      "[00:54:01.540 --> 00:54:03.540]   Okay. Any questions about the construction,\n",
      "[00:54:03.540 --> 00:54:07.540]   and quality union bound or whatever we have talked about so far?\n",
      "[00:54:07.540 --> 00:54:11.540]   And if not, we're going to move to the next part.\n",
      "[00:54:11.540 --> 00:54:13.540]   We're going to talk about how we're going to do\n",
      "[00:54:13.540 --> 00:54:17.540]   learning for like MDP, reinforcement learning.\n",
      "[00:54:17.540 --> 00:54:21.540]   [SIDE CONVERSATION]\n",
      "[00:54:21.540 --> 00:54:26.540]   Okay. So as we promised earlier,\n",
      "[00:54:26.540 --> 00:54:28.540]   all the reason we kind of do this concentration in quality\n",
      "[00:54:28.540 --> 00:54:31.540]   is because we're not only interested in the planning of MDP,\n",
      "[00:54:31.540 --> 00:54:34.540]   but we're also interested in the learning of MDP.\n",
      "[00:54:34.540 --> 00:54:37.540]   So we will start with the easiest learning setting\n",
      "[00:54:37.540 --> 00:54:39.540]   that is called a generative model.\n",
      "[00:54:39.540 --> 00:54:46.540]   [SIDE CONVERSATION]\n",
      "[00:54:46.540 --> 00:54:50.540]   Or a lot of literature also called like a simulator setting.\n",
      "[00:54:50.540 --> 00:54:53.540]   [SIDE CONVERSATION]\n",
      "[00:54:53.540 --> 00:54:59.540]   [SIDE CONVERSATION]\n",
      "[00:54:59.540 --> 00:55:04.540]   So I still want to recall, it's a bit time like last week,\n",
      "[00:55:04.540 --> 00:55:10.540]   we always focus on this MDP where we have a lot of elements\n",
      "[00:55:10.540 --> 00:55:16.540]   that is the set of states, set of action, reward, transition,\n",
      "[00:55:16.540 --> 00:55:20.540]   and also the horizon.\n",
      "[00:55:20.540 --> 00:55:26.540]   So in the last lecture, we're talking about the planning\n",
      "[00:55:26.540 --> 00:55:30.540]   that is we need to know the environment.\n",
      "[00:55:30.540 --> 00:55:34.540]   That is the entire PNR unknown.\n",
      "[00:55:34.540 --> 00:55:37.540]   [SIDE CONVERSATION]\n",
      "[00:55:37.540 --> 00:55:40.540]   So today we'll talk about the learning setting.\n",
      "[00:55:40.540 --> 00:55:42.540]   [SIDE CONVERSATION]\n",
      "[00:55:42.540 --> 00:55:45.540]   This PR are not unknown.\n",
      "[00:55:45.540 --> 00:55:48.540]   [SIDE CONVERSATION]\n",
      "[00:55:48.540 --> 00:55:54.540]   [SIDE CONVERSATION]\n",
      "[00:55:54.540 --> 00:55:57.540]   So whenever we talk about a PR and unknown,\n",
      "[00:55:57.540 --> 00:56:01.540]   we need to talk about how we're going to collect samples\n",
      "[00:56:01.540 --> 00:56:09.540]   and to learn those PR, like what is the product of to collect data.\n",
      "[00:56:09.540 --> 00:56:17.540]   [SIDE CONVERSATION]\n",
      "[00:56:17.540 --> 00:56:20.540]   So the simulator setting is one of the easier setting\n",
      "[00:56:20.540 --> 00:56:22.540]   like how we specify the collected data.\n",
      "[00:56:22.540 --> 00:56:24.540]   And later on, we will talk about more challenging\n",
      "[00:56:24.540 --> 00:56:27.540]   like online and offline setting.\n",
      "[00:56:27.540 --> 00:56:32.540]   [SIDE CONVERSATION]\n",
      "[00:56:32.540 --> 00:56:39.540]   [SIDE CONVERSATION]\n",
      "[00:56:39.540 --> 00:56:42.540]   So for now, let's focus on the tabular setting.\n",
      "[00:56:42.540 --> 00:56:46.540]   In the tabular setting, we means--\n",
      "[00:56:46.540 --> 00:56:49.540]   [SIDE CONVERSATION]\n",
      "[00:56:49.540 --> 00:56:52.540]   We mean the state and action are finite.\n",
      "[00:56:52.540 --> 00:56:55.540]   The setup has finite carnality.\n",
      "[00:56:55.540 --> 00:56:58.540]   [SIDE CONVERSATION]\n",
      "[00:56:58.540 --> 00:57:01.540]   And in the second half of the course,\n",
      "[00:57:01.540 --> 00:57:04.540]   we will like generalize this tabular to like a more generic,\n",
      "[00:57:04.540 --> 00:57:07.540]   like a large state space or you can have infinite number of states\n",
      "[00:57:07.540 --> 00:57:08.540]   or something.\n",
      "[00:57:08.540 --> 00:57:10.540]   But for now, let's just focus on the simplistic case.\n",
      "[00:57:10.540 --> 00:57:13.540]   That is we have finite number of states and finite number of actions.\n",
      "[00:57:13.540 --> 00:57:17.540]   [SIDE CONVERSATION]\n",
      "[00:57:17.540 --> 00:57:18.540]   So what is simulator?\n",
      "[00:57:18.540 --> 00:57:22.540]   A simulator, you can in general be understand as a black box.\n",
      "[00:57:22.540 --> 00:57:29.540]   [SIDE CONVERSATION]\n",
      "[00:57:29.540 --> 00:57:31.540]   You have a simulator here.\n",
      "[00:57:31.540 --> 00:57:34.540]   So a simulator is a one specific way to collect data.\n",
      "[00:57:34.540 --> 00:57:36.540]   [SIDE CONVERSATION]\n",
      "[00:57:36.540 --> 00:57:40.540]   So the simulator essentially allows you to take some input.\n",
      "[00:57:40.540 --> 00:57:42.540]   [SIDE CONVERSATION]\n",
      "[00:57:42.540 --> 00:57:45.540]   So you can ask simulator what is state, what is action,\n",
      "[00:57:45.540 --> 00:57:47.540]   what is current step H?\n",
      "[00:57:47.540 --> 00:57:49.540]   So this is the input of the simulator.\n",
      "[00:57:49.540 --> 00:57:53.540]   [SIDE CONVERSATION]\n",
      "[00:57:53.540 --> 00:57:58.540]   And output of the simulator is the S prime, which is the next state.\n",
      "[00:57:58.540 --> 00:58:02.540]   [SIDE CONVERSATION]\n",
      "[00:58:02.540 --> 00:58:07.540]   And this is like stochastically randomly drawn from the probability,\n",
      "[00:58:07.540 --> 00:58:13.540]   transition probability of SA at H step.\n",
      "[00:58:13.540 --> 00:58:18.540]   And also we will have the reward R. This is like output.\n",
      "[00:58:18.540 --> 00:58:26.540]   [SIDE CONVERSATION]\n",
      "[00:58:26.540 --> 00:58:31.540]   And this R will be just equal to R S in.\n",
      "[00:58:31.540 --> 00:58:37.540]   [SIDE CONVERSATION]\n",
      "[00:58:37.540 --> 00:58:41.540]   You can essentially think you have a computer program called a simulator.\n",
      "[00:58:41.540 --> 00:58:44.540]   You can just query whatever the state action is.\n",
      "[00:58:44.540 --> 00:58:48.540]   It will not directly tell you what is the transition, what is the reward,\n",
      "[00:58:48.540 --> 00:58:54.540]   but instead it will give you a random sample of the next states and the reward.\n",
      "[00:58:54.540 --> 00:59:01.540]   [SIDE CONVERSATION]\n",
      "[00:59:01.540 --> 00:59:08.540]   [SIDE CONVERSATION]\n",
      "[00:59:08.540 --> 00:59:10.540]   So we'll comment a little bit here.\n",
      "[00:59:10.540 --> 00:59:13.540]   We will again assume this reward is deterministic.\n",
      "[00:59:13.540 --> 00:59:17.540]   That has no randomness here.\n",
      "[00:59:17.540 --> 00:59:26.540]   I think the reason it's OK in a sense like it's not without lots of generality.\n",
      "[00:59:26.540 --> 00:59:31.540]   Because in this case we usually need to learn two things.\n",
      "[00:59:31.540 --> 00:59:37.540]   That is this as distribution, where this is like a multinomial distribution.\n",
      "[00:59:37.540 --> 00:59:39.540]   And the other thing we need to learn is the reward.\n",
      "[00:59:39.540 --> 00:59:42.540]   And in fact, we only need to learn the meaning of the reward.\n",
      "[00:59:42.540 --> 00:59:44.540]   That is the expectation of the reward.\n",
      "[00:59:44.540 --> 00:59:45.540]   And we assume it's deterministic.\n",
      "[00:59:45.540 --> 00:59:50.540]   It's OK because in general we're learning a multinomial distribution.\n",
      "[00:59:50.540 --> 00:59:57.540]   [SIDE CONVERSATION]\n",
      "[00:59:57.540 --> 01:00:04.540]   [SIDE CONVERSATION]\n",
      "[01:00:04.540 --> 01:00:12.540]   This is harder than learning the expectation of a scalar random variable.\n",
      "[01:00:12.540 --> 01:00:33.540]   [SIDE CONVERSATION]\n",
      "[01:00:33.540 --> 01:00:36.540]   Since all I think, although we assume it's deterministic,\n",
      "[01:00:36.540 --> 01:00:41.540]   but if you make it random, all the proof will be easily translated there\n",
      "[01:00:41.540 --> 01:00:43.540]   with very little modification.\n",
      "[01:00:43.540 --> 01:00:46.540]   For simplicity, we just assume reward is deterministic.\n",
      "[01:00:46.540 --> 01:00:52.540]   And only the randomness is through the next stage, the phase generation.\n",
      "[01:00:52.540 --> 01:01:05.540]   [SIDE CONVERSATION]\n",
      "[01:01:05.540 --> 01:01:13.540]   So let's intuitively understand what is this power of simulator and what it tells us.\n",
      "[01:01:13.540 --> 01:01:21.540]   Let's look at an example of like a maze.\n",
      "[01:01:21.540 --> 01:01:28.540]   [SIDE CONVERSATION]\n",
      "[01:01:28.540 --> 01:01:39.540]   A maze essentially is a map where you have a lot of wars here and there or something.\n",
      "[01:01:39.540 --> 01:01:42.540]   [SIDE CONVERSATION]\n",
      "[01:01:42.540 --> 01:01:45.540]   Let's just randomly draw something like that.\n",
      "[01:01:45.540 --> 01:01:47.540]   And we have a starting point here.\n",
      "[01:01:47.540 --> 01:01:51.540]   [SIDE CONVERSATION]\n",
      "[01:01:51.540 --> 01:01:58.540]   And we have an endpoint here.\n",
      "[01:01:58.540 --> 01:02:01.540]   [SIDE CONVERSATION]\n",
      "[01:02:01.540 --> 01:02:12.540]   So in this problem, we can formulate as an MDP where state is like the position in the maze.\n",
      "[01:02:12.540 --> 01:02:19.540]   An action is essentially we have four possible actions.\n",
      "[01:02:19.540 --> 01:02:22.540]   We can move upwards, downwards, and move leftwards, rightwards.\n",
      "[01:02:22.540 --> 01:02:24.540]   Essentially, we have four actions.\n",
      "[01:02:24.540 --> 01:02:31.540]   [SIDE CONVERSATION]\n",
      "[01:02:31.540 --> 01:02:39.540]   So the next state, the output of the simulator or usually the next state is the position.\n",
      "[01:02:39.540 --> 01:02:42.540]   [SIDE CONVERSATION]\n",
      "[01:02:42.540 --> 01:02:44.540]   After moved.\n",
      "[01:02:44.540 --> 01:02:47.540]   [SIDE CONVERSATION]\n",
      "[01:02:47.540 --> 01:02:52.540]   So in this case, for example, you are moving towards the direction of war.\n",
      "[01:02:52.540 --> 01:02:54.540]   Like, for example, you are moving on the right.\n",
      "[01:02:54.540 --> 01:02:55.540]   Like, you are going right.\n",
      "[01:02:55.540 --> 01:02:57.540]   You are taking an action to go right.\n",
      "[01:02:57.540 --> 01:03:00.540]   And in this case, you are stuck in this position because you cannot go right.\n",
      "[01:03:00.540 --> 01:03:03.540]   But if you go down from this position and you can go down.\n",
      "[01:03:03.540 --> 01:03:07.540]   So the next state will actually be a function of the action you are taking\n",
      "[01:03:07.540 --> 01:03:11.540]   and you are starting position and also whether you have a war, like some environment there.\n",
      "[01:03:11.540 --> 01:03:14.540]   So it's like it's complicated, a transition probability.\n",
      "[01:03:14.540 --> 01:03:20.540]   [SIDE CONVERSATION]\n",
      "[01:03:20.540 --> 01:03:32.540]   And the reward is essentially you get one if you leave the, if you reach the end.\n",
      "[01:03:32.540 --> 01:03:38.540]   [SIDE CONVERSATION]\n",
      "[01:03:38.540 --> 01:03:44.540]   [SIDE CONVERSATION]\n",
      "[01:03:44.540 --> 01:03:58.540]   [SIDE CONVERSATION]\n",
      "[01:03:58.540 --> 01:04:04.540]   So we talk about the important thing here is like how we are going to collect the data to learn this maze.\n",
      "[01:04:04.540 --> 01:04:06.540]   Like, imagine you are an agent.\n",
      "[01:04:06.540 --> 01:04:09.540]   So you actually, originally, you don't really know the map of maze.\n",
      "[01:04:09.540 --> 01:04:12.540]   So you cannot really know whether you can go right at this position or not.\n",
      "[01:04:12.540 --> 01:04:14.540]   You are keeping the duck.\n",
      "[01:04:14.540 --> 01:04:17.540]   So in this case, I think the simulator setting.\n",
      "[01:04:17.540 --> 01:04:20.540]   [SIDE CONVERSATION]\n",
      "[01:04:20.540 --> 01:04:23.540]   It essentially corresponds to the way of learning.\n",
      "[01:04:23.540 --> 01:04:28.540]   Essentially, you can query any state action pair and you get a next state and reward.\n",
      "[01:04:28.540 --> 01:04:49.540]   So it essentially corresponds to, agent is allowed to teleport to any location\n",
      "[01:04:49.540 --> 01:04:57.540]   to learn the map in the neighborhood.\n",
      "[01:04:57.540 --> 01:05:02.540]   In this case, in the maze problem, the entire environment is essentially the map.\n",
      "[01:05:02.540 --> 01:05:08.540]   And the query of any state action pair corresponds to you are allowed to teleport to anywhere.\n",
      "[01:05:08.540 --> 01:05:13.540]   So this is somehow quite strong.\n",
      "[01:05:13.540 --> 01:05:18.540]   For some application, for example, for some simple games, you are actually able to do that.\n",
      "[01:05:18.540 --> 01:05:23.540]   But for a lot of complicated games or like for robotic tests, maybe you cannot do something like that.\n",
      "[01:05:23.540 --> 01:05:29.540]   So this corresponds to later, we will talk about two settings, online setting and offline setting.\n",
      "[01:05:29.540 --> 01:05:34.540]   [SIDE CONVERSATION]\n",
      "[01:05:34.540 --> 01:05:38.540]   And in the later online setting, you are no longer allowed to be teleport to any location.\n",
      "[01:05:38.540 --> 01:05:50.540]   You have to reset to the star location.\n",
      "[01:05:50.540 --> 01:05:54.540]   So this is more challenging because you can no longer teleport to any location.\n",
      "[01:05:54.540 --> 01:05:57.540]   Every time you want to reset, you have to go from the very beginning.\n",
      "[01:05:57.540 --> 01:06:03.540]   So in that case, if you are allowed to teleport, scanning through the map is pretty trivial.\n",
      "[01:06:03.540 --> 01:06:09.540]   You just query all the possible states. But in this online case, even reach some states is not very trivial.\n",
      "[01:06:09.540 --> 01:06:15.540]   You need to have some reasonable policies so that you can reach some location of the states.\n",
      "[01:06:15.540 --> 01:06:33.540]   [SIDE CONVERSATION]\n",
      "[01:06:33.540 --> 01:06:44.540]   And in the offline setting, you are given some data, given some trajectories,\n",
      "[01:06:44.540 --> 01:06:47.540]   collected by other agents.\n",
      "[01:06:47.540 --> 01:06:54.540]   [SIDE CONVERSATION]\n",
      "[01:06:54.540 --> 01:07:01.540]   So in this way, it's also more difficult. In a sense, like now, in a similar case, you are allowed to teleport to any location.\n",
      "[01:07:01.540 --> 01:07:05.540]   You want to learn whatever you can basically learn whatever you want to learn.\n",
      "[01:07:05.540 --> 01:07:09.540]   But here, you are very limited to what other agents have done so far.\n",
      "[01:07:09.540 --> 01:07:14.540]   And you need to learn good policy from those limited data collected by other agents.\n",
      "[01:07:14.540 --> 01:07:19.540]   So those are two more challenging settings we will talk later in this lecture.\n",
      "[01:07:19.540 --> 01:07:23.540]   So far, we will focus on a simulator for now because this is an easier setting\n",
      "[01:07:23.540 --> 01:07:27.540]   and we will always start from an easier setting and do the result there first.\n",
      "[01:07:27.540 --> 01:07:30.540]   And then we go to a more complicated scenario.\n",
      "[01:07:30.540 --> 01:07:55.540]   [SIDE CONVERSATION]\n",
      "[01:07:55.540 --> 01:08:02.540]   We just want to recall that the final goal of reinforcement learning is we want to find some policy.\n",
      "[01:08:02.540 --> 01:08:07.540]   [SIDE CONVERSATION]\n",
      "[01:08:07.540 --> 01:08:16.540]   Let's say pi hat so that I think a lot of times because now we have the statistical arrow,\n",
      "[01:08:16.540 --> 01:08:19.540]   like we never able to find exactly the optimal policy,\n",
      "[01:08:19.540 --> 01:08:22.540]   but we want to find something close to the optimal policy.\n",
      "[01:08:22.540 --> 01:08:27.540]   It's a epsilon optimal and epsilon optimal is measured in terms of a value.\n",
      "[01:08:27.540 --> 01:08:31.540]   So we will look at the value of the initial states.\n",
      "[01:08:31.540 --> 01:08:34.540]   Let's consider again the fixed initial states.\n",
      "[01:08:34.540 --> 01:08:39.540]   [SIDE CONVERSATION]\n",
      "[01:08:39.540 --> 01:08:43.540]   So basically, this is the optimal value we can achieve.\n",
      "[01:08:43.540 --> 01:08:46.540]   That is the value achieved by the optimal policy.\n",
      "[01:08:46.540 --> 01:08:51.540]   So in terms of epsilon optimality, we need to measure the value in terms of the comparison\n",
      "[01:08:51.540 --> 01:08:53.540]   to the optimal value we can't achieve.\n",
      "[01:08:53.540 --> 01:09:02.540]   So a typical way is we say this subtract the value of the policy we find by the algorithm\n",
      "[01:09:02.540 --> 01:09:07.540]   is upper bounded by epsilon.\n",
      "[01:09:07.540 --> 01:09:11.540]   So in this case, we'll guarantee our value is reasonably large.\n",
      "[01:09:11.540 --> 01:09:19.540]   It's at least the optimal value subtract by epsilon.\n",
      "[01:09:19.540 --> 01:09:24.540]   [INAUDIBLE]\n",
      "[01:09:24.540 --> 01:09:27.540]   Because we always have statistical arrow there.\n",
      "[01:09:27.540 --> 01:09:28.540]   We only observe the sample.\n",
      "[01:09:28.540 --> 01:09:34.540]   And even with concentration inequality, we have some arrow in estimating the true probability.\n",
      "[01:09:34.540 --> 01:09:36.540]   We never able to learn what is true probability.\n",
      "[01:09:36.540 --> 01:09:44.540]   So you always suffer some statistical arrow.\n",
      "[01:09:44.540 --> 01:09:51.540]   And one of the very important questions we will ask in this scenario is also called sample complexity.\n",
      "[01:09:51.540 --> 01:09:56.540]   [SIDE CONVERSATION]\n",
      "[01:09:56.540 --> 01:10:01.540]   So sample complexity essentially, we will ask how many queries.\n",
      "[01:10:01.540 --> 01:10:08.540]   [SIDE CONVERSATION]\n",
      "[01:10:08.540 --> 01:10:20.540]   We need a query essentially, we say one query is like, we ask a similar one SH and a similar\n",
      "[01:10:20.540 --> 01:10:23.540]   to give me one next state and one reward.\n",
      "[01:10:23.540 --> 01:10:25.540]   We call it one query.\n",
      "[01:10:25.540 --> 01:10:31.540]   And we say how many queries we need to find epsilon optimal policy.\n",
      "[01:10:31.540 --> 01:10:39.540]   [SIDE CONVERSATION]\n",
      "[01:10:39.540 --> 01:10:46.540]   [SIDE CONVERSATION]\n",
      "[01:10:46.540 --> 01:10:56.540]   [SIDE CONVERSATION]\n",
      "[01:10:56.540 --> 01:11:24.540]   [SIDE CONVERSATION]\n",
      "[01:11:24.540 --> 01:11:25.540]   We are talking about a setting.\n",
      "[01:11:25.540 --> 01:11:27.540]   Now let's talk about our solutions.\n",
      "[01:11:27.540 --> 01:11:33.540]   If we are given this similar setting, what would be the algorithm you are going to develop to solve this?\n",
      "[01:11:33.540 --> 01:11:39.540]   We say in the planning scenario, we give exactly the P and R and then we can directly use the\n",
      "[01:11:39.540 --> 01:11:40.540]   Belmo optimal equation.\n",
      "[01:11:40.540 --> 01:11:44.540]   And now the only thing different, we no longer have access to the P.\n",
      "[01:11:44.540 --> 01:11:50.540]   So one naive strategy you might think of is I want to first use this sample to estimate P.\n",
      "[01:11:50.540 --> 01:11:54.540]   And then I will just directly use the Belmo optimal equation.\n",
      "[01:11:54.540 --> 01:11:56.540]   And that sounds like a pretty straightforward solution.\n",
      "[01:11:56.540 --> 01:11:59.540]   And this is exactly what we're going to study next.\n",
      "[01:11:59.540 --> 01:12:07.540]   So we essentially going to -- so the only thing I'll know here is the transition probability.\n",
      "[01:12:07.540 --> 01:12:16.540]   The reward is more deterministic, so we don't really need to estimate it.\n",
      "[01:12:16.540 --> 01:12:24.540]   So one way to estimate this transition probability of pH assay is we can calculate the empirical\n",
      "[01:12:24.540 --> 01:12:25.540]   frequency.\n",
      "[01:12:25.540 --> 01:12:36.540]   That is, we will essentially sample a lot of S1 prime, S2 prime, TOS and prime.\n",
      "[01:12:36.540 --> 01:12:51.540]   So we'll sample a lot of samples by query simulator, the same as AH.\n",
      "[01:12:51.540 --> 01:12:57.540]   So basically keep repeating like query simulator, the same step H and the same state action\n",
      "[01:12:57.540 --> 01:12:58.540]   assay.\n",
      "[01:12:58.540 --> 01:13:00.540]   And then we get a lot of the next state samples.\n",
      "[01:13:00.540 --> 01:13:05.540]   They are all like random samples from the next state distribution.\n",
      "[01:13:05.540 --> 01:13:11.540]   And we will estimate this transition probability to be basically the empirical frequency that\n",
      "[01:13:11.540 --> 01:13:24.540]   is 1 over n times the summation i from 1 to n indicator of S i prime equal to S prime.\n",
      "[01:13:24.540 --> 01:13:26.540]   So what is the probability I'm going to estimate?\n",
      "[01:13:26.540 --> 01:13:28.540]   I would just look at all those samples.\n",
      "[01:13:28.540 --> 01:13:34.540]   And I see like basically how many times I observe the next state is exactly equal to S prime.\n",
      "[01:13:34.540 --> 01:13:35.540]   And I divide it by n.\n",
      "[01:13:35.540 --> 01:13:37.540]   This is essentially my empirical frequency.\n",
      "[01:13:37.540 --> 01:13:53.540]   And I use this as my estimate of transition probability.\n",
      "[01:13:53.540 --> 01:14:03.540]   So this gives a very straightforward algorithm we're going to actually analysis.\n",
      "[01:14:03.540 --> 01:14:15.540]   Again, we call it a value iteration, but in a simulator setting.\n",
      "[01:14:15.540 --> 01:14:17.540]   We're essentially just doing this two steps.\n",
      "[01:14:17.540 --> 01:14:35.540]   So we first estimate the p and then we use the velmo-optimistic equation.\n",
      "[01:14:35.540 --> 01:14:39.540]   So users need to specify what is the n sample here.\n",
      "[01:14:39.540 --> 01:14:47.540]   Like how many samples we need to collect for each state action pair.\n",
      "[01:14:47.540 --> 01:14:50.540]   And then the algorithm will run the follows.\n",
      "[01:14:50.540 --> 01:14:56.540]   That is for all state action each, essentially we do a sweep over all the state action pair\n",
      "[01:14:56.540 --> 01:15:04.540]   and for every step.\n",
      "[01:15:04.540 --> 01:15:23.540]   We do the following that we will query S a h and collect n samples.\n",
      "[01:15:23.540 --> 01:15:29.540]   That is S 1 prime to S m prime.\n",
      "[01:15:29.540 --> 01:15:46.540]   And I will directly do the estimation of the transition probability.\n",
      "[01:15:46.540 --> 01:16:08.540]   By this star equation, let's call this star, or S prime in S.\n",
      "[01:16:08.540 --> 01:16:19.540]   And then we will just do the velmo-optimistic equation.\n",
      "[01:16:19.540 --> 01:16:44.540]   The only difference for the original velmo-optimistic equation is now we no longer have access to the p,\n",
      "[01:16:44.540 --> 01:17:04.540]   so we will use the p hat, our estimate to replace p in the velmo-optimistic equation.\n",
      "[01:17:04.540 --> 01:17:30.540]   V h star hat S a is equal to the max taking over action q hat star S a.\n",
      "[01:17:30.540 --> 01:17:57.540]   And the output is just the greedy policy with respect to this q hat, which is, we output this p hat h S is equal to just the arg max.\n",
      "[01:17:57.540 --> 01:18:06.540]   So, we are going to use our q hat h S a.\n",
      "[01:18:06.540 --> 01:18:08.540]   So, essentially it has two steps.\n",
      "[01:18:08.540 --> 01:18:15.540]   One is you estimate the transition matrix p.\n",
      "[01:18:15.540 --> 01:18:22.540]   And the second is doing like a value iteration, original value iteration, that is the velmo-optimistic equation.\n",
      "[01:18:22.540 --> 01:18:30.540]   So, we have the estimated MDP, which used the new transition p hat, our estimate, and the reward.\n",
      "[01:18:30.540 --> 01:18:36.540]   Essentially, the algorithm is following these two steps.\n",
      "[01:18:36.540 --> 01:18:37.540]   Okay.\n",
      "[01:18:37.540 --> 01:18:43.540]   This is also like what is, what we will think, yes.\n",
      "[01:18:43.540 --> 01:18:44.540]   Right.\n",
      "[01:18:44.540 --> 01:18:51.540]   I think because we're going to take a sweep over all the state action pair, and so you're all going to observe what at least once.\n",
      "[01:18:51.540 --> 01:18:53.540]   So, we kind of just know that reward.\n",
      "[01:18:53.540 --> 01:19:00.540]   So, I didn't write it here, but you should, like, just capture reward.\n",
      "[01:19:00.540 --> 01:19:01.540]   Okay.\n",
      "[01:19:01.540 --> 01:19:09.540]   So, one very important thing here, we want to still know, like, in terms of theories, like we said about our sample complexity.\n",
      "[01:19:09.540 --> 01:19:12.540]   Like, we just say we're going to collect N samples and do this estimate.\n",
      "[01:19:12.540 --> 01:19:20.540]   But the question is how many samples we need to collect, so to guarantee, we're going to have the epsilon optimal policy.\n",
      "[01:19:20.540 --> 01:19:22.540]   And this is what we're going to address in the next lecture.\n",
      "[01:19:22.540 --> 01:19:29.540]   We're going to compute, essentially, provide some guarantees for the sample complexity of this very tuition algorithm.\n",
      "[01:19:29.540 --> 01:19:38.540]   And we will actually see, like, roughly how many samples we need to learn to, we need to learn the MDP, like, learn the optimal policy accurately.\n",
      "[01:19:38.540 --> 01:19:39.540]   Okay.\n",
      "[01:19:39.540 --> 01:19:43.540]   And how is skill with number of states, number of action, and the horizon that's.\n",
      "[01:19:43.540 --> 01:19:48.540]   [MUSIC]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/Szae6pHKr60.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/Szae6pHKr60.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/Szae6pHKr60.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/Szae6pHKr60.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1278.81 ms\n",
      "whisper_print_timings:     fallbacks =   8 p /  19 h\n",
      "whisper_print_timings:      mel time =  2745.29 ms\n",
      "whisper_print_timings:   sample time = 24522.40 ms / 63050 runs (    0.39 ms per run)\n",
      "whisper_print_timings:   encode time =   369.28 ms /   201 runs (    1.84 ms per run)\n",
      "whisper_print_timings:   decode time =   718.52 ms /   404 runs (    1.78 ms per run)\n",
      "whisper_print_timings:   batchd time = 32303.28 ms / 61604 runs (    0.52 ms per run)\n",
      "whisper_print_timings:   prompt time = 10317.14 ms / 46825 runs (    0.22 ms per run)\n",
      "whisper_print_timings:    total time = 72718.23 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=_Hxt8CibzIs started\n",
      "_Hxt8CibzIs\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/_Hxt8CibzIs.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/_Hxt8CibzIs.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:19:24.83, start: 0.000000, bitrate: 271 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 172 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/_Hxt8CibzIs.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  148901kB time=01:19:24.82 bitrate= 256.0kbits/s speed=1.4e+03x     \n",
      "video:0kB audio:148901kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000051%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/_Hxt8CibzIs.wav' (76237241 samples, 4764.8 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.080]   [MUSIC PLAYING]\n",
      "[00:00:03.080 --> 00:00:09.320]   So the happening thing in quality says that x1 to xn\n",
      "[00:00:09.320 --> 00:00:23.680]   be independent of integrables, where with probability 1,\n",
      "[00:00:23.680 --> 00:00:24.640]   I would just use shot.\n",
      "[00:00:24.640 --> 00:00:29.200]   So with probability that is w dot p dot,\n",
      "[00:00:29.200 --> 00:00:32.120]   with probability 1, that x is bounded--\n",
      "[00:00:32.120 --> 00:00:37.920]   x n sound like a bounded domain, that is in ai and bi.\n",
      "[00:00:37.920 --> 00:00:46.040]   And we let ri donate the length of the h interval.\n",
      "[00:00:49.000 --> 00:00:59.860]   And then we have for rt greater equal to 0.\n",
      "[00:00:59.860 --> 00:01:07.440]   And that is the probability of summation xi minus e xi.\n",
      "[00:01:07.440 --> 00:01:09.880]   That is the fluctuation of each variable.\n",
      "[00:01:09.880 --> 00:01:13.840]   And I sum them together from 1 to n.\n",
      "[00:01:13.840 --> 00:01:16.400]   We say this fluctuation is greater than t.\n",
      "[00:01:16.400 --> 00:01:17.800]   And the probability of this event\n",
      "[00:01:17.800 --> 00:01:21.240]   that happened is less than something exponential\n",
      "[00:01:21.240 --> 00:01:29.400]   to the minus 2t squared over summation i from 1 to n i\n",
      "[00:01:29.400 --> 00:01:32.380]   squared.\n",
      "[00:01:32.380 --> 00:01:35.380]   And in the last lecture, we say this is essentially\n",
      "[00:01:35.380 --> 00:01:36.440]   some tail bound.\n",
      "[00:01:36.440 --> 00:01:40.280]   A tail bound, in a sense, we're saying some distribution,\n",
      "[00:01:40.280 --> 00:01:45.320]   which if we look at this summation of xi as something\n",
      "[00:01:45.320 --> 00:01:50.400]   like Sn, which is i from 1 to n, this summation.\n",
      "[00:01:50.400 --> 00:01:53.480]   And this is like the distribution of Sn.\n",
      "[00:01:53.480 --> 00:01:56.280]   And this is the expectation of Sn,\n",
      "[00:01:56.280 --> 00:02:01.160]   where it essentially says the probability of it\n",
      "[00:02:01.160 --> 00:02:04.560]   goes beyond the expectation plus t.\n",
      "[00:02:04.560 --> 00:02:06.320]   So this is like offset.\n",
      "[00:02:06.320 --> 00:02:08.640]   We're saying the total probability,\n",
      "[00:02:08.640 --> 00:02:12.040]   if the random variable goes beyond the expectation plus t,\n",
      "[00:02:12.040 --> 00:02:13.160]   is like pretty small.\n",
      "[00:02:13.160 --> 00:02:16.120]   It's something bounded by something exponentially small.\n",
      "[00:02:16.120 --> 00:02:19.120]   Exponential to the e to the minus t squared, something like that.\n",
      "[00:02:19.120 --> 00:02:20.840]   So this is like a tail bound.\n",
      "[00:02:20.840 --> 00:02:23.200]   Since this tail probability is very, very small.\n",
      "[00:02:23.200 --> 00:02:26.120]   So most of the probability mass is concentrated\n",
      "[00:02:26.120 --> 00:02:27.400]   around the expectation.\n",
      "[00:02:27.400 --> 00:02:29.960]   So that's why it's called concentration inequality.\n",
      "[00:02:29.960 --> 00:02:37.720]   So today, we will talk about why something like this concentration\n",
      "[00:02:37.720 --> 00:02:39.280]   is actually true.\n",
      "[00:02:39.280 --> 00:02:40.800]   So before we're talking about that,\n",
      "[00:02:40.800 --> 00:02:43.880]   I will also just do a slightly transformation\n",
      "[00:02:43.880 --> 00:02:48.560]   of the theorem so that to give some formula which\n",
      "[00:02:48.560 --> 00:02:52.240]   we will frequently use in the class.\n",
      "[00:02:52.240 --> 00:02:54.080]   So the one very easy thing to do is\n",
      "[00:02:54.080 --> 00:02:56.800]   we can make this probability equal to delta.\n",
      "[00:02:56.800 --> 00:02:59.680]   So we just do some change of variable.\n",
      "[00:02:59.680 --> 00:03:02.160]   Let's say we say this delta is defined to be\n",
      "[00:03:02.160 --> 00:03:09.000]   e to the minus 2t squared over summation i squared.\n",
      "[00:03:09.000 --> 00:03:11.640]   i from 1 to t.\n",
      "[00:03:11.640 --> 00:03:12.600]   Oh, i from 1 to n.\n",
      "[00:03:12.600 --> 00:03:23.120]   So essentially, I'm just saying the probability is delta.\n",
      "[00:03:23.120 --> 00:03:25.080]   When we set the probability equal to delta,\n",
      "[00:03:25.080 --> 00:03:28.880]   then we can calculate what is this t.\n",
      "[00:03:28.880 --> 00:03:31.840]   We can express this t in terms of this delta.\n",
      "[00:03:31.840 --> 00:03:34.960]   So it turns out by doing this, we can easily\n",
      "[00:03:34.960 --> 00:03:39.920]   solve that t is equal to something like a square root\n",
      "[00:03:39.920 --> 00:03:49.840]   of summation i from 1 to n i squared log 1 over delta over 2.\n",
      "[00:03:49.840 --> 00:03:57.280]   This is just some change of variable.\n",
      "[00:03:57.280 --> 00:03:58.920]   So in this case, we can immediately\n",
      "[00:03:58.920 --> 00:04:01.200]   get the same result, the same half-ding bound,\n",
      "[00:04:01.200 --> 00:04:05.080]   but it's written in some slightly different way.\n",
      "[00:04:05.080 --> 00:04:08.800]   So this is a query, 1.\n",
      "[00:04:08.800 --> 00:04:27.200]   We say under the same condition, as theorem 1.\n",
      "[00:04:28.200 --> 00:04:43.320]   We can see for any delta with probability,\n",
      "[00:04:43.320 --> 00:04:57.160]   at least 1 minus delta, we have that summation.\n",
      "[00:04:57.160 --> 00:05:09.720]   Again, this fluctuation i from 1 to n is less than\n",
      "[00:05:09.720 --> 00:05:20.280]   square root of summation i from 1 to n i square log 1 over delta\n",
      "[00:05:20.280 --> 00:05:20.840]   over 2.\n",
      "[00:05:24.280 --> 00:05:25.960]   This is essentially the same thing.\n",
      "[00:05:25.960 --> 00:05:31.840]   So this theorem 1 says the probability\n",
      "[00:05:31.840 --> 00:05:35.360]   of greater than some threshold is very, very small.\n",
      "[00:05:35.360 --> 00:05:37.560]   So this essentially says with majority property,\n",
      "[00:05:37.560 --> 00:05:40.920]   that is, if we get a read of this low probability events,\n",
      "[00:05:40.920 --> 00:05:42.520]   what's with high probability?\n",
      "[00:05:42.520 --> 00:05:45.640]   What happens is this fluctuation will be smaller than something.\n",
      "[00:05:45.640 --> 00:05:49.600]   And this thing is basically solved by a change variable.\n",
      "[00:05:49.600 --> 00:05:51.840]   So that I make this probability to be delta.\n",
      "[00:05:51.840 --> 00:05:54.280]   And then explicitly write out what this T is.\n",
      "[00:05:54.280 --> 00:05:58.800]   Yes?\n",
      "[00:05:58.800 --> 00:06:03.320]   What is the meaning of with probability 1 to 1 minus delta?\n",
      "[00:06:03.320 --> 00:06:07.760]   With probability 1 is essentially some probability statement.\n",
      "[00:06:07.760 --> 00:06:09.520]   You can just think with probability 1, essentially,\n",
      "[00:06:09.520 --> 00:06:11.480]   just this always happens.\n",
      "[00:06:11.480 --> 00:06:14.920]   This is just-- xi is always bounded.\n",
      "[00:06:14.920 --> 00:06:16.440]   And with probability 1 minus delta\n",
      "[00:06:16.440 --> 00:06:18.520]   is because this is a probability event.\n",
      "[00:06:18.520 --> 00:06:20.080]   This is a random variable.\n",
      "[00:06:20.080 --> 00:06:23.360]   So with some probability, you're going to end up here.\n",
      "[00:06:23.360 --> 00:06:24.920]   So this is like a tail events.\n",
      "[00:06:24.920 --> 00:06:27.680]   So with 1 minus delta is essentially what we say here.\n",
      "[00:06:27.680 --> 00:06:29.880]   So we get a read of this tail events.\n",
      "[00:06:29.880 --> 00:06:32.200]   So we say with high probability that is--\n",
      "[00:06:32.200 --> 00:06:35.200]   so essentially, we just make this probability to be delta.\n",
      "[00:06:35.200 --> 00:06:37.600]   We're saying once 1 minus delta probability,\n",
      "[00:06:37.600 --> 00:06:39.320]   we're going to be smaller than this threshold.\n",
      "[00:06:39.320 --> 00:06:43.440]   And this threshold, we can't compute this one.\n",
      "[00:06:43.440 --> 00:06:44.280]   You all come.\n",
      "[00:06:44.280 --> 00:06:49.280]   OK.\n",
      "[00:06:49.280 --> 00:07:01.680]   So I think the benefit of doing this is a lot of times we can't\n",
      "[00:07:01.680 --> 00:07:03.760]   just say this data is something like extremely small,\n",
      "[00:07:03.760 --> 00:07:08.720]   like 0.1 or 0.0 or 0.0 or 0.0 or something like that.\n",
      "[00:07:08.720 --> 00:07:12.880]   And we really want to look at how this fluctuation of fluctuation\n",
      "[00:07:12.880 --> 00:07:17.040]   like a scale with n, like a scale with the number of samples\n",
      "[00:07:17.040 --> 00:07:18.280]   we have.\n",
      "[00:07:18.280 --> 00:07:25.600]   So in this case, we can just consider a simple case\n",
      "[00:07:25.600 --> 00:07:31.440]   where we can say if r1 equal to r2 equal to rn.\n",
      "[00:07:31.440 --> 00:07:34.760]   So in this case, for example, all the random variable\n",
      "[00:07:34.760 --> 00:07:39.560]   are identically like a sample from the same distribution.\n",
      "[00:07:39.560 --> 00:07:41.800]   In this kind of scenario, it will be much easier\n",
      "[00:07:41.800 --> 00:07:44.360]   to see how those things scale.\n",
      "[00:07:44.360 --> 00:07:48.960]   So we can roughly say this r summation, i from 1 to n,\n",
      "[00:07:48.960 --> 00:07:50.920]   xi minus e xi.\n",
      "[00:07:50.920 --> 00:07:53.760]   This is the summation of the fluctuation.\n",
      "[00:07:53.760 --> 00:07:58.840]   And we know if this ri becomes r, it's just nr square.\n",
      "[00:07:58.840 --> 00:08:01.560]   So the entire scaling would be something like square root n.\n",
      "[00:08:01.560 --> 00:08:10.760]   Actually, it's like o squared n because that's all equal to.\n",
      "[00:08:11.760 --> 00:08:27.760]   And if we consider the average, this scale is 1 over square root n.\n",
      "[00:08:27.760 --> 00:08:35.760]   OK.\n",
      "[00:08:39.760 --> 00:08:44.600]   So why is this claim interesting or like a very strong--\n",
      "[00:08:44.600 --> 00:08:46.840]   or like a non-trivial?\n",
      "[00:08:46.840 --> 00:08:50.520]   So we can think-- for example, we can write out\n",
      "[00:08:50.520 --> 00:08:55.840]   what is this like how s1 to sn grows.\n",
      "[00:08:55.840 --> 00:08:57.840]   So essentially, we have-- we start from--\n",
      "[00:08:57.840 --> 00:09:06.360]   so this fluctuation is something like a random work\n",
      "[00:09:06.360 --> 00:09:10.520]   we start from here. And for example, I go first here.\n",
      "[00:09:10.520 --> 00:09:12.200]   This is like s1.\n",
      "[00:09:12.200 --> 00:09:16.360]   S1, s1 is like the summation of the first fluctuation.\n",
      "[00:09:16.360 --> 00:09:19.440]   And then I go to second summation, like some\n",
      "[00:09:19.440 --> 00:09:22.640]   of the two fluctuation and summation of three fluctuation.\n",
      "[00:09:22.640 --> 00:09:24.160]   So it will be something like that.\n",
      "[00:09:24.160 --> 00:09:28.040]   This is like a random work.\n",
      "[00:09:28.040 --> 00:09:34.400]   So what we are seeing here is if this is not random,\n",
      "[00:09:34.400 --> 00:09:36.920]   like this can be correlated or anything,\n",
      "[00:09:36.920 --> 00:09:38.640]   then the summation of the fluctuation\n",
      "[00:09:38.640 --> 00:09:40.880]   could be possibly something skilled with n.\n",
      "[00:09:40.880 --> 00:09:42.680]   Like for example, if everything is 1,\n",
      "[00:09:42.680 --> 00:09:44.840]   then the summation is just all the n.\n",
      "[00:09:44.840 --> 00:09:47.920]   But what we are seeing is exactly because this is like random\n",
      "[00:09:47.920 --> 00:09:49.120]   and it's independent.\n",
      "[00:09:49.120 --> 00:09:58.600]   So this kind of says the fluctuation or the noise\n",
      "[00:09:58.600 --> 00:10:00.520]   can cancel out each other.\n",
      "[00:10:00.520 --> 00:10:12.760]   To a very large extent, so that the summation will actually\n",
      "[00:10:12.760 --> 00:10:16.160]   be bounded by some square root of n curve.\n",
      "[00:10:16.160 --> 00:10:18.200]   Like this is square root of n curve.\n",
      "[00:10:18.200 --> 00:10:20.600]   So the growth of the summation of the noise\n",
      "[00:10:20.600 --> 00:10:24.400]   is like, we're always a square root of n instead of n.\n",
      "[00:10:24.400 --> 00:10:27.440]   So this is like what we claim about the concentration.\n",
      "[00:10:27.440 --> 00:10:32.120]   And eventually what we'll also say like the mean will converge,\n",
      "[00:10:32.120 --> 00:10:34.120]   like this average will converge to mean\n",
      "[00:10:34.120 --> 00:10:36.320]   at a 1 over square root of n rate.\n",
      "[00:10:36.320 --> 00:10:37.960]   So that is when angle to infinity,\n",
      "[00:10:37.960 --> 00:10:42.640]   we will actually get infinite close to the expectation,\n",
      "[00:10:42.640 --> 00:10:46.360]   which kind of recovers the law of large number in that sense.\n",
      "[00:10:46.360 --> 00:11:08.680]   Let me write this group in here.\n",
      "[00:11:08.680 --> 00:11:11.280]   So any questions about this picture so far?\n",
      "[00:11:11.280 --> 00:11:35.200]   OK, so if not we will start by talking about how we prove the theorem 1.\n",
      "[00:11:35.200 --> 00:11:38.920]   So essentially this proof of theorem 1\n",
      "[00:11:38.920 --> 00:11:43.560]   will be the most important part of the entire concentration in quality.\n",
      "[00:11:43.560 --> 00:11:47.720]   I think the remaining thing will won't go that extensively into the proof.\n",
      "[00:11:47.720 --> 00:11:50.680]   And we will just briefly talking about how to modify this proof.\n",
      "[00:11:50.680 --> 00:11:53.960]   So this is like the important thing.\n",
      "[00:11:53.960 --> 00:11:59.480]   And this probably last like we need to do three lemmas to prove this.\n",
      "[00:11:59.480 --> 00:12:06.480]   OK, so first lemmas is something like pretty straightforward.\n",
      "[00:12:06.480 --> 00:12:09.800]   You probably already learn this from the probability class.\n",
      "[00:12:09.800 --> 00:12:11.240]   That is Markov inequality.\n",
      "[00:12:11.240 --> 00:12:25.120]   It states that let's x be a non-active random variable.\n",
      "[00:12:25.120 --> 00:12:48.520]   Then for r a greater than 0, any scalar a greater than 0,\n",
      "[00:12:48.520 --> 00:12:54.600]   we have probability of x greater than a greater or equal to a,\n",
      "[00:12:54.600 --> 00:13:06.440]   less or equal to expectation of x divided by a.\n",
      "[00:13:06.440 --> 00:13:09.200]   OK, I think this is like very elementary.\n",
      "[00:13:09.200 --> 00:13:11.120]   You probably already learn this.\n",
      "[00:13:11.120 --> 00:13:18.080]   We'll just do a quick proof, not much to say.\n",
      "[00:13:18.080 --> 00:13:19.520]   This is like also a tail bound.\n",
      "[00:13:19.520 --> 00:13:21.480]   But it's much less than this one.\n",
      "[00:13:21.480 --> 00:13:27.600]   This is exponentially small, but this is just say I'm bounded by some expectation\n",
      "[00:13:27.600 --> 00:13:29.160]   and divided by a.\n",
      "[00:13:29.160 --> 00:13:37.480]   So the proof is essentially we say a note from the definition of expectation.\n",
      "[00:13:37.480 --> 00:13:54.720]   [INAUDIBLE]\n",
      "[00:13:54.720 --> 00:13:57.440]   We have expectation of x.\n",
      "[00:13:57.440 --> 00:14:02.520]   It's greater or equal to--\n",
      "[00:14:02.520 --> 00:14:07.720]   [INAUDIBLE]\n",
      "[00:14:07.720 --> 00:14:10.000]   So essentially, we have two parts.\n",
      "[00:14:10.000 --> 00:14:13.240]   One part is less than a.\n",
      "[00:14:13.240 --> 00:14:18.760]   The other part is x greater than a, greater or equal to a.\n",
      "[00:14:18.760 --> 00:14:21.160]   So what we are saying-- because expectation essentially\n",
      "[00:14:21.160 --> 00:14:28.000]   is integral of the density of x times x itself.\n",
      "[00:14:28.000 --> 00:14:31.440]   So what we can do is we can relax all the x\n",
      "[00:14:31.440 --> 00:14:35.720]   in this range to be 0, like we lower bounded by 0.\n",
      "[00:14:35.720 --> 00:14:40.040]   Because we know x is non-active, non-active random variable.\n",
      "[00:14:40.040 --> 00:14:42.600]   Well, in this range, because all x is greater than a,\n",
      "[00:14:42.600 --> 00:14:44.920]   so I lower bounded by a.\n",
      "[00:14:44.920 --> 00:14:46.160]   So I'm both range.\n",
      "[00:14:46.160 --> 00:14:48.960]   I'm doing lower bounds.\n",
      "[00:14:48.960 --> 00:14:54.240]   And because we know the first term is like 0, then we cross it out,\n",
      "[00:14:54.240 --> 00:14:56.480]   and it basically just we have the second term.\n",
      "[00:14:56.480 --> 00:15:01.120]   That is x greater than equal to a and times a, which essentially\n",
      "[00:15:01.120 --> 00:15:02.120]   will finish this proof.\n",
      "[00:15:02.120 --> 00:15:19.520]   So although this is also a tail bound,\n",
      "[00:15:19.520 --> 00:15:21.960]   but we kind of know this is not very sharp.\n",
      "[00:15:21.960 --> 00:15:24.800]   This is a decrease.\n",
      "[00:15:24.800 --> 00:15:27.360]   Offset STN decreases with E to the minus T squared.\n",
      "[00:15:27.360 --> 00:15:29.120]   But this is offset as A, but you're\n",
      "[00:15:29.120 --> 00:15:32.280]   kind of decreased with only one array or something like that.\n",
      "[00:15:32.280 --> 00:15:35.680]   So we'll essentially use some techniques\n",
      "[00:15:35.680 --> 00:15:39.440]   to make this inequality much sharper,\n",
      "[00:15:39.440 --> 00:15:42.880]   and eventually we're getting something like that.\n",
      "[00:15:42.880 --> 00:15:44.960]   So the remaining step will be essentially,\n",
      "[00:15:44.960 --> 00:15:46.160]   we'll do two things.\n",
      "[00:15:46.160 --> 00:15:53.560]   So the second step will essentially\n",
      "[00:15:53.560 --> 00:15:57.240]   say something like, we'll define a new kind of random variable\n",
      "[00:15:57.240 --> 00:15:59.560]   which is called like sub Gaussian random variable.\n",
      "[00:15:59.560 --> 00:16:07.320]   But we won't say too much.\n",
      "[00:16:07.320 --> 00:16:10.000]   Basically, this is defined by some abstract condition.\n",
      "[00:16:10.000 --> 00:16:12.640]   We will say this kind of random variable\n",
      "[00:16:12.640 --> 00:16:13.800]   have a good concentration.\n",
      "[00:16:13.800 --> 00:16:25.240]   And then the third step will actually\n",
      "[00:16:25.240 --> 00:16:32.360]   say the bounded random variable, which essentially\n",
      "[00:16:32.360 --> 00:16:34.480]   is the precondition of theorem 1, which\n",
      "[00:16:34.480 --> 00:16:37.120]   we say all the xi are bounded random variables.\n",
      "[00:16:37.120 --> 00:16:41.000]   So we say bounded random variables are sub Gaussians.\n",
      "[00:16:53.680 --> 00:16:55.720]   So we will essentially do the remaining proof\n",
      "[00:16:55.720 --> 00:16:58.040]   for splitting into two steps.\n",
      "[00:16:58.040 --> 00:17:00.120]   So we'll first use some abstract condition\n",
      "[00:17:00.120 --> 00:17:02.280]   to say, as long as the random variable\n",
      "[00:17:02.280 --> 00:17:05.160]   satisfies some abstract condition called like sub Gaussian,\n",
      "[00:17:05.160 --> 00:17:07.280]   and we will have good concentration.\n",
      "[00:17:07.280 --> 00:17:10.240]   And then the final step will be, say, bounded random variable\n",
      "[00:17:10.240 --> 00:17:13.200]   actually is a class of sub Gaussian random variable.\n",
      "[00:17:13.200 --> 00:17:15.800]   It actually satisfies this abstract condition.\n",
      "[00:17:15.800 --> 00:17:18.440]   So that's why the entire thing goes through.\n",
      "[00:17:21.920 --> 00:17:25.200]   So we'll directly go to the lemma 2, which\n",
      "[00:17:25.200 --> 00:17:26.920]   talk about the second thing.\n",
      "[00:17:26.920 --> 00:17:48.800]   We say let x1, xn, to xm be independent random variable\n",
      "[00:17:48.800 --> 00:18:03.800]   where for our real lambda in r, like any real number,\n",
      "[00:18:03.800 --> 00:18:09.720]   we have the-- this is called like a moment generating function.\n",
      "[00:18:09.720 --> 00:18:19.240]   The expectation of e to the lambda xi minus e xi\n",
      "[00:18:19.240 --> 00:18:24.960]   less or equal to e to the lambda square, sigma square,\n",
      "[00:18:24.960 --> 00:18:26.320]   sigma i square over 2.\n",
      "[00:18:26.320 --> 00:18:37.640]   Then we have the concentration inequality\n",
      "[00:18:37.640 --> 00:18:42.840]   that is for all t greater or equal to 0.\n",
      "[00:18:42.840 --> 00:18:55.840]   We have the probability of summation i from 1 to n, xi minus e\n",
      "[00:18:55.840 --> 00:18:56.680]   xi.\n",
      "[00:18:56.680 --> 00:19:00.920]   Again, this summation of the fluctuation, greater or equal\n",
      "[00:19:00.920 --> 00:19:06.840]   to t, is less or equal to e to the minus t\n",
      "[00:19:06.840 --> 00:19:13.080]   square over 2 summation i from 1 to n, sigma i square.\n",
      "[00:19:13.080 --> 00:19:26.720]   So this is essentially corresponding to the first statement.\n",
      "[00:19:26.720 --> 00:19:29.120]   And we can see the claim of this lemma.\n",
      "[00:19:29.120 --> 00:19:31.600]   Essentially corresponding to what do we want in a half-dins\n",
      "[00:19:31.600 --> 00:19:33.640]   concentration inequality, the only difference\n",
      "[00:19:33.640 --> 00:19:35.840]   is like this ri is different from sigma i.\n",
      "[00:19:35.840 --> 00:19:40.240]   And we have two on the top that have two at the bottom,\n",
      "[00:19:40.240 --> 00:19:41.360]   in the denominator.\n",
      "[00:19:41.360 --> 00:19:43.960]   But other than this constant difference, it's almost the same.\n",
      "[00:19:43.960 --> 00:19:46.920]   So essentially, we will handle this difference in 2\n",
      "[00:19:46.920 --> 00:19:50.040]   and in r and sigma in the next lemma.\n",
      "[00:19:50.040 --> 00:19:51.800]   But this lemma essentially says this already\n",
      "[00:19:51.800 --> 00:19:53.160]   has a concentration.\n",
      "[00:19:53.160 --> 00:19:54.520]   And by sub-gougean, we actually\n",
      "[00:19:54.520 --> 00:20:00.360]   means this condition is usually the definition\n",
      "[00:20:00.360 --> 00:20:02.800]   of sub-gougean random variables.\n",
      "[00:20:23.880 --> 00:20:26.880]   So in this class, because we mostly just\n",
      "[00:20:26.880 --> 00:20:29.640]   need to use boundy random variables,\n",
      "[00:20:29.640 --> 00:20:32.360]   so we won't go that deep in terms of talking\n",
      "[00:20:32.360 --> 00:20:33.960]   about why this is a causal sub-gougean.\n",
      "[00:20:33.960 --> 00:20:36.400]   But I will just give intuitive reason why\n",
      "[00:20:36.400 --> 00:20:38.880]   this is a causal sub-gougean.\n",
      "[00:20:38.880 --> 00:20:45.040]   I think the reason is essentially for the tail.\n",
      "[00:20:45.040 --> 00:20:52.080]   So for this, if any random variable satisfy this condition,\n",
      "[00:20:52.080 --> 00:20:53.320]   this is like intuition.\n",
      "[00:20:53.320 --> 00:20:54.800]   We don't do proof here.\n",
      "[00:20:54.800 --> 00:21:15.880]   If xi satisfy sub-gougean condition,\n",
      "[00:21:15.880 --> 00:21:21.400]   it means if you look at the distribution of this xi\n",
      "[00:21:21.400 --> 00:21:26.520]   and look at this tail, and it essentially means the tail\n",
      "[00:21:26.520 --> 00:21:34.280]   will decay, no slower than the Gaussian.\n",
      "[00:21:34.280 --> 00:21:50.560]   So it turns out we can formally prove this claim,\n",
      "[00:21:50.560 --> 00:21:55.760]   like, any random variable satisfy this in quality,\n",
      "[00:21:55.760 --> 00:21:58.480]   we can actually prove the tail will actually decay very fast.\n",
      "[00:21:58.480 --> 00:22:03.080]   It's equal or faster than Gaussian.\n",
      "[00:22:03.080 --> 00:22:06.000]   But we won't go to that deep in this class.\n",
      "[00:22:06.000 --> 00:22:09.120]   So this is just intuition why this is called sub-gougean.\n",
      "[00:22:09.120 --> 00:22:13.440]   But essentially, you should know Gaussian\n",
      "[00:22:13.440 --> 00:22:21.920]   or bounded random variable belongs to sub-gougean.\n",
      "[00:22:21.920 --> 00:22:29.040]   The reasoning is like tail of Gaussian\n",
      "[00:22:29.040 --> 00:22:31.520]   definitely like decay, no slower than Gaussian.\n",
      "[00:22:31.520 --> 00:22:34.200]   And tail of bounded variable is very good.\n",
      "[00:22:34.200 --> 00:22:36.720]   Because bounded variable essentially says like,\n",
      "[00:22:36.720 --> 00:22:38.840]   I am only supporting this region.\n",
      "[00:22:38.840 --> 00:22:41.040]   I have no probability outside this region.\n",
      "[00:22:41.040 --> 00:22:43.400]   So clearly, this tail decay is very fast.\n",
      "[00:22:43.400 --> 00:22:45.720]   So that's why bounded variable is also sub-gougean.\n",
      "[00:22:45.720 --> 00:22:49.320]   But here, we just stay abstractly with this condition.\n",
      "[00:22:49.320 --> 00:22:53.320]   Let's not go deep in terms of proving everything in that app.\n",
      "[00:22:53.320 --> 00:23:02.800]   Yes?\n",
      "[00:23:02.800 --> 00:23:08.200]   What are the x and y actions of the graph of the little ball?\n",
      "[00:23:08.200 --> 00:23:09.880]   X and what?\n",
      "[00:23:09.880 --> 00:23:12.520]   Y actions, what are the x and y?\n",
      "[00:23:12.520 --> 00:23:14.680]   Oh, yeah, that's very good.\n",
      "[00:23:14.680 --> 00:23:19.480]   So this is like n, or maybe i.\n",
      "[00:23:19.480 --> 00:23:20.520]   And this is a si.\n",
      "[00:23:20.520 --> 00:23:29.720]   Si is like the summation of all the fluctuations up to step i.\n",
      "[00:23:29.720 --> 00:23:35.680]   And this is like a number of variables you added.\n",
      "[00:23:35.680 --> 00:23:39.040]   And is the horizontal line mean-- a plus horizontal line\n",
      "[00:23:39.040 --> 00:23:42.640]   means the expectation of x.\n",
      "[00:23:42.640 --> 00:23:45.240]   No horizontal means the summation of xi\n",
      "[00:23:45.240 --> 00:23:47.440]   subtracted the expectation.\n",
      "[00:23:47.440 --> 00:23:48.960]   So that's why it's mean 0.\n",
      "[00:23:48.960 --> 00:23:57.240]   So you can think this is like a fluctuation at time step i.\n",
      "[00:23:57.240 --> 00:24:00.120]   And this si is just summation of the fluctuation\n",
      "[00:24:00.120 --> 00:24:02.880]   from the first step to the i step.\n",
      "[00:24:02.880 --> 00:24:05.400]   So that's why every step I'm doing some fluctuation.\n",
      "[00:24:05.400 --> 00:24:07.280]   But the fluctuation is independent,\n",
      "[00:24:07.280 --> 00:24:10.640]   so they cancel out with each other.\n",
      "[00:24:10.640 --> 00:24:15.000]   Also, the graph is xi, like n as si.\n",
      "[00:24:15.000 --> 00:24:17.040]   Si, yeah.\n",
      "[00:24:17.040 --> 00:24:18.680]   S is the summation.\n",
      "[00:24:18.680 --> 00:24:25.560]   So I think like each time this is like--\n",
      "[00:24:25.560 --> 00:24:30.240]   if we talk-- if we say this difference is could see i,\n",
      "[00:24:30.240 --> 00:24:32.400]   then this difference is like could see 1.\n",
      "[00:24:32.400 --> 00:24:34.360]   And this difference is could see 2.\n",
      "[00:24:34.360 --> 00:24:36.120]   So we're kind of sum them together.\n",
      "[00:24:36.120 --> 00:24:37.400]   And so that's why it's si.\n",
      "[00:24:37.400 --> 00:24:43.800]   Like this is because it's 3 or something like that.\n",
      "[00:24:43.800 --> 00:24:47.800]   So the solid bounded bounded lines are xi.\n",
      "[00:24:47.800 --> 00:24:50.880]   Bounded lines are like my upper bounds.\n",
      "[00:24:50.880 --> 00:24:53.000]   This is like I'm saying this will be less than--\n",
      "[00:24:53.000 --> 00:24:54.560]   less or equal to square root of n.\n",
      "[00:24:54.560 --> 00:24:56.560]   You're saying this one, right?\n",
      "[00:24:56.560 --> 00:25:00.920]   This is like square root of n.\n",
      "[00:25:00.920 --> 00:25:03.040]   Like linear in n is like something like that.\n",
      "[00:25:03.040 --> 00:25:07.320]   And square root of n is something like that.\n",
      "[00:25:07.320 --> 00:25:11.320]   Is this clear or is still confusing?\n",
      "[00:25:11.320 --> 00:25:14.160]   OK, sounds good.\n",
      "[00:25:14.160 --> 00:25:17.400]   We'll come back to this again, like when we talk about--\n",
      "[00:25:17.400 --> 00:25:20.040]   when we later talk about another concentration in a quadrant.\n",
      "[00:25:20.040 --> 00:25:29.400]   OK, so let's do the proof for this lemma.\n",
      "[00:25:29.400 --> 00:25:50.280]   So we start with what we want to prove.\n",
      "[00:25:50.280 --> 00:25:56.440]   That is probability of summation, i from 1 to n, xi minus e xi.\n",
      "[00:25:57.440 --> 00:26:00.960]   Greater or equal to t.\n",
      "[00:26:00.960 --> 00:26:09.440]   And the first thing we use is this e to the lambda x is monotonic.\n",
      "[00:26:09.440 --> 00:26:17.440]   For a lambda greater than 0.\n",
      "[00:26:17.440 --> 00:26:23.640]   So what we will do is like we just consider whatever\n",
      "[00:26:23.640 --> 00:26:26.080]   a lambda is greater or equal to 0.\n",
      "[00:26:26.080 --> 00:26:29.640]   And we raise everything to the exponential.\n",
      "[00:26:29.640 --> 00:26:33.280]   So we say this is equal to e to the lambda summation\n",
      "[00:26:33.280 --> 00:26:41.760]   of fluctuation, xi minus e xi, and greater or equal\n",
      "[00:26:41.760 --> 00:26:42.960]   to e to the lambda t.\n",
      "[00:26:42.960 --> 00:26:44.960]   [INAUDIBLE]\n",
      "[00:26:44.960 --> 00:26:46.960]   [INAUDIBLE]\n",
      "[00:26:46.960 --> 00:27:14.960]   [INAUDIBLE]\n",
      "[00:27:14.960 --> 00:27:18.440]   So the good thing about raising to the exponential\n",
      "[00:27:18.440 --> 00:27:21.800]   is now we can actually use the mark of inequality,\n",
      "[00:27:21.800 --> 00:27:25.840]   but we're still getting something exponentially decreased.\n",
      "[00:27:25.840 --> 00:27:27.600]   So this is the next step.\n",
      "[00:27:27.600 --> 00:27:29.120]   We use mark of inequality.\n",
      "[00:27:29.120 --> 00:27:42.040]   By mark of inequality on the top is the expectation of what\n",
      "[00:27:42.040 --> 00:27:42.920]   we have.\n",
      "[00:27:42.920 --> 00:27:51.160]   That is the e to the lambda summation, i from 1 to n, xi\n",
      "[00:27:51.160 --> 00:27:54.620]   minus e xi.\n",
      "[00:27:54.620 --> 00:27:57.520]   And in the denominator is what we have.\n",
      "[00:27:57.520 --> 00:27:58.800]   That is e to the lambda t.\n",
      "[00:27:58.800 --> 00:28:07.760]   Essentially, we just consider this whole thing as a new x.\n",
      "[00:28:07.760 --> 00:28:09.200]   This whole thing as a new a.\n",
      "[00:28:09.200 --> 00:28:11.320]   And we apply the mark of inequality.\n",
      "[00:28:11.320 --> 00:28:12.200]   And we get this bound.\n",
      "[00:28:12.200 --> 00:28:20.240]   So next step is the most important step here.\n",
      "[00:28:20.240 --> 00:28:23.040]   And you will see the reason why we want to consider this moment\n",
      "[00:28:23.040 --> 00:28:24.560]   generating function.\n",
      "[00:28:24.560 --> 00:28:28.280]   And the reason is we can essentially\n",
      "[00:28:28.280 --> 00:28:34.080]   make this equal to pi i from 1 to n.\n",
      "[00:28:34.080 --> 00:28:37.680]   That is a product of i from 1 to n.\n",
      "[00:28:37.680 --> 00:28:45.180]   Expectation of e to the lambda xi minus e xi.\n",
      "[00:28:45.180 --> 00:28:57.240]   I think we first need to say we have expectation outside\n",
      "[00:28:57.240 --> 00:28:59.800]   and e to the lambda t.\n",
      "[00:28:59.800 --> 00:29:02.840]   First step is we notice by the exponential we know the summation.\n",
      "[00:29:02.840 --> 00:29:05.920]   We can put summation outside to make it a product.\n",
      "[00:29:05.920 --> 00:29:07.200]   And the next thing we're going to use\n",
      "[00:29:07.200 --> 00:29:09.120]   is for independent x and y.\n",
      "[00:29:09.120 --> 00:29:21.240]   We actually notice that e x, y, the expectation of x times y\n",
      "[00:29:21.240 --> 00:29:25.440]   is equal to expectation of x times expectation of y.\n",
      "[00:29:25.440 --> 00:29:27.720]   This is the most important property of the independence\n",
      "[00:29:27.720 --> 00:29:29.000]   we're going to use.\n",
      "[00:29:29.000 --> 00:29:32.920]   So that's why we can actually push this expectation inside\n",
      "[00:29:32.920 --> 00:29:35.720]   the product, just because we use the independent.\n",
      "[00:29:36.680 --> 00:29:37.640]   Independence.\n",
      "[00:29:37.640 --> 00:29:39.680]   So we can push it inside using independence.\n",
      "[00:29:39.680 --> 00:29:52.720]   So once we push inside, the entire thing become very easy.\n",
      "[00:29:52.720 --> 00:29:57.920]   This just exactly corresponds to our precondition,\n",
      "[00:29:57.920 --> 00:30:00.280]   which is expectation of this moment generating function.\n",
      "[00:30:00.280 --> 00:30:01.920]   We say this is upper bounded by this.\n",
      "[00:30:04.560 --> 00:30:06.160]   So after we push it inside, we use\n",
      "[00:30:06.160 --> 00:30:09.640]   a precondition of the sub-gouchen random variable.\n",
      "[00:30:09.640 --> 00:30:13.600]   We say this is less than or equal to e to the minus lambda t,\n",
      "[00:30:13.600 --> 00:30:15.800]   which we copy from the denominator.\n",
      "[00:30:15.800 --> 00:30:34.480]   And then plus lambda square summation\n",
      "[00:30:34.480 --> 00:30:41.080]   sigma i squared, sigma i squared, i from 1 to n over 2.\n",
      "[00:30:41.080 --> 00:30:48.240]   This is like e to the everything.\n",
      "[00:30:48.240 --> 00:30:49.600]   Everything is on the exponent.\n",
      "[00:30:49.600 --> 00:31:03.120]   This is pretty straightforward.\n",
      "[00:31:03.120 --> 00:31:05.240]   So we're plugging the precondition\n",
      "[00:31:05.240 --> 00:31:06.440]   of sub-gouchen random variable.\n",
      "[00:31:32.400 --> 00:31:36.680]   So we start from what we want, and we already rich here.\n",
      "[00:31:36.680 --> 00:31:39.880]   The only thing we need to do is now we still\n",
      "[00:31:39.880 --> 00:31:41.640]   have additional lambda here.\n",
      "[00:31:41.640 --> 00:31:44.960]   But a good thing is that we can make lambda to be anything.\n",
      "[00:31:44.960 --> 00:31:48.360]   So that's why the final step is we just optimize over lambda,\n",
      "[00:31:48.360 --> 00:31:52.480]   and then we can get the tightest bound we want.\n",
      "[00:31:52.480 --> 00:31:56.000]   So to optimize over lambda, it's relatively straightforward.\n",
      "[00:31:56.000 --> 00:31:57.440]   I won't go into details, because this\n",
      "[00:31:57.440 --> 00:31:59.280]   is like a second order polynomial.\n",
      "[00:31:59.280 --> 00:32:02.760]   And we all know how to optimize over a second order polynomial.\n",
      "[00:32:02.760 --> 00:32:14.400]   So it turns out we can pick optimal lambda that\n",
      "[00:32:14.400 --> 00:32:23.040]   is equal to t over summation of sigma i squared, i from 1\n",
      "[00:32:23.040 --> 00:32:25.720]   to n, which corresponds to the lambda that\n",
      "[00:32:25.720 --> 00:32:27.680]   will minimize this probability.\n",
      "[00:32:27.680 --> 00:32:50.440]   So that we will have this p of i from 1 to n, xi minus xi,\n",
      "[00:32:50.440 --> 00:32:56.200]   greater or equal to t, it's less or equal to e\n",
      "[00:32:56.200 --> 00:33:01.760]   to the minus the lambda star t and the plus lambda star\n",
      "[00:33:01.760 --> 00:33:07.680]   square summation i from 1 to n sigma i square over 2,\n",
      "[00:33:07.680 --> 00:33:09.920]   and which you plug in and calculate out.\n",
      "[00:33:09.920 --> 00:33:17.800]   And it's exactly equal to e to the minus e to t square\n",
      "[00:33:17.800 --> 00:33:26.000]   over 2 summation, sigma i squared, and this finish the proof.\n",
      "[00:33:26.000 --> 00:33:29.000]   [SIDE CONVERSATION]\n",
      "[00:33:29.000 --> 00:33:32.000]   [SIDE CONVERSATION]\n",
      "[00:33:32.000 --> 00:33:34.000]   [SIDE CONVERSATION]\n",
      "[00:33:34.000 --> 00:33:36.000]   [SIDE CONVERSATION]\n",
      "[00:33:36.000 --> 00:34:05.000]   [SIDE CONVERSATION]\n",
      "[00:34:05.000 --> 00:34:08.240]   So now you have more idea of why we\n",
      "[00:34:08.240 --> 00:34:10.560]   want to make some precondition like that\n",
      "[00:34:10.560 --> 00:34:13.080]   and call this sub-gouchen.\n",
      "[00:34:13.080 --> 00:34:16.440]   And it's exactly because we want to use independence.\n",
      "[00:34:16.440 --> 00:34:18.240]   And the one we very easily use independence\n",
      "[00:34:18.240 --> 00:34:20.960]   is like to raise this summation to the exponent\n",
      "[00:34:20.960 --> 00:34:24.480]   so that we can make this summation to become a product.\n",
      "[00:34:24.480 --> 00:34:26.000]   And once we have the product structure,\n",
      "[00:34:26.000 --> 00:34:27.920]   we can use independence.\n",
      "[00:34:27.920 --> 00:34:29.720]   And then everything just follows through.\n",
      "[00:34:29.720 --> 00:34:31.400]   So that's why it's naturally just\n",
      "[00:34:31.400 --> 00:34:37.880]   to assume something expectation of this exponent.\n",
      "[00:34:37.880 --> 00:34:40.040]   And we just make this sub-sub-sub-gouchen.\n",
      "[00:34:40.040 --> 00:34:46.160]   So the final step is the lambda 3.\n",
      "[00:34:46.160 --> 00:34:54.200]   As we will say, we kind of assume this abstract condition.\n",
      "[00:34:54.200 --> 00:34:57.600]   And then we need to still verify that boundary random variable\n",
      "[00:34:57.600 --> 00:35:00.360]   actually satisfy this abstract condition.\n",
      "[00:35:00.360 --> 00:35:02.440]   This is what we will say here.\n",
      "[00:35:02.440 --> 00:35:18.960]   We say if x is a random variable, where x is in a range of a\n",
      "[00:35:18.960 --> 00:35:31.280]   to b with probability 1, and r equal to b minus a\n",
      "[00:35:31.280 --> 00:35:39.000]   is the range, the length of the interval.\n",
      "[00:35:39.000 --> 00:35:48.320]   Then we have the expectation of e to the lambda x minus x.\n",
      "[00:35:48.320 --> 00:35:50.720]   Essentially, this is like a moment in generating function.\n",
      "[00:35:50.720 --> 00:35:56.880]   We say this is less or equal to e to the lambda square r\n",
      "[00:35:56.880 --> 00:36:10.800]   square over 8 for any lambda in R.\n",
      "[00:36:10.800 --> 00:36:14.560]   This is our third claim that boundary random variable actually\n",
      "[00:36:14.560 --> 00:36:18.720]   satisfy the abstract condition, which we call sub-gouchen,\n",
      "[00:36:18.720 --> 00:36:24.480]   which precisely this sigma i is equal to r over 2,\n",
      "[00:36:24.480 --> 00:36:25.240]   something like that.\n",
      "[00:36:25.240 --> 00:36:51.920]   [INAUDIBLE]\n",
      "[00:36:51.920 --> 00:36:52.800]   Any questions so far?\n",
      "[00:36:53.720 --> 00:37:00.320]   Or if you have any questions about any step of derivation,\n",
      "[00:37:00.320 --> 00:37:03.880]   like you're not very clear, I can explain again.\n",
      "[00:37:03.880 --> 00:37:10.680]   So the final premise would be relatively straightforward.\n",
      "[00:37:10.680 --> 00:37:11.960]   It's about single random variable.\n",
      "[00:37:11.960 --> 00:37:16.440]   So there's not really much about the cancellation noise going on.\n",
      "[00:37:16.440 --> 00:37:20.040]   It's just essentially a bad proof for its computation,\n",
      "[00:37:20.040 --> 00:37:21.680]   because this is just a single random variable,\n",
      "[00:37:21.680 --> 00:37:26.440]   and you want to prove some expectation is less than some quantity.\n",
      "[00:37:26.440 --> 00:37:38.280]   So we'll just do the calculation that--\n",
      "[00:37:38.280 --> 00:37:41.560]   so for convenience and notation simplicity,\n",
      "[00:37:41.560 --> 00:37:46.960]   we define z is equal to the mean zero version of x.\n",
      "[00:37:46.960 --> 00:37:49.800]   So we define z equal to x minus ux.\n",
      "[00:37:49.800 --> 00:37:59.840]   So we also define a function, lambda, phi lambda--\n",
      "[00:37:59.840 --> 00:38:14.160]   phi psi lambda, is that phi psi lambda\n",
      "[00:38:14.160 --> 00:38:25.120]   is equal to the log of expectation e\n",
      "[00:38:25.120 --> 00:38:35.960]   to the power of lambda x minus ex,\n",
      "[00:38:35.960 --> 00:38:40.480]   which is equal to log e to the lambda z.\n",
      "[00:38:40.480 --> 00:38:47.400]   Essentially, this is a moment generating function,\n",
      "[00:38:47.400 --> 00:38:50.320]   but we take a log outside.\n",
      "[00:38:50.320 --> 00:38:56.600]   So it's a log moment generating function.\n",
      "[00:38:56.600 --> 00:38:59.600]   [INAUDIBLE]\n",
      "[00:38:59.600 --> 00:39:02.600]   [INAUDIBLE]\n",
      "[00:39:02.600 --> 00:39:29.600]   [INAUDIBLE]\n",
      "[00:39:29.600 --> 00:39:36.280]   So we can use the Taylor theorem or Taylor expansion.\n",
      "[00:39:36.280 --> 00:39:51.120]   We say this phi psi lambda is equal to psi zero plus\n",
      "[00:39:51.120 --> 00:39:55.520]   plus psi prime zero, lambda plus i prime zero,\n",
      "[00:39:55.520 --> 00:39:57.680]   and plus the second order approximation,\n",
      "[00:39:57.680 --> 00:40:00.960]   with Taylor expansion with second order remainder,\n",
      "[00:40:00.960 --> 00:40:05.200]   is lambda square over 2 times the second order of psi.\n",
      "[00:40:05.200 --> 00:40:07.040]   And the only difference is now it's not\n",
      "[00:40:07.040 --> 00:40:09.680]   no longer evaluated at zero, but evaluated at lambda prime.\n",
      "[00:40:09.680 --> 00:40:15.240]   So that lambda prime is something like in zero and the lambda,\n",
      "[00:40:15.240 --> 00:40:17.080]   some intermediate point.\n",
      "[00:40:24.240 --> 00:40:28.000]   So we know that this-- what do we want to bound essentially\n",
      "[00:40:28.000 --> 00:40:31.640]   is like this, per se, is just the log of objective.\n",
      "[00:40:31.640 --> 00:40:34.600]   So as long as we can have some upper bound on per se,\n",
      "[00:40:34.600 --> 00:40:36.720]   we already finished the lambda.\n",
      "[00:40:36.720 --> 00:40:38.880]   So what are we going to do next?\n",
      "[00:40:38.880 --> 00:40:41.520]   Essentially, we want to bound this per se, per se lambda.\n",
      "[00:40:42.080 --> 00:41:00.440]   [INAUDIBLE]\n",
      "[00:41:00.440 --> 00:41:02.680]   So to use this theorem, we need to calculate\n",
      "[00:41:02.680 --> 00:41:06.360]   what is like the gradient and the second order derivative\n",
      "[00:41:06.360 --> 00:41:07.560]   of this per se.\n",
      "[00:41:07.560 --> 00:41:10.320]   So I would just do a calculation for you.\n",
      "[00:41:10.320 --> 00:41:13.400]   And we start from per se of this,\n",
      "[00:41:13.400 --> 00:41:16.400]   and you take the derivative of over lambda.\n",
      "[00:41:16.400 --> 00:41:18.400]   And you can actually do the calculation,\n",
      "[00:41:18.400 --> 00:41:22.320]   which you see the gradient is equal to the expectation\n",
      "[00:41:22.320 --> 00:41:32.200]   of e to the lambda z times z over e to the lambda z.\n",
      "[00:41:32.200 --> 00:41:38.360]   This is the gradient, and this is second order derivative.\n",
      "[00:41:38.360 --> 00:41:46.080]   This is equal to expectation of e to the lambda z\n",
      "[00:41:46.080 --> 00:41:52.720]   divided by expectation of e to the lambda z, z square,\n",
      "[00:41:52.720 --> 00:41:57.440]   subtracted by expectation of e to the lambda z.\n",
      "[00:41:57.440 --> 00:42:02.440]   [INAUDIBLE]\n",
      "[00:42:02.440 --> 00:42:31.720]   Subtracted by-- this is like a front,\n",
      "[00:42:31.720 --> 00:42:39.320]   and subtracted by e to the lambda z over expectation\n",
      "[00:42:39.320 --> 00:42:47.560]   of e to the lambda z, z, this entire thing square.\n",
      "[00:42:47.560 --> 00:43:10.840]   So the reason we want to do this Taylor expansion is because,\n",
      "[00:43:10.840 --> 00:43:15.520]   first, we immediately notice this psi is zero.\n",
      "[00:43:15.520 --> 00:43:18.800]   It's just we plugging in lambda equal to zero,\n",
      "[00:43:18.800 --> 00:43:21.280]   which e to the zero is like 1.\n",
      "[00:43:21.280 --> 00:43:22.520]   So log of 1 is zero.\n",
      "[00:43:22.520 --> 00:43:26.200]   So precise zero is equal to zero, so the first term is gone.\n",
      "[00:43:26.200 --> 00:43:30.160]   And then the second term, precise prime zero,\n",
      "[00:43:30.160 --> 00:43:32.840]   is also again we put lambda equal to zero here,\n",
      "[00:43:32.840 --> 00:43:34.480]   and this become 1, this become 1.\n",
      "[00:43:34.480 --> 00:43:38.600]   It's just equal to expectation of z.\n",
      "[00:43:38.600 --> 00:43:41.920]   And we notice this z is defined to be mean zero.\n",
      "[00:43:41.920 --> 00:43:44.520]   z is defined to be x minus e to the x.\n",
      "[00:43:44.520 --> 00:43:45.980]   So this is again equal to zero.\n",
      "[00:43:45.980 --> 00:43:51.400]   So all we need to bound is the third term.\n",
      "[00:43:51.400 --> 00:43:54.920]   That is the second order derivative of this psi.\n",
      "[00:43:54.920 --> 00:44:06.920]   So in order to bound the second order derivative of psi,\n",
      "[00:44:06.920 --> 00:44:10.720]   we need to look at those terms, like those seemingly\n",
      "[00:44:10.720 --> 00:44:14.240]   very complicated terms.\n",
      "[00:44:14.240 --> 00:44:17.240]   So let's, for example, look at this term.\n",
      "[00:44:17.240 --> 00:44:18.840]   Let's call this term star.\n",
      "[00:44:18.840 --> 00:44:29.040]   So in the sense of, let's say, if it's continuous random variable,\n",
      "[00:44:29.040 --> 00:44:33.320]   so with a star is actually equal to the integral of we\n",
      "[00:44:33.320 --> 00:44:40.280]   have the density function of z times e to the lambda z\n",
      "[00:44:40.280 --> 00:44:45.080]   and divided by, again, the integral of the density of z\n",
      "[00:44:45.080 --> 00:44:50.800]   and e to the lambda z d z.\n",
      "[00:44:50.800 --> 00:44:55.040]   And we have additional z here, so it times z d z.\n",
      "[00:45:08.720 --> 00:45:11.560]   So one very smart observation is we can actually\n",
      "[00:45:11.560 --> 00:45:15.360]   bundle those things together.\n",
      "[00:45:15.360 --> 00:45:20.160]   We make this thing a new density, like a p tilde z.\n",
      "[00:45:20.160 --> 00:45:27.160]   So we know p tilde is also a density function.\n",
      "[00:45:27.160 --> 00:45:37.560]   Probability density.\n",
      "[00:45:38.560 --> 00:45:47.560]   Because two things, in order to check his probability,\n",
      "[00:45:47.560 --> 00:45:48.920]   we want to check two things.\n",
      "[00:45:48.920 --> 00:45:53.520]   One is p tilde z is greater than 0, greater or equal to 0\n",
      "[00:45:53.520 --> 00:45:55.720]   for any z.\n",
      "[00:45:55.720 --> 00:46:00.040]   This is obviously true, because it's density times something\n",
      "[00:46:00.040 --> 00:46:02.240]   non-active, so this is always true.\n",
      "[00:46:02.240 --> 00:46:09.160]   And the second thing, we need to do this integral over the density\n",
      "[00:46:09.160 --> 00:46:12.440]   and that's going to summation equal to 1.\n",
      "[00:46:12.440 --> 00:46:15.160]   And this is also very straightforward,\n",
      "[00:46:15.160 --> 00:46:17.960]   because if we define this as a p tilde,\n",
      "[00:46:17.960 --> 00:46:20.560]   and then when we do the integration,\n",
      "[00:46:20.560 --> 00:46:23.040]   we essentially just integrate in the numerator part.\n",
      "[00:46:23.040 --> 00:46:25.400]   And eventually, that's exactly equal to the denominator,\n",
      "[00:46:25.400 --> 00:46:26.720]   and so that's equal to 1.\n",
      "[00:46:26.720 --> 00:46:29.700]   [VIDEO PLAYBACK]\n",
      "[00:46:29.700 --> 00:46:32.680]   [END PLAYBACK]\n",
      "[00:46:32.680 --> 00:46:35.660]   [VIDEO PLAYBACK]\n",
      "[00:47:00.760 --> 00:47:07.440]   So this essentially means this p, this lambda, double prime,\n",
      "[00:47:07.440 --> 00:47:10.840]   this second derivative of per sine\n",
      "[00:47:10.840 --> 00:47:15.680]   is actually equal to the expectation.\n",
      "[00:47:15.680 --> 00:47:18.320]   You can also do the similar trick for the first term,\n",
      "[00:47:18.320 --> 00:47:22.800]   and it becomes some new expectation of z squared\n",
      "[00:47:22.800 --> 00:47:28.600]   divided by expectation of z squared.\n",
      "[00:47:28.600 --> 00:47:31.520]   This expectation is taking an expectation\n",
      "[00:47:31.520 --> 00:47:32.920]   under new distribution.\n",
      "[00:47:32.920 --> 00:47:45.820]   [END PLAYBACK]\n",
      "[00:47:45.820 --> 00:47:47.280]   [END PLAYBACK]\n",
      "[00:47:47.280 --> 00:48:10.120]   P tilde. OK.\n",
      "[00:48:10.120 --> 00:48:14.440]   And this just becomes-- you know, this is like the variance\n",
      "[00:48:14.440 --> 00:48:23.640]   now, this is just equal to e tilde z minus e tilde square.\n",
      "[00:48:23.640 --> 00:48:31.560]   I think it's pretty elementary.\n",
      "[00:48:31.560 --> 00:48:35.000]   You can expand it out, and then this is equal to this.\n",
      "[00:48:35.000 --> 00:48:54.200]   So now, the good thing about this is we know this is just\n",
      "[00:48:54.200 --> 00:48:56.720]   expectation of something bounded.\n",
      "[00:48:56.720 --> 00:48:59.400]   Because z is bounded, expectation of z is bounded,\n",
      "[00:48:59.400 --> 00:49:02.040]   so this difference is also bounded.\n",
      "[00:49:02.040 --> 00:49:03.800]   So we know very straightforwardly,\n",
      "[00:49:03.800 --> 00:49:10.160]   because z is in some range that is at most r.\n",
      "[00:49:10.160 --> 00:49:13.040]   So z minus e z is at most r.\n",
      "[00:49:13.040 --> 00:49:16.000]   So one obvious upper bound is like r squared.\n",
      "[00:49:22.280 --> 00:49:34.240]   But you can actually, by careful analysis,\n",
      "[00:49:34.240 --> 00:49:35.760]   you can actually do better.\n",
      "[00:49:35.760 --> 00:49:45.000]   You can actually do a titer bound,\n",
      "[00:49:45.000 --> 00:49:50.240]   which we will actually say this is less or equal to r\n",
      "[00:49:50.240 --> 00:49:52.360]   squared over 4.\n",
      "[00:49:52.360 --> 00:49:54.600]   So we can get a constant improvement.\n",
      "[00:49:54.600 --> 00:49:56.720]   So for most of this class, constant is a matter.\n",
      "[00:49:56.720 --> 00:49:58.440]   So we'll actually leave this to homework.\n",
      "[00:49:58.440 --> 00:50:23.480]   So we're more less down.\n",
      "[00:50:23.480 --> 00:50:26.960]   So just to recap what we have been doing so far,\n",
      "[00:50:26.960 --> 00:50:30.440]   we essentially say we start from the Poisson lambda,\n",
      "[00:50:30.440 --> 00:50:31.880]   and we do a titer expansion.\n",
      "[00:50:31.880 --> 00:50:33.880]   And the first term is 0, second term is 0.\n",
      "[00:50:33.880 --> 00:50:36.920]   And third term, we kind of already get the upper bounds.\n",
      "[00:50:36.920 --> 00:50:41.200]   So we say this is upper bounded by lambda squared divided\n",
      "[00:50:41.200 --> 00:50:44.080]   by 2 times r squared divided by 4,\n",
      "[00:50:44.080 --> 00:50:49.920]   which is lambda squared r squared over 8.\n",
      "[00:50:49.920 --> 00:50:53.880]   And we know this Poisson lambda is exactly the log\n",
      "[00:50:53.880 --> 00:50:55.280]   of what we want to upper bound.\n",
      "[00:50:55.280 --> 00:50:58.280]   So essentially, we just take exponential on both side\n",
      "[00:50:58.280 --> 00:50:59.480]   and we finish the proof.\n",
      "[00:51:24.760 --> 00:51:25.520]   Yes.\n",
      "[00:51:25.520 --> 00:51:26.760]   Any questions?\n",
      "[00:51:26.760 --> 00:51:31.160]   [INAUDIBLE]\n",
      "[00:51:31.160 --> 00:51:33.040]   Yeah, it's equal to 0.\n",
      "[00:51:33.040 --> 00:51:35.560]   [INAUDIBLE]\n",
      "[00:51:35.560 --> 00:51:38.280]   Because we define z to be the mean 0 version of x.\n",
      "[00:51:38.280 --> 00:51:45.000]   Yes?\n",
      "[00:51:45.000 --> 00:51:52.720]   [INAUDIBLE]\n",
      "[00:51:52.720 --> 00:51:54.840]   P tilde.\n",
      "[00:51:54.840 --> 00:51:56.160]   P tilde.\n",
      "[00:51:56.160 --> 00:51:57.640]   Yes.\n",
      "[00:51:57.640 --> 00:51:58.640]   Yes.\n",
      "[00:51:58.640 --> 00:52:08.760]   Yes.\n",
      "[00:52:08.760 --> 00:52:10.040]   I think this stuff essentially, we just\n",
      "[00:52:10.040 --> 00:52:13.680]   change the distribution from P to P tilde.\n",
      "[00:52:13.680 --> 00:52:14.680]   Yes.\n",
      "[00:52:14.680 --> 00:52:17.280]   [INAUDIBLE]\n",
      "[00:52:17.280 --> 00:52:22.640]   I think the reason is, as I said,\n",
      "[00:52:22.640 --> 00:52:27.880]   I think the third one is a bit about brute force calculation.\n",
      "[00:52:27.880 --> 00:52:30.000]   And we can see from the second lemma,\n",
      "[00:52:30.000 --> 00:52:31.520]   looking at some moment generating function\n",
      "[00:52:31.520 --> 00:52:35.200]   is very natural when we handle in this summation\n",
      "[00:52:35.200 --> 00:52:37.760]   of independent random variables.\n",
      "[00:52:37.760 --> 00:52:40.000]   Because I think last time I wrote here,\n",
      "[00:52:40.000 --> 00:52:43.600]   we essentially to use independent property,\n",
      "[00:52:43.600 --> 00:52:48.200]   we need to look at the product of a random variable\n",
      "[00:52:48.200 --> 00:52:50.760]   instead of a summation of random variable.\n",
      "[00:52:50.760 --> 00:52:53.280]   The summation of independent random variable, right?\n",
      "[00:52:53.280 --> 00:52:55.920]   So it's very natural to raise it to the exponent\n",
      "[00:52:55.920 --> 00:52:59.000]   so the summation become the product.\n",
      "[00:52:59.000 --> 00:53:01.720]   So that's why I found a second lemma is pretty easy\n",
      "[00:53:01.720 --> 00:53:05.120]   to see why we care about some quantity like that.\n",
      "[00:53:05.120 --> 00:53:07.080]   So the third one is essentially the bridge.\n",
      "[00:53:07.080 --> 00:53:09.440]   Essentially it says, I want to prove\n",
      "[00:53:09.440 --> 00:53:11.480]   the boundary random variable actually satisfy something\n",
      "[00:53:11.480 --> 00:53:13.920]   like this so that we finish the halving.\n",
      "[00:53:13.920 --> 00:53:27.160]   OK.\n",
      "[00:53:27.160 --> 00:53:29.840]   So all of this is to prove halving.\n",
      "[00:53:29.840 --> 00:53:32.400]   All of this is just to prove halving, yep.\n",
      "[00:53:32.400 --> 00:53:33.240]   That's good.\n",
      "[00:53:33.240 --> 00:53:41.800]   So eventually, I think the energy is\n",
      "[00:53:41.800 --> 00:53:44.000]   too much about the proof of halving in quality.\n",
      "[00:53:44.000 --> 00:53:56.000]   We're essentially preparing our lemma\n",
      "[00:53:56.000 --> 00:53:57.600]   to prove the halving in quality.\n",
      "[00:53:57.600 --> 00:53:59.600]   Essentially just the lemma 2 plus lemma 3.\n",
      "[00:53:59.600 --> 00:54:10.600]   Like lemma 2 says, sub-gautian random variable\n",
      "[00:54:10.600 --> 00:54:13.080]   has concentration, and lemma 3 says\n",
      "[00:54:13.080 --> 00:54:15.280]   the boundary random variable has sub-gautian.\n",
      "[00:54:15.280 --> 00:54:16.840]   So that's clear the boundary random variable\n",
      "[00:54:16.840 --> 00:54:19.560]   also have concentration, and that's finished to prove.\n",
      "[00:54:19.560 --> 00:54:34.720]   In case you feel like this proof is pretty complicated\n",
      "[00:54:34.720 --> 00:54:39.480]   and you cannot very follow, it's OK.\n",
      "[00:54:39.480 --> 00:54:40.960]   We can go back to take a look.\n",
      "[00:54:40.960 --> 00:54:43.920]   And on the other hand, when we continue the lecture,\n",
      "[00:54:43.920 --> 00:54:48.600]   we don't really need this proof.\n",
      "[00:54:48.600 --> 00:54:51.200]   All we need is we want to use the halving in quality.\n",
      "[00:54:51.200 --> 00:54:54.720]   So the theorem statement and the query\n",
      "[00:54:54.720 --> 00:54:56.240]   is the most important part.\n",
      "[00:54:56.240 --> 00:54:58.000]   We would repeatedly use it when we're\n",
      "[00:54:58.000 --> 00:54:59.480]   doing the learning scenario.\n",
      "[00:54:59.480 --> 00:55:02.240]   And this is just to backen up why some concentration\n",
      "[00:55:02.240 --> 00:55:04.000]   inquiry like that is true.\n",
      "[00:55:04.000 --> 00:55:24.320]   OK.\n",
      "[00:55:24.320 --> 00:55:25.800]   So, yes.\n",
      "[00:55:25.800 --> 00:55:33.240]   [INAUDIBLE]\n",
      "[00:55:33.240 --> 00:55:35.240]   Sorry, could you say again?\n",
      "[00:55:35.240 --> 00:55:38.960]   [INAUDIBLE]\n",
      "[00:55:38.960 --> 00:55:42.040]   The second lemma is what word?\n",
      "[00:55:42.040 --> 00:55:43.800]   [INAUDIBLE]\n",
      "[00:55:43.800 --> 00:55:45.640]   [INAUDIBLE]\n",
      "[00:55:45.640 --> 00:55:47.440]   [INAUDIBLE]\n",
      "[00:55:47.440 --> 00:55:49.400]   [INAUDIBLE]\n",
      "[00:55:49.400 --> 00:55:51.480]   The function of precise convex.\n",
      "[00:55:51.480 --> 00:55:59.360]   [INAUDIBLE]\n",
      "[00:55:59.360 --> 00:56:01.960]   Second order iterative is always greater or equal to 0.\n",
      "[00:56:01.960 --> 00:56:04.920]   That is true, yes.\n",
      "[00:56:04.920 --> 00:56:05.880]   Yeah, yeah.\n",
      "[00:56:05.880 --> 00:56:07.960]   That's a good comment here.\n",
      "[00:56:07.960 --> 00:56:09.320]   Yeah, thank you.\n",
      "[00:56:09.320 --> 00:56:17.440]   [INAUDIBLE]\n",
      "[00:56:17.440 --> 00:56:22.320]   Yes, I think the question is why the first two terms are\n",
      "[00:56:22.320 --> 00:56:25.160]   at 0 and third term is at some intermediate point.\n",
      "[00:56:25.160 --> 00:56:27.560]   I think this is essentially the--\n",
      "[00:56:27.560 --> 00:56:31.560]   in the calculus, when you do the Taylor expansion,\n",
      "[00:56:31.560 --> 00:56:34.520]   typically you need to expand to the infinite order, right?\n",
      "[00:56:34.520 --> 00:56:36.560]   It's not like you truncate at a second order.\n",
      "[00:56:36.560 --> 00:56:38.080]   You need to have third order, fourth order,\n",
      "[00:56:38.080 --> 00:56:40.240]   and something like that, then you can expand at 0.\n",
      "[00:56:40.240 --> 00:56:41.640]   So what do we use here is that we\n",
      "[00:56:41.640 --> 00:56:44.040]   don't want to do the calculation for the third order,\n",
      "[00:56:44.040 --> 00:56:45.920]   fourth order, that's like too much.\n",
      "[00:56:45.920 --> 00:56:49.080]   So we use a second order, remain the reversion of Taylor.\n",
      "[00:56:49.080 --> 00:56:51.760]   It's saying like if you truncate at a second order\n",
      "[00:56:51.760 --> 00:56:53.560]   and you don't look at the third order,\n",
      "[00:56:53.560 --> 00:56:57.280]   then the last term is no longer center at 0,\n",
      "[00:56:57.280 --> 00:57:02.160]   but to center some intermediate point of 0 and it can be anything.\n",
      "[00:57:02.160 --> 00:57:03.600]   [INAUDIBLE]\n",
      "[00:57:03.600 --> 00:57:04.600]   Yeah, yeah.\n",
      "[00:57:04.600 --> 00:57:17.440]   OK.\n",
      "[00:57:17.440 --> 00:57:20.040]   So if you know for the question about hot things in quality,\n",
      "[00:57:20.040 --> 00:57:23.440]   we will just briefly talk about the second concentration\n",
      "[00:57:23.440 --> 00:57:26.520]   in a quality and we'll finish the random variable part,\n",
      "[00:57:26.520 --> 00:57:28.800]   independent random variable part.\n",
      "[00:57:28.800 --> 00:57:32.440]   So the theorem 2 is so-called a brain-stance in quality.\n",
      "[00:57:32.440 --> 00:57:48.280]   So this is still an independent random variable,\n",
      "[00:57:48.280 --> 00:57:52.320]   but I think sometimes in a lecture and homework,\n",
      "[00:57:52.320 --> 00:57:54.920]   we will use this in quality, because this in quality\n",
      "[00:57:54.920 --> 00:57:58.640]   in some scenario is actually tighter than hot things in quality.\n",
      "[00:57:58.640 --> 00:58:03.720]   So let's again, let xi be independent random variable.\n",
      "[00:58:18.040 --> 00:58:28.000]   And with probability 1, we have, again, its bounded-- let's\n",
      "[00:58:28.000 --> 00:58:33.280]   say this difference is bounded by R. Let's not\n",
      "[00:58:33.280 --> 00:58:35.360]   care about the constants.\n",
      "[00:58:35.360 --> 00:58:38.280]   And I think the only difference is in a brain-stance in quality,\n",
      "[00:58:38.280 --> 00:58:41.680]   we have one additional condition, which can be stronger.\n",
      "[00:58:41.680 --> 00:58:45.400]   We actually say the variance of each xi\n",
      "[00:58:45.400 --> 00:58:59.120]   is bounded by sigma i squared, then for R t greater than 0.\n",
      "[00:58:59.120 --> 00:59:01.000]   We, again, have the concentration in quality.\n",
      "[00:59:01.000 --> 00:59:13.760]   That is the probability of the summation of the fluctuation,\n",
      "[00:59:13.760 --> 00:59:21.600]   xi minus u xi, greater or equal to t,\n",
      "[00:59:21.600 --> 00:59:28.400]   is less or equal to e to the minus t square,\n",
      "[00:59:28.400 --> 00:59:34.440]   2 times the summation of i from 1 to n, sigma i squared,\n",
      "[00:59:34.440 --> 00:59:36.920]   plus 2 over 3 R t.\n",
      "[00:59:37.680 --> 00:59:47.000]   So no longer do the proof, for instance, in quality\n",
      "[00:59:47.000 --> 00:59:48.480]   is a little bit involved.\n",
      "[00:59:48.480 --> 00:59:51.040]   But I think this constants only requires\n",
      "[00:59:51.040 --> 00:59:54.480]   you to be able to use this kind of concentration in quality.\n",
      "[00:59:54.480 --> 01:00:02.360]   So we can think this is like a slightly stronger condition,\n",
      "[01:00:02.360 --> 01:00:05.520]   which says it's not only a random variable with bounded radius R,\n",
      "[01:00:05.520 --> 01:00:08.560]   but it also has some variance bound, sigma i.\n",
      "[01:00:08.560 --> 01:00:10.560]   And potentially, the brain-stance will\n",
      "[01:00:10.560 --> 01:00:16.240]   be much sharper if the sigma i is much smaller than this R.\n",
      "[01:00:16.240 --> 01:00:18.560]   In that case, we'll have concentration in quality,\n",
      "[01:00:18.560 --> 01:00:20.080]   which basically, everything is same,\n",
      "[01:00:20.080 --> 01:00:23.280]   except that the denominator is slightly modified.\n",
      "[01:00:23.280 --> 01:00:30.200]   So it will be much easier to see what\n",
      "[01:00:30.200 --> 01:00:33.840]   is the more modification in the other term.\n",
      "[01:00:33.840 --> 01:00:40.680]   So we can, again, make this to be lambda, to be delta.\n",
      "[01:00:40.680 --> 01:00:42.960]   So the brain-stance in quality is, again, equivalent\n",
      "[01:00:42.960 --> 01:00:47.320]   to with probability at least 1 minus delta.\n",
      "[01:00:47.320 --> 01:00:53.080]   We will have the following.\n",
      "[01:00:53.080 --> 01:01:01.480]   That is summation i from 1 to n, xi minus e xi.\n",
      "[01:01:01.800 --> 01:01:07.180]   It's less or equal to some constant,\n",
      "[01:01:07.180 --> 01:01:18.160]   c times summation of sigma i square, i from 1 to n, log 1\n",
      "[01:01:18.160 --> 01:01:25.200]   over delta, plus R times log 1 over delta.\n",
      "[01:01:29.960 --> 01:01:33.160]   Again, this is just some change of variables.\n",
      "[01:01:33.160 --> 01:01:38.400]   You can make this lambda, and you can express t in terms\n",
      "[01:01:38.400 --> 01:01:40.400]   of-- you can express t in terms of delta,\n",
      "[01:01:40.400 --> 01:01:42.200]   and you will get this bound.\n",
      "[01:01:42.200 --> 01:01:44.180]   This is a Bernstein concentration in quality.\n",
      "[01:01:44.180 --> 01:01:50.320]   This is Bernstein.\n",
      "[01:01:50.320 --> 01:01:53.440]   And if you still remember, the half-dings concentration\n",
      "[01:01:53.440 --> 01:01:56.560]   in quality tells you everything is the same,\n",
      "[01:01:56.560 --> 01:01:58.480]   but the half-dings in quality on the right hand side\n",
      "[01:01:58.480 --> 01:02:01.520]   is something like this, something like summation\n",
      "[01:02:01.520 --> 01:02:08.240]   of i from 1 to n, i square, log 1 over delta.\n",
      "[01:02:08.240 --> 01:02:18.520]   Yes?\n",
      "[01:02:18.520 --> 01:02:19.400]   [INAUDIBLE]\n",
      "[01:02:19.400 --> 01:02:23.520]   C is just some constant, like 10 or something, absolute constant.\n",
      "[01:02:23.520 --> 01:02:30.400]   Yeah, I think a lot of times we don't really\n",
      "[01:02:30.400 --> 01:02:31.800]   care about the exact constant.\n",
      "[01:02:31.800 --> 01:02:35.760]   So we'll just look at how everything is scaled with n,\n",
      "[01:02:35.760 --> 01:02:37.040]   how everything is good.\n",
      "[01:02:37.040 --> 01:02:49.000]   So we can already see the major term of those two things,\n",
      "[01:02:49.000 --> 01:02:50.760]   except this is like a low-order term.\n",
      "[01:02:50.760 --> 01:02:52.920]   The major term is this term and this term,\n",
      "[01:02:52.920 --> 01:02:55.560]   because this term will grow with square root of n.\n",
      "[01:02:55.560 --> 01:02:57.360]   This is like summation of 1 to n,\n",
      "[01:02:57.360 --> 01:02:59.560]   so this is like something scaled with n.\n",
      "[01:02:59.560 --> 01:03:01.640]   So we'll take square root of this square root of n.\n",
      "[01:03:01.640 --> 01:03:03.720]   And this is also square root of n term.\n",
      "[01:03:03.720 --> 01:03:05.720]   So we will see the Bernstein square root of n term\n",
      "[01:03:05.720 --> 01:03:07.600]   actually only scaled with a variance.\n",
      "[01:03:07.600 --> 01:03:10.280]   Well, the half-dings like a square root of n term\n",
      "[01:03:10.280 --> 01:03:12.280]   scale with the entire bound.\n",
      "[01:03:12.280 --> 01:03:15.520]   So when the variance is much smaller than the bound,\n",
      "[01:03:15.520 --> 01:03:18.640]   the Bernstein will be sharper.\n",
      "[01:03:18.640 --> 01:03:21.000]   So we can take a look at one example.\n",
      "[01:03:21.000 --> 01:03:23.280]   Quick, quick look at one example.\n",
      "[01:03:23.280 --> 01:03:26.260]   [VIDEO PLAYBACK]\n",
      "[01:03:26.260 --> 01:03:29.240]   [END PLAYBACK]\n",
      "[01:03:29.240 --> 01:03:32.240]   [END PLAYBACK]\n",
      "[01:03:32.240 --> 01:03:35.240]   [END PLAYBACK]\n",
      "[01:03:35.240 --> 01:03:38.240]   [END PLAYBACK]\n",
      "[01:03:38.240 --> 01:03:41.240]   [END PLAYBACK]\n",
      "[01:04:01.240 --> 01:04:03.840]   Example is like unfair coin toss.\n",
      "[01:04:03.840 --> 01:04:14.680]   So we just say for each xi is sampled from Bernoulli p.\n",
      "[01:04:14.680 --> 01:04:25.600]   That is the waste probability of p, which essentially says\n",
      "[01:04:25.600 --> 01:04:28.800]   that's equal to 1 with probability p.\n",
      "[01:04:28.800 --> 01:04:32.780]   And equal to 0 with probability 1 minus p.\n",
      "[01:04:32.780 --> 01:04:43.000]   So in this case, we can see that the bound on x-axis bounded,\n",
      "[01:04:43.000 --> 01:04:46.600]   but the range radius is r is equal to 1.\n",
      "[01:04:46.600 --> 01:04:50.680]   But we can also compute the expectation and the variance.\n",
      "[01:04:50.680 --> 01:04:53.320]   So the expectation is only equal to p.\n",
      "[01:04:53.320 --> 01:05:01.480]   And the variance of x is equal to something p times 1 minus p.\n",
      "[01:05:01.480 --> 01:05:03.960]   And it's upper bounded by p.\n",
      "[01:05:03.960 --> 01:05:05.520]   We can also say it's upper bounded by p.\n",
      "[01:05:05.520 --> 01:05:16.920]   So in this case, in half density and quality,\n",
      "[01:05:16.920 --> 01:05:20.600]   we can only say something like 1 over square root of n,\n",
      "[01:05:20.600 --> 01:05:22.800]   a 1 over n average.\n",
      "[01:05:22.800 --> 01:05:30.960]   The difference between average and its expectation\n",
      "[01:05:30.960 --> 01:05:34.640]   is upper bounded by this term divided by n.\n",
      "[01:05:34.640 --> 01:05:37.120]   Because this is like a summation of the fluctuation.\n",
      "[01:05:37.120 --> 01:05:40.160]   What we look at there is the average fluctuation.\n",
      "[01:05:40.160 --> 01:05:42.320]   So everything is divided by n.\n",
      "[01:05:42.320 --> 01:05:46.600]   And we plug in this r equal to 1.\n",
      "[01:05:46.600 --> 01:05:55.160]   So what we get is something like C times log 1 over delta over n.\n",
      "[01:05:55.160 --> 01:05:57.800]   This is what we get from half density and quality.\n",
      "[01:05:57.800 --> 01:06:06.800]   But by breathing in quality, we can actually plug in the variance\n",
      "[01:06:06.800 --> 01:06:09.880]   is equal to p here.\n",
      "[01:06:09.880 --> 01:06:12.160]   And we just divide it by everything by n.\n",
      "[01:06:12.160 --> 01:06:17.240]   So we actually get something like 1 over n summation i from 1\n",
      "[01:06:17.240 --> 01:06:20.760]   to n xi, subtract by p.\n",
      "[01:06:20.760 --> 01:06:30.360]   It's less or equal to C times square root of p times log 1\n",
      "[01:06:30.360 --> 01:06:39.040]   over delta over n plus log 1 over delta over n.\n",
      "[01:06:39.040 --> 01:07:01.040]   [INAUDIBLE]\n",
      "[01:07:01.040 --> 01:07:03.820]   We know the second term is the lower order term.\n",
      "[01:07:04.620 --> 01:07:12.060]   Well, the first term is the leading order term.\n",
      "[01:07:12.060 --> 01:07:23.140]   And if we compare the leading order term of the two concentration\n",
      "[01:07:23.140 --> 01:07:27.100]   in quality, we notice the variance in quality has additional p there.\n",
      "[01:07:27.100 --> 01:07:33.740]   So this kind of concludes that if p much less than 1,\n",
      "[01:07:33.740 --> 01:07:39.100]   then the variance in quality is a lot better than half thing.\n",
      "[01:07:39.100 --> 01:07:42.100]   [INAUDIBLE]\n",
      "[01:07:42.100 --> 01:07:47.100]   [INAUDIBLE]\n",
      "[01:07:47.100 --> 01:07:52.100]   [INAUDIBLE]\n",
      "[01:07:52.100 --> 01:07:58.100]   [INAUDIBLE]\n",
      "[01:07:58.100 --> 01:08:03.100]   [INAUDIBLE]\n",
      "[01:08:03.100 --> 01:08:08.100]   [INAUDIBLE]\n",
      "[01:08:08.100 --> 01:08:13.100]   [INAUDIBLE]\n",
      "[01:08:13.100 --> 01:08:18.100]   [INAUDIBLE]\n",
      "[01:08:18.100 --> 01:08:23.100]   [INAUDIBLE]\n",
      "[01:08:23.100 --> 01:08:28.100]   [INAUDIBLE]\n",
      "[01:08:28.100 --> 01:08:47.100]   [INAUDIBLE]\n",
      "[01:08:47.100 --> 01:08:52.100]   OK, any question about the same in quality or this comparison?\n",
      "[01:08:52.100 --> 01:09:06.100]   [INAUDIBLE]\n",
      "[01:09:06.100 --> 01:09:12.100]   So if not, I think finally I want to talk a little bit more about another extension,\n",
      "[01:09:12.100 --> 01:09:16.100]   like not only being sharper, but like how we're going to relax the independent condition.\n",
      "[01:09:16.100 --> 01:09:20.100]   So this is what we're going to use eventually in the reinforcement setting.\n",
      "[01:09:20.100 --> 01:09:33.100]   That is concentration in quality for martingale difference or for martingale.\n",
      "[01:09:33.100 --> 01:09:43.100]   [INAUDIBLE]\n",
      "[01:09:43.100 --> 01:09:49.100]   So don't be scared by the words, and eventually you will feel this concept is pretty intuitive.\n",
      "[01:09:49.100 --> 01:09:56.100]   And the concentration in quality we derived for martingale will be almost identical to the concentration in quality we got from independence.\n",
      "[01:09:56.100 --> 01:10:00.100]   So you can think this is like a generalization of independence.\n",
      "[01:10:00.100 --> 01:10:13.100]   [INAUDIBLE]\n",
      "[01:10:13.100 --> 01:10:22.100]   So we'll first talk about the intuition of what kind of thing is like a martingale and why and something is not independent but a martingale.\n",
      "[01:10:22.100 --> 01:10:32.100]   So I think the intuition about martingale is for example, we have like C1, C2, C2, Dada, C3, N.\n",
      "[01:10:32.100 --> 01:10:39.100]   So independent really just says the randomness here and randomness here is completely independent.\n",
      "[01:10:39.100 --> 01:10:41.100]   They are not correlated.\n",
      "[01:10:41.100 --> 01:10:49.100]   Well, this kind of concentration is a lot easier, but a lot of times in sequential decision making reinforcement learning,\n",
      "[01:10:49.100 --> 01:10:53.100]   this is really a luxury. I think a lot of times it doesn't hold.\n",
      "[01:10:53.100 --> 01:10:57.100]   So that's why we want something like generalization of independence.\n",
      "[01:10:57.100 --> 01:11:05.100]   We want something weaker than independence so that it holds in the sequential decision making, but somehow still have good concentration.\n",
      "[01:11:05.100 --> 01:11:18.100]   So intuitively, the martingale is something like if we want to look at again summation of this cosii,\n",
      "[01:11:18.100 --> 01:11:31.100]   and each cosii that is no longer independent, but we have some condition like this.\n",
      "[01:11:31.100 --> 01:11:39.100]   That is the expectation of cosii conditioned on everything in the past.\n",
      "[01:11:39.100 --> 01:11:45.100]   So this is a little bit abstract, but you can think of one very easy scenario of the past.\n",
      "[01:11:45.100 --> 01:11:50.100]   It's basically just cosii minus 1, till cosii 1.\n",
      "[01:11:50.100 --> 01:11:53.100]   So this is everything that happened in the past.\n",
      "[01:11:53.100 --> 01:11:59.100]   So we're saying conditional everything in the past, like for example, this equal to 1, this equal to minus 1, this equal to something else.\n",
      "[01:11:59.100 --> 01:12:07.100]   Like whatever condition on this, and I have this equal to 0.\n",
      "[01:12:07.100 --> 01:12:16.100]   Then we will call epsilon i, then this epsilon i is called like this sequence of epsilon i.\n",
      "[01:12:16.100 --> 01:12:19.100]   It's called the martingale difference sequence.\n",
      "[01:12:32.100 --> 01:12:41.100]   So we will come to the rigorous definition later, but I want to first talk about the intuitive understanding of what is the martingale.\n",
      "[01:12:41.100 --> 01:12:51.100]   Martingale essentially is saying I no longer care about whether each cosii is independent, but I care about when I condition on what happened in the past,\n",
      "[01:12:51.100 --> 01:12:56.100]   my new randomness, cosii, is mean 0. This is very important to me.\n",
      "[01:12:56.100 --> 01:13:03.100]   So let's look at an example to see what is the difference between independence of this and this martingale kind of thing.\n",
      "[01:13:03.100 --> 01:13:15.100]   So for example, we can look at a sequence of length 2, like n equal to 2, which is like to showcase what is the difference.\n",
      "[01:13:15.100 --> 01:13:25.100]   So let's say we have two, in the length of two cases, we have like two random variables, that is cosii 1 and cosii 2.\n",
      "[01:13:25.100 --> 01:13:30.100]   So let's see, cosii 1 takes two values, that is minus 1 and 1.\n",
      "[01:13:30.100 --> 01:13:37.100]   And cosii 2 takes three values, that is minus 1, 0 and 1.\n",
      "[01:13:37.100 --> 01:13:46.100]   So the joint probability, or maybe let's not talk about joint probability, let's talk about the conditional probability of p,\n",
      "[01:13:46.100 --> 01:14:06.100]   cosii 2, conditional and cosii 1. This is the table for conditional probability, that is 0.5, 0, 0.5 and 0.10.\n",
      "[01:14:06.100 --> 01:14:26.100]   So we clearly know, this p, cosii 2, conditional and cosii 1 equal to 1, is not equal to p, cosii 2, conditional and cosii 1 equal to minus 1.\n",
      "[01:14:26.100 --> 01:14:42.100]   Because one distribution is 0.5, 0, 0.5, the other distribution is 0, 1, 0.\n",
      "[01:14:42.100 --> 01:14:51.100]   So this clearly says, cosii 1 is not independent of cosii 2.\n",
      "[01:14:51.100 --> 01:14:58.100]   Because conditional, different value of cosii, my distribution of cosii 2 actually can be different.\n",
      "[01:14:58.100 --> 01:15:01.100]   So they are not no longer independent.\n",
      "[01:15:01.100 --> 01:15:11.100]   But the good thing about that is, we can look at the expectation of cosii 2, conditional and cosii 1 equal to 1.\n",
      "[01:15:11.100 --> 01:15:18.100]   And we can also look at the expectation of cosii 2, conditional and cosii 1 equal to minus 1.\n",
      "[01:15:18.100 --> 01:15:25.100]   If you look at both of them, they are equal to 0.\n",
      "[01:15:25.100 --> 01:15:31.100]   This essentially corresponds to this equation that says, if you look at the expectation of cosii 2,\n",
      "[01:15:31.100 --> 01:15:41.100]   no matter what happens in the past, this is equal to 0, which corresponds to this exactly this 2 equality.\n",
      "[01:15:41.100 --> 01:15:52.100]   So this somehow says, this cosii 1 and the cosii 2 is actually a martingale difference sequence.\n",
      "[01:15:52.100 --> 01:15:56.100]   [CLAPPING]\n",
      "[01:15:56.100 --> 01:15:59.100]   [CLAPPING]\n",
      "[01:15:59.100 --> 01:16:04.100]   [CLAPPING]\n",
      "[01:16:04.100 --> 01:16:33.100]   [CLAPPING]\n",
      "[01:16:33.100 --> 01:16:47.100]   So in the previous picture of concentration in the independent variable case, let's say, for example,\n",
      "[01:16:47.100 --> 01:17:00.100]   if we look at the summation of i from 1 to n, cosii, and those are independent,\n",
      "[01:17:00.100 --> 01:17:06.100]   and let's say the expectation of cosii is equal to 0.\n",
      "[01:17:06.100 --> 01:17:10.100]   Let's just say this is like a mean 0 random variable and it's independent.\n",
      "[01:17:10.100 --> 01:17:16.100]   We already have the picture that essentially if we track the summation, it becomes a random word.\n",
      "[01:17:16.100 --> 01:17:20.100]   So first step, we're going to go up by cosii 1.\n",
      "[01:17:20.100 --> 01:17:25.100]   And the second step, we're going to probably go down or go up by cosii 2.\n",
      "[01:17:25.100 --> 01:17:31.100]   And then third step, we're going to go up or go down by cosii 3, and then fourth step by cosii 4.\n",
      "[01:17:31.100 --> 01:17:35.100]   But we will see because they're independent, so a lot of noise is going to cancel out,\n",
      "[01:17:35.100 --> 01:17:39.100]   and eventually just become some random work.\n",
      "[01:17:39.100 --> 01:17:42.100]   And we say the concentration in quality kind of exists.\n",
      "[01:17:42.100 --> 01:17:50.100]   This random work, if you look at the summation, is bounded by a square root of n with high probability.\n",
      "[01:17:50.100 --> 01:17:55.100]   So what are we going to say in a particle concentration in a quality is\n",
      "[01:17:55.100 --> 01:17:58.100]   we actually no longer need this independence.\n",
      "[01:17:58.100 --> 01:18:07.100]   We can replace this independence by a martingale difference to be martingale.\n",
      "[01:18:07.100 --> 01:18:11.100]   The only difference about this independence in martingale is now, for example,\n",
      "[01:18:11.100 --> 01:18:14.100]   once your lecture is looked like this, and we end up here.\n",
      "[01:18:14.100 --> 01:18:18.100]   This is like a cosii n.\n",
      "[01:18:18.100 --> 01:18:20.100]   So, I forget about a cosii key.\n",
      "[01:18:20.100 --> 01:18:23.100]   And some other trajectory goes up here, and then we end up here.\n",
      "[01:18:23.100 --> 01:18:25.100]   This is like a cosii key.\n",
      "[01:18:25.100 --> 01:18:27.100]   This is like a second trajectory.\n",
      "[01:18:27.100 --> 01:18:34.100]   So what we are seeing now, they are not independent because this distribution can be different from this distribution.\n",
      "[01:18:34.100 --> 01:18:38.100]   Like in this case, this can be like 0.5 equal to 1, 0.5 equal to minus 1.\n",
      "[01:18:38.100 --> 01:18:41.100]   And this can be with probability 1 equal to 0.\n",
      "[01:18:41.100 --> 01:18:44.100]   So the distribution actually depends on the past history.\n",
      "[01:18:44.100 --> 01:18:49.100]   The most important thing is the expectation of the new random noise,\n",
      "[01:18:49.100 --> 01:18:52.100]   conditional path is always equal to 0.\n",
      "[01:18:52.100 --> 01:18:56.100]   And as long as this martingale condition is actually holds,\n",
      "[01:18:56.100 --> 01:18:59.100]   we can also get the concentration in a quote.\n",
      "[01:18:59.100 --> 01:19:00.100]   Okay.\n",
      "[01:19:00.100 --> 01:19:05.100]   And this is what we will actually see in the next lecture.\n",
      "[01:19:05.100 --> 01:19:06.100]   Yes, question.\n",
      "[01:19:06.100 --> 01:19:09.100]   I was wondering when the first homework will be posted.\n",
      "[01:19:09.100 --> 01:19:14.100]   First homework will be posted by next Monday or Tuesday, yeah, or the next week.\n",
      "[01:19:14.100 --> 01:19:19.100]   Okay, and this ends the lecture today.\n",
      "[01:19:19.100 --> 01:19:24.100]   [MUSIC]\n",
      "\n",
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=n3WxDCvjlAs started\n",
      "n3WxDCvjlAs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/_Hxt8CibzIs.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/_Hxt8CibzIs.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/_Hxt8CibzIs.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/_Hxt8CibzIs.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1272.49 ms\n",
      "whisper_print_timings:     fallbacks =  15 p /  22 h\n",
      "whisper_print_timings:      mel time =  2688.38 ms\n",
      "whisper_print_timings:   sample time = 24002.75 ms / 61428 runs (    0.39 ms per run)\n",
      "whisper_print_timings:   encode time =   386.83 ms /   205 runs (    1.89 ms per run)\n",
      "whisper_print_timings:   decode time =  1063.71 ms /   604 runs (    1.76 ms per run)\n",
      "whisper_print_timings:   batchd time = 31285.41 ms / 59725 runs (    0.52 ms per run)\n",
      "whisper_print_timings:   prompt time = 10576.53 ms / 49166 runs (    0.22 ms per run)\n",
      "whisper_print_timings:    total time = 71747.44 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/n3WxDCvjlAs.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/n3WxDCvjlAs.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:19:15.61, start: 0.000000, bitrate: 273 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 173 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/n3WxDCvjlAs.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  148613kB time=01:19:15.60 bitrate= 256.0kbits/s speed=1.4e+03x     \n",
      "video:0kB audio:148613kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000051%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/n3WxDCvjlAs.wav' (76089748 samples, 4755.6 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.160]   [MUSIC PLAYING]\n",
      "[00:00:03.160 --> 00:00:04.680]   So in the very end of last lecture,\n",
      "[00:00:04.680 --> 00:00:06.800]   we talk about the Bellman-Optimatic equation.\n",
      "[00:00:06.800 --> 00:00:09.360]   Let's just quickly recap what we have been talking about.\n",
      "[00:00:09.360 --> 00:00:25.840]   So the Bellman-Optimatic equation\n",
      "[00:00:25.840 --> 00:00:27.920]   is similar to Bellman equation.\n",
      "[00:00:27.920 --> 00:00:29.600]   But the only modification is like when\n",
      "[00:00:29.600 --> 00:00:33.040]   we take some policy pi instead of taking action that\n",
      "[00:00:33.040 --> 00:00:37.520]   is averaging over a policy pi, but we take the max action.\n",
      "[00:00:37.520 --> 00:00:39.880]   So it has like two equations.\n",
      "[00:00:39.880 --> 00:00:45.240]   The first one is V star, H, S, is equal to taking\n",
      "[00:00:45.240 --> 00:00:52.800]   max over action A and Q star, H, S, S, S, A. And the other part\n",
      "[00:00:52.800 --> 00:00:59.000]   is the optimal Q function, Q star, H, S, A. It's\n",
      "[00:00:59.000 --> 00:01:04.760]   equal to intermediate reward plus--\n",
      "[00:01:04.760 --> 00:01:07.780]   we take an expectation over the value in the next step.\n",
      "[00:01:07.780 --> 00:01:18.280]   And in the last lecture, we also says\n",
      "[00:01:18.280 --> 00:01:20.320]   the Bellman-Optimatic equation directly\n",
      "[00:01:20.320 --> 00:01:28.440]   gives a policy, optimal policy, Q star, A, S, which\n",
      "[00:01:28.440 --> 00:01:31.360]   essentially this new policy, we're just\n",
      "[00:01:31.360 --> 00:01:34.360]   taking it greedily respect to Q star.\n",
      "[00:01:34.360 --> 00:01:36.320]   That is, we will, with probability one,\n",
      "[00:01:36.320 --> 00:01:39.120]   take the action that equal to the action that\n",
      "[00:01:39.120 --> 00:01:46.260]   maximizes the Q value, Q star value, A prime, Q star, H,\n",
      "[00:01:46.260 --> 00:01:47.080]   S, A prime.\n",
      "[00:01:47.080 --> 00:01:53.520]   So in the very end of the last lecture,\n",
      "[00:01:53.520 --> 00:01:56.240]   we kind of reason about, like we first initially\n",
      "[00:01:56.240 --> 00:01:59.120]   like a star from gas in this Bellman-Optimatic equation\n",
      "[00:01:59.120 --> 00:02:00.840]   and gas in this optimal policy.\n",
      "[00:02:00.840 --> 00:02:03.240]   And then we do the proposition and prove\n",
      "[00:02:03.240 --> 00:02:06.520]   we prove this policy pi star is actually, indeed,\n",
      "[00:02:06.520 --> 00:02:09.680]   the optimal policy, which achieved the optimal value,\n",
      "[00:02:09.680 --> 00:02:13.600]   not only in the very first step, but also in all H and R\n",
      "[00:02:13.600 --> 00:02:16.200]   as simultaneously.\n",
      "[00:02:16.200 --> 00:02:19.600]   So this is what we concluded at the very last part\n",
      "[00:02:19.600 --> 00:02:20.800]   of the lecture.\n",
      "[00:02:20.800 --> 00:02:26.000]   So today we're talking about this optimal policy\n",
      "[00:02:26.000 --> 00:02:29.840]   and so the task we're going to talk about today is planning.\n",
      "[00:02:29.840 --> 00:02:37.120]   So planning, you can think it's something similar to policy\n",
      "[00:02:37.120 --> 00:02:41.360]   evaluation, but instead of evaluating a fixed policy,\n",
      "[00:02:41.360 --> 00:02:44.840]   now I want to find the optimal policy.\n",
      "[00:02:44.840 --> 00:02:47.560]   So the planning task is defined as follows.\n",
      "[00:02:47.560 --> 00:03:00.520]   We're given with the environment P and also reward R.\n",
      "[00:03:00.520 --> 00:03:09.080]   This is similar to the setting of policy evaluation.\n",
      "[00:03:09.080 --> 00:03:11.440]   We all know the environment.\n",
      "[00:03:11.440 --> 00:03:13.320]   This is like the environment, if you know.\n",
      "[00:03:16.960 --> 00:03:20.160]   And our goal is we want to compute the optimal policy.\n",
      "[00:03:20.160 --> 00:03:29.600]   Pi star.\n",
      "[00:03:29.600 --> 00:03:37.080]   So similar to policy evaluation, the only difference\n",
      "[00:03:37.080 --> 00:03:39.520]   is now we no longer just evaluate the value of some fixed\n",
      "[00:03:39.520 --> 00:03:43.520]   policy, which is directly one to find optimal policy.\n",
      "[00:03:43.520 --> 00:03:47.360]   So we will basically introduce a couple of algorithm\n",
      "[00:03:47.360 --> 00:03:51.280]   which can solve this problem.\n",
      "[00:03:51.280 --> 00:03:56.200]   So the first algorithm we can immediately also take analog\n",
      "[00:03:56.200 --> 00:03:59.520]   to the policy evaluation, where from policy evaluation,\n",
      "[00:03:59.520 --> 00:04:02.880]   the basic algorithm we do is we use essentially leverage\n",
      "[00:04:02.880 --> 00:04:05.560]   development equation and the dual dynamic programming\n",
      "[00:04:05.560 --> 00:04:08.560]   based on that, and we directly do the policy evaluation.\n",
      "[00:04:08.560 --> 00:04:10.680]   So similarly, in the planning here,\n",
      "[00:04:10.680 --> 00:04:12.680]   we can also just directly leverage the development\n",
      "[00:04:12.680 --> 00:04:15.520]   optimization equation, which gives a recursive form\n",
      "[00:04:15.520 --> 00:04:17.600]   on how the value is going to propagate.\n",
      "[00:04:17.600 --> 00:04:19.480]   And then we use dynamic programming\n",
      "[00:04:19.480 --> 00:04:21.280]   to compute optimal policy.\n",
      "[00:04:21.280 --> 00:04:23.560]   So this gives the first algorithm that\n",
      "[00:04:23.560 --> 00:04:24.760]   is called the value iteration.\n",
      "[00:04:24.760 --> 00:04:31.080]   So the first idea is basically leverage.\n",
      "[00:04:31.080 --> 00:04:52.960]   So and use dynamo programming.\n",
      "[00:04:52.960 --> 00:04:55.600]   I will just-- the strategy is a DP.\n",
      "[00:04:55.600 --> 00:05:03.200]   So the algorithm-- again, that's also very simple.\n",
      "[00:05:03.200 --> 00:05:05.640]   That's called algorithm 1 of today.\n",
      "[00:05:05.640 --> 00:05:06.800]   It's called a value iteration.\n",
      "[00:05:06.800 --> 00:05:21.480]   It's called value iteration, which is called VI.\n",
      "[00:05:21.480 --> 00:05:23.040]   It turns out this value iteration is also\n",
      "[00:05:23.040 --> 00:05:25.500]   of a very important algorithm we will actually\n",
      "[00:05:25.500 --> 00:05:29.480]   see throughout this class.\n",
      "[00:05:29.480 --> 00:05:31.800]   So a value iteration consists of follows.\n",
      "[00:05:31.800 --> 00:05:38.440]   First, we initialize similar to policy evaluation.\n",
      "[00:05:38.440 --> 00:05:43.920]   We also initialize the value at last step to be always 0,\n",
      "[00:05:43.920 --> 00:05:48.000]   because there is no future reward to collect after H plus 1\n",
      "[00:05:48.000 --> 00:05:49.800]   step is for any else.\n",
      "[00:05:52.800 --> 00:05:54.800]   And then we just do dynamo programming\n",
      "[00:05:54.800 --> 00:05:57.760]   from the last step to the first step.\n",
      "[00:05:57.760 --> 00:06:03.240]   So for H, from the last step, capital H, H minus 1,\n",
      "[00:06:03.240 --> 00:06:06.240]   dot of r t o 1, we do the following.\n",
      "[00:06:06.240 --> 00:06:11.800]   Essentially, we just copy the Bellmo-optimistic equation.\n",
      "[00:06:11.800 --> 00:06:22.760]   That is a Q star, H, S, A. It's equal to r H, S, A.\n",
      "[00:06:23.760 --> 00:06:33.760]   And the plus V H, V H plus 1, star, S, A.\n",
      "[00:06:33.760 --> 00:06:39.260]   We compute this for all state action pair at a step H.\n",
      "[00:06:39.260 --> 00:06:44.040]   And then we do the second Bellmo equation.\n",
      "[00:06:44.040 --> 00:06:49.640]   That is, V star, S is equal to Q star, S, A,\n",
      "[00:06:49.640 --> 00:06:51.120]   by taking max over action.\n",
      "[00:06:51.120 --> 00:06:57.080]   We do this for any states.\n",
      "[00:06:57.080 --> 00:07:05.560]   This is essentially just a copy the Bellmo-optimistic equation.\n",
      "[00:07:05.560 --> 00:07:07.800]   And finally, we say optimal policy\n",
      "[00:07:07.800 --> 00:07:13.840]   is just what we wrote here.\n",
      "[00:07:13.840 --> 00:07:24.600]   That is, in the caterer, if we take the argmax of Q star\n",
      "[00:07:24.600 --> 00:07:43.760]   value, it's for any H, S, A.\n",
      "[00:07:43.760 --> 00:07:46.320]   Since we already proved in the last lecture,\n",
      "[00:07:46.320 --> 00:07:49.760]   pi star is the optimal policy.\n",
      "[00:07:49.760 --> 00:07:51.520]   And y-value iteration works.\n",
      "[00:07:51.520 --> 00:07:52.960]   It's pretty straightforward.\n",
      "[00:07:52.960 --> 00:07:55.520]   This directly gives the optimal policy.\n",
      "[00:07:55.520 --> 00:07:58.440]   And we can also see this is like a demo programming.\n",
      "[00:07:58.440 --> 00:08:00.600]   Because if you look at the commutation flow,\n",
      "[00:08:00.600 --> 00:08:03.400]   what we did is, again, very similar to the last lecture.\n",
      "[00:08:03.400 --> 00:08:07.560]   We first start with the V H plus 1.\n",
      "[00:08:07.560 --> 00:08:09.160]   Only difference is now it's no longer\n",
      "[00:08:09.160 --> 00:08:12.560]   for some particular policies, pi, but for the optimal policy.\n",
      "[00:08:12.560 --> 00:08:14.840]   That is the S dimensional vector.\n",
      "[00:08:14.840 --> 00:08:18.120]   And then the first step, we can show the--\n",
      "[00:08:18.120 --> 00:08:24.280]   this Q star capital H, which is S, A dimensional vector.\n",
      "[00:08:24.280 --> 00:08:29.160]   And then we can show the V star H,\n",
      "[00:08:29.160 --> 00:08:32.640]   which is the S dimensional vector, and go on.\n",
      "[00:08:32.640 --> 00:08:35.840]   So essentially, we only take some linear 2H computation,\n",
      "[00:08:35.840 --> 00:08:37.960]   and we can compute the optimal policy.\n",
      "[00:08:37.960 --> 00:08:48.820]   OK, any questions so far?\n",
      "[00:08:48.820 --> 00:08:49.800]   Yes?\n",
      "[00:08:49.800 --> 00:08:53.600]   Is pi star inside the full loop or outside?\n",
      "[00:08:53.600 --> 00:08:56.640]   The question is, is pi star inside the full loop?\n",
      "[00:08:56.640 --> 00:08:59.280]   Oh, sorry.\n",
      "[00:08:59.280 --> 00:09:03.800]   Because we're already given this value of policy for any H,\n",
      "[00:09:03.800 --> 00:09:06.320]   so it can be outside the loop.\n",
      "[00:09:06.320 --> 00:09:07.640]   But you can also put it inside the loop,\n",
      "[00:09:07.640 --> 00:09:10.440]   and then you don't need to do any H, because you already\n",
      "[00:09:10.440 --> 00:09:11.480]   do follow.\n",
      "[00:09:11.480 --> 00:09:12.080]   Both are fine.\n",
      "[00:09:12.080 --> 00:09:34.120]   So for some historical reasons, I will also introduce\n",
      "[00:09:34.120 --> 00:09:39.280]   another algorithm, also doing the planning, which is the algorithm\n",
      "[00:09:39.280 --> 00:09:42.600]   actually that does not leverage the boundary\n",
      "[00:09:42.600 --> 00:09:44.440]   optimization question, but actually only\n",
      "[00:09:44.440 --> 00:09:46.800]   is the boundary equation.\n",
      "[00:09:46.800 --> 00:09:51.320]   So that's the second algorithm called policy equation.\n",
      "[00:09:51.320 --> 00:10:10.320]   So yeah, in general planning, you\n",
      "[00:10:10.320 --> 00:10:12.080]   can have a lot of different planning algorithm,\n",
      "[00:10:12.080 --> 00:10:17.280]   but I think those two are mostly commonly described\n",
      "[00:10:17.280 --> 00:10:18.760]   in the intro textbook.\n",
      "[00:10:18.760 --> 00:10:23.260]   So policy iteration-- so I think the value iteration,\n",
      "[00:10:23.260 --> 00:10:24.880]   you can think of the most important part\n",
      "[00:10:24.880 --> 00:10:27.360]   is that we compute this value of v star,\n",
      "[00:10:27.360 --> 00:10:30.640]   and we use the optimum optimal equation to compute the v star.\n",
      "[00:10:30.640 --> 00:10:32.480]   Well, on the other hand, the policy iteration\n",
      "[00:10:32.480 --> 00:10:35.600]   will maintain a policy, and we keep improving the policy\n",
      "[00:10:35.600 --> 00:10:37.120]   at every iteration.\n",
      "[00:10:37.120 --> 00:10:39.720]   So what are we going to do is we will initialize--\n",
      "[00:10:43.240 --> 00:10:48.240]   let's call it pi 0.\n",
      "[00:10:48.240 --> 00:10:52.960]   Basically, we already know pi 0 is a subject which essentially\n",
      "[00:10:52.960 --> 00:11:00.080]   is an H different policy, which have one policy at every step,\n",
      "[00:11:00.080 --> 00:11:14.080]   to be some arbitrary policy on all states.\n",
      "[00:11:14.080 --> 00:11:21.920]   So the initialization doesn't really matter that much,\n",
      "[00:11:21.920 --> 00:11:26.000]   and typically you can either do some uniformly random policy\n",
      "[00:11:26.000 --> 00:11:27.720]   or you have some prior knowledge,\n",
      "[00:11:27.720 --> 00:11:32.240]   and you just use a prior knowledge to initialize your policy.\n",
      "[00:11:32.240 --> 00:11:36.920]   So the policy iteration also is like iterative algorithm,\n",
      "[00:11:36.920 --> 00:11:39.320]   so it has a for loop.\n",
      "[00:11:39.320 --> 00:11:43.880]   For t equal to 1, 2, [INAUDIBLE]\n",
      "[00:11:43.880 --> 00:11:47.400]   do follow in two steps.\n",
      "[00:11:47.400 --> 00:11:50.600]   So the policy iteration mainly consists of two parts.\n",
      "[00:11:50.600 --> 00:11:53.280]   The first part is so-called policy evaluation.\n",
      "[00:11:53.280 --> 00:11:57.240]   Basically, it directly leverages a policy evaluation.\n",
      "[00:11:57.240 --> 00:12:21.000]   That is, we will evaluate the value of pi t minus 1 as a.\n",
      "[00:12:21.000 --> 00:12:26.480]   So we will evaluate the value of previous policy\n",
      "[00:12:26.480 --> 00:12:28.480]   at all SH pair.\n",
      "[00:12:28.480 --> 00:12:34.920]   So essentially, it leverages the knowledge\n",
      "[00:12:34.920 --> 00:12:36.320]   we learned in the last lecture.\n",
      "[00:12:36.320 --> 00:12:39.480]   We can essentially just run a dynamic programming\n",
      "[00:12:39.480 --> 00:12:42.480]   using Bellman equation, and to compute this value\n",
      "[00:12:42.480 --> 00:12:47.120]   for all the state action at every step.\n",
      "[00:12:47.120 --> 00:12:49.880]   And the second step is called policy improvement.\n",
      "[00:12:49.880 --> 00:13:02.360]   So because our goal of planning is\n",
      "[00:13:02.360 --> 00:13:05.240]   we want to find optimal policy, so it's not very useful.\n",
      "[00:13:05.240 --> 00:13:06.920]   If we just evaluate the value, we\n",
      "[00:13:06.920 --> 00:13:09.720]   need to improve based on the current policy.\n",
      "[00:13:09.720 --> 00:13:13.040]   So what we do is we will update the policy.\n",
      "[00:13:13.040 --> 00:13:16.240]   So this is a previous policy that is pi t minus 1.\n",
      "[00:13:16.240 --> 00:13:21.120]   So we will update the new policy, pi t h s.\n",
      "[00:13:21.120 --> 00:13:38.640]   That is equal to r max a in a, q pi t minus 1, h s a.\n",
      "[00:13:38.640 --> 00:13:46.600]   This is also, we do this for all SH pair as a H pair.\n",
      "[00:13:46.600 --> 00:13:48.160]   So what we're essentially doing is\n",
      "[00:13:48.160 --> 00:13:52.360]   we use a previous policy to evaluate its value.\n",
      "[00:13:52.360 --> 00:13:54.080]   And then we use this value.\n",
      "[00:13:54.080 --> 00:13:56.680]   Essentially, we take the greedy action, greedy action\n",
      "[00:13:56.680 --> 00:13:58.160]   that maximizes this value.\n",
      "[00:13:58.160 --> 00:13:59.960]   And we make this as a new policy.\n",
      "[00:13:59.960 --> 00:14:09.900]   [INAUDIBLE]\n",
      "[00:14:09.900 --> 00:14:12.380]   So this is a slightly different approach than valetration,\n",
      "[00:14:12.380 --> 00:14:14.960]   which this used Bellman optimal equation.\n",
      "[00:14:14.960 --> 00:14:17.080]   Well, this essentially just used original Bellman equation\n",
      "[00:14:17.080 --> 00:14:19.080]   without using the Bellman optimal equation.\n",
      "[00:14:29.920 --> 00:14:34.360]   So the question here is, OK, we know that valetration algorithm\n",
      "[00:14:34.360 --> 00:14:38.600]   clearly works, clearly we'll find the optimal policy.\n",
      "[00:14:38.600 --> 00:14:40.400]   But would this algorithm eventually\n",
      "[00:14:40.400 --> 00:14:42.160]   converge to optimal policy?\n",
      "[00:14:42.160 --> 00:14:48.040]   Or how many iterations we need to find the optimal policy?\n",
      "[00:14:48.040 --> 00:14:49.760]   So this is answered in the following theorem.\n",
      "[00:14:49.760 --> 00:15:00.160]   [INAUDIBLE]\n",
      "[00:15:00.160 --> 00:15:14.840]   We will say, pi t is optimal for any t greater or equal to h.\n",
      "[00:15:14.840 --> 00:15:18.760]   That is after H iteration of this policy evaluation\n",
      "[00:15:18.760 --> 00:15:22.480]   and policy improvement, we will guarantee this policy\n",
      "[00:15:22.480 --> 00:15:26.120]   iteration already finds the optimal policy.\n",
      "[00:15:26.120 --> 00:15:29.560]   Maybe in practice, it can find optimal policy a bit earlier,\n",
      "[00:15:29.560 --> 00:15:31.640]   but this is like guaranteed in the worst case.\n",
      "[00:15:31.640 --> 00:15:49.640]   [INAUDIBLE]\n",
      "[00:15:49.640 --> 00:15:52.080]   Any question about the algorithm and the statement so far?\n",
      "[00:15:52.080 --> 00:16:01.200]   So we'll do the proof for why this is true.\n",
      "[00:16:01.200 --> 00:16:04.840]   So I think because a lot of the algorithm of reinforcement\n",
      "[00:16:04.840 --> 00:16:07.160]   in MDP is based on the number of programming,\n",
      "[00:16:07.160 --> 00:16:10.000]   so a lot of proof is also based on induction.\n",
      "[00:16:10.000 --> 00:16:13.360]   You can think it's like a mathematical version\n",
      "[00:16:13.360 --> 00:16:15.960]   to prove something related to the number of programming.\n",
      "[00:16:15.960 --> 00:16:20.280]   So we will again do an induction,\n",
      "[00:16:20.280 --> 00:16:29.400]   and we'll use induction hypothesis\n",
      "[00:16:29.400 --> 00:16:34.000]   that we will actually prove a stronger result, which says\n",
      "[00:16:34.000 --> 00:16:41.320]   this kill H pi t, this is the value of pi t,\n",
      "[00:16:41.320 --> 00:16:51.600]   is equal to kill star h for any h greater than h minus t.\n",
      "[00:16:58.000 --> 00:17:00.560]   So what did this induction hypothesis talking about?\n",
      "[00:17:00.560 --> 00:17:06.440]   We have h equal to 1, h equal to 2,\n",
      "[00:17:06.440 --> 00:17:08.520]   and eventually we have h equal to h.\n",
      "[00:17:08.520 --> 00:17:22.080]   And this is t equal to 0, t equal to 1, and the dot, t equal to h.\n",
      "[00:17:22.080 --> 00:17:22.840]   This is a step.\n",
      "[00:17:22.840 --> 00:17:37.880]   So what we are talking about is that in the very first step,\n",
      "[00:17:37.880 --> 00:17:43.280]   that is t equal to 0, only h equal to h,\n",
      "[00:17:43.280 --> 00:17:44.440]   we have the optimal value.\n",
      "[00:17:44.440 --> 00:17:47.040]   That is only in the very last step, our value\n",
      "[00:17:47.040 --> 00:17:48.120]   equal to the optimal value.\n",
      "[00:17:48.120 --> 00:17:50.920]   And everything else is not equal to the optimal value.\n",
      "[00:17:50.920 --> 00:17:54.240]   Or we don't guarantee it equal to optimal value.\n",
      "[00:17:54.240 --> 00:17:55.960]   In the second step, we're guaranteed\n",
      "[00:17:55.960 --> 00:17:57.840]   not only the very last step, our value\n",
      "[00:17:57.840 --> 00:18:00.680]   is equal to optimal value, but also the second last step,\n",
      "[00:18:00.680 --> 00:18:03.760]   our value is equal to optimal value.\n",
      "[00:18:03.760 --> 00:18:06.440]   But we don't guarantee everything else.\n",
      "[00:18:06.440 --> 00:18:09.640]   And so what we are seeing is essentially in every step,\n",
      "[00:18:09.640 --> 00:18:13.160]   we're guaranteed one more step so that our value\n",
      "[00:18:13.160 --> 00:18:16.600]   is equal to the optimal value, and eventually in the h step,\n",
      "[00:18:16.600 --> 00:18:19.280]   we guarantee all the value is equal to the optimal value.\n",
      "[00:18:20.280 --> 00:18:26.280]   So this is what this induction hypothesis is talking about.\n",
      "[00:18:26.280 --> 00:18:53.880]   OK, so let's start an induction.\n",
      "[00:18:53.880 --> 00:18:55.280]   So we first look at the base case.\n",
      "[00:18:55.280 --> 00:19:02.880]   Base case, we only need to check t equal to 0,\n",
      "[00:19:02.880 --> 00:19:04.520]   our induction is true.\n",
      "[00:19:04.520 --> 00:19:08.880]   And when t equals 0, all we say is the value at the very last step\n",
      "[00:19:08.880 --> 00:19:11.360]   is equal to the optimal value.\n",
      "[00:19:11.360 --> 00:19:15.680]   So the very last step is q pi 0, h, s a.\n",
      "[00:19:21.560 --> 00:19:25.520]   And we know this is the value at the very last step.\n",
      "[00:19:25.520 --> 00:19:28.280]   And we know the value at the very last step\n",
      "[00:19:28.280 --> 00:19:31.800]   is basically just an intermediate reward of I receive,\n",
      "[00:19:31.800 --> 00:19:33.680]   that is, r, h, s a.\n",
      "[00:19:33.680 --> 00:19:36.680]   And then we no longer have any future reward.\n",
      "[00:19:36.680 --> 00:19:38.000]   This is basically all the reward\n",
      "[00:19:38.000 --> 00:19:40.800]   I'm going to get in the future.\n",
      "[00:19:40.800 --> 00:19:43.080]   And then we realize because this q value already\n",
      "[00:19:43.080 --> 00:19:47.120]   specified both state and action, so this both s and a are fixed.\n",
      "[00:19:47.120 --> 00:19:49.400]   None of them are depends on the policy.\n",
      "[00:19:49.400 --> 00:19:51.440]   So although this is like arbitrary policy,\n",
      "[00:19:51.440 --> 00:19:54.800]   but it doesn't really affect the value at the very last step.\n",
      "[00:19:54.800 --> 00:20:01.800]   So this is also equal to q star and t s a.\n",
      "[00:20:01.800 --> 00:20:03.480]   So the base case is correct.\n",
      "[00:20:15.400 --> 00:20:28.680]   So then we say suppose to hypothesis,\n",
      "[00:20:28.680 --> 00:20:40.720]   suppose the induction hypothesis is true for step t\n",
      "[00:20:40.720 --> 00:20:48.600]   or for iteration t.\n",
      "[00:20:48.600 --> 00:20:51.680]   Let's look at the exam t plus 1.\n",
      "[00:20:51.680 --> 00:21:07.760]   So basically, by looking at t plus 1 situation,\n",
      "[00:21:07.760 --> 00:21:15.200]   we need to look at the policy of t plus 1 situation of s.\n",
      "[00:21:15.200 --> 00:21:17.960]   I would just use this notation because we already\n",
      "[00:21:17.960 --> 00:21:22.800]   seen this like a policy evaluation in all the policies\n",
      "[00:21:22.800 --> 00:21:25.320]   like a deterministic policy.\n",
      "[00:21:25.320 --> 00:21:27.400]   And we know by definition, this is\n",
      "[00:21:27.400 --> 00:21:39.120]   equal to arg max a of q h pi t s a.\n",
      "[00:21:39.120 --> 00:21:41.760]   It's equal to the greedy policy, which\n",
      "[00:21:41.760 --> 00:21:44.560]   look at the value of the previous policy.\n",
      "[00:21:44.560 --> 00:21:50.400]   This is directly by PR algorithm.\n",
      "[00:21:52.920 --> 00:22:06.160]   The policy information algorithm, and we\n",
      "[00:22:06.160 --> 00:22:11.120]   exam for any h greater or equal to h minus t.\n",
      "[00:22:11.120 --> 00:22:18.720]   And we also know, because by induction hypothesis,\n",
      "[00:22:18.720 --> 00:22:22.840]   we know for any h that is greater than capital H minus t,\n",
      "[00:22:22.840 --> 00:22:26.720]   this q value is actually already equal to q star.\n",
      "[00:22:26.720 --> 00:22:34.200]   So we know this is equal to arg max a of q star s a.\n",
      "[00:22:34.200 --> 00:22:39.680]   This is by induction hypothesis.\n",
      "[00:22:39.680 --> 00:23:02.320]   And finally, we remember the optimal policy is basically\n",
      "[00:23:02.320 --> 00:23:05.520]   just the greedy policy with respect to q star.\n",
      "[00:23:05.520 --> 00:23:09.200]   So we know this is just equal to pi star h s.\n",
      "[00:23:09.200 --> 00:23:27.120]   So this essentially just already implied\n",
      "[00:23:27.120 --> 00:23:30.560]   that for any h greater than h minus t,\n",
      "[00:23:30.560 --> 00:23:34.080]   I'm just going to copy this precondition here.\n",
      "[00:23:34.080 --> 00:23:38.600]   We can say for any step that is larger than this h minus t,\n",
      "[00:23:38.600 --> 00:23:40.920]   we already know the policy we learned\n",
      "[00:23:40.920 --> 00:23:44.480]   up to t minus y iteration is already optimal.\n",
      "[00:23:44.480 --> 00:23:46.500]   Those policies is equal to the optimal policy.\n",
      "[00:23:46.500 --> 00:23:53.160]   And the final argument is just the argument\n",
      "[00:23:53.160 --> 00:23:55.840]   we essentially already said in the last lecture.\n",
      "[00:23:55.840 --> 00:24:03.120]   Now, this we look at the q h minus y pi t plus y.\n",
      "[00:24:03.120 --> 00:24:14.640]   We notice this q value and this s a only\n",
      "[00:24:14.640 --> 00:24:26.520]   depends on all the future policy, like pi h, 2 plus 1,\n",
      "[00:24:26.520 --> 00:24:34.720]   pi h plus 1, 2 plus 1, and the dot 2 pi h, 2 plus 1.\n",
      "[00:24:34.720 --> 00:24:37.160]   So the important part is this q value,\n",
      "[00:24:37.160 --> 00:24:39.920]   although at the h minus 1 step, doesn't really\n",
      "[00:24:39.920 --> 00:24:43.080]   depends on the policy at the h minus 1 step.\n",
      "[00:24:43.080 --> 00:24:45.760]   The reason is the action at h minus 1 step\n",
      "[00:24:45.760 --> 00:24:50.920]   is already specified in this a, which basically like the policy\n",
      "[00:24:50.920 --> 00:24:54.040]   at h minus 1 step actually doesn't affect this action a.\n",
      "[00:24:54.040 --> 00:24:56.920]   So that's why it doesn't depend on the intermediate policy.\n",
      "[00:24:56.920 --> 00:25:00.200]   It only depends on the future policy.\n",
      "[00:25:00.200 --> 00:25:03.600]   And we already know all of them are equal to pi star.\n",
      "[00:25:03.600 --> 00:25:19.080]   So this just implies that this is also equal to q star, h\n",
      "[00:25:19.080 --> 00:25:22.200]   minus 1, s a.\n",
      "[00:25:22.200 --> 00:25:24.640]   The reason is because the policy is optimal policy,\n",
      "[00:25:24.640 --> 00:25:26.280]   so the value is also optimal value.\n",
      "[00:25:31.840 --> 00:25:35.840]   So now we can just reenvex a little bit.\n",
      "[00:25:35.840 --> 00:25:39.320]   Because now we say for any h greater than capital H minus t,\n",
      "[00:25:39.320 --> 00:25:43.040]   we have a value equal to the optimal value up to h minus 1\n",
      "[00:25:43.040 --> 00:25:44.160]   step.\n",
      "[00:25:44.160 --> 00:25:49.000]   So this is also equivalent to say for any h,\n",
      "[00:25:49.000 --> 00:25:53.880]   we just do some reenvexing for any h greater than capital H minus\n",
      "[00:25:53.880 --> 00:25:57.840]   t minus 1.\n",
      "[00:25:57.840 --> 00:26:04.720]   We have our value, q pi t plus 1, h,\n",
      "[00:26:04.720 --> 00:26:09.760]   equal to the optimal value, q star h.\n",
      "[00:26:09.760 --> 00:26:13.280]   And this is what we want to say in the induction hypothesis.\n",
      "[00:26:13.280 --> 00:26:14.880]   And so we finish the proof.\n",
      "[00:26:14.880 --> 00:26:37.860]   [INAUDIBLE]\n",
      "[00:26:37.860 --> 00:26:38.380]   Yes?\n",
      "[00:26:38.380 --> 00:26:42.840]   What's the rationale for the base case of the optimal value?\n",
      "[00:26:42.840 --> 00:26:46.200]   You're asking why did this is true again?\n",
      "[00:26:46.200 --> 00:26:49.320]   OK, so the question is why base case is true?\n",
      "[00:26:49.320 --> 00:26:50.800]   As we already said, the base case\n",
      "[00:26:50.800 --> 00:26:53.000]   look at the value at the very last step.\n",
      "[00:26:53.000 --> 00:26:55.160]   And if you expand, the value essentially\n",
      "[00:26:55.160 --> 00:26:56.600]   is the cumulative reward.\n",
      "[00:26:56.600 --> 00:26:58.800]   That is summation of all the future reward.\n",
      "[00:26:58.800 --> 00:27:00.320]   So in the very last step, the reward\n",
      "[00:27:00.320 --> 00:27:02.480]   that we're going to receive is only at the very last step.\n",
      "[00:27:02.480 --> 00:27:03.600]   That is rh.\n",
      "[00:27:03.600 --> 00:27:05.520]   And we don't have any future reward.\n",
      "[00:27:05.520 --> 00:27:08.240]   And this rh only depends on the state action pair,\n",
      "[00:27:08.240 --> 00:27:10.560]   which is already specified here.\n",
      "[00:27:10.560 --> 00:27:12.600]   So the both of them are fixed.\n",
      "[00:27:12.600 --> 00:27:14.720]   And they don't depend on the policy.\n",
      "[00:27:14.720 --> 00:27:17.200]   So that's why those two are equal.\n",
      "[00:27:17.200 --> 00:27:21.880]   [INAUDIBLE]\n",
      "[00:27:21.880 --> 00:27:22.760]   Yes, a good question.\n",
      "[00:27:22.760 --> 00:27:32.600]   I think up to some tie, tie-breaking, I think they are equal.\n",
      "[00:27:32.600 --> 00:27:34.120]   But if they're tie-breaking, maybe they\n",
      "[00:27:34.120 --> 00:27:36.080]   will converge to a different action, but that's fine.\n",
      "[00:27:36.080 --> 00:27:39.760]   [INAUDIBLE]\n",
      "[00:27:39.760 --> 00:27:41.960]   That's also another good question.\n",
      "[00:27:41.960 --> 00:27:45.720]   I think after the questions, asking\n",
      "[00:27:45.720 --> 00:27:49.920]   about the speed of policy iteration versus value iteration.\n",
      "[00:27:49.920 --> 00:27:53.880]   So from what we can see here in this episodic setting.\n",
      "[00:27:53.880 --> 00:27:59.120]   And if we were talking about the computation in the worst\n",
      "[00:27:59.120 --> 00:28:02.920]   case, and we will see probably the answers,\n",
      "[00:28:02.920 --> 00:28:04.680]   value iteration is a lot faster.\n",
      "[00:28:04.680 --> 00:28:08.600]   And the reason is here, we need to do h iteration.\n",
      "[00:28:08.600 --> 00:28:12.880]   And in average iteration, we need to do this like a policy\n",
      "[00:28:12.880 --> 00:28:15.680]   evaluation, which is already doing a Bellman equation.\n",
      "[00:28:15.680 --> 00:28:17.920]   And some people will argue for this Bellman equation,\n",
      "[00:28:17.920 --> 00:28:21.920]   because we're evaluating only the optimal policy.\n",
      "[00:28:21.920 --> 00:28:23.440]   Only the determinant policy.\n",
      "[00:28:23.440 --> 00:28:26.440]   So that's why we can do this policy iteration a little bit\n",
      "[00:28:26.440 --> 00:28:28.880]   faster than a policy evaluation, a little bit faster\n",
      "[00:28:28.880 --> 00:28:30.400]   than this Bellman-optimal equation.\n",
      "[00:28:30.400 --> 00:28:32.640]   Because this actually look at all the a.\n",
      "[00:28:32.640 --> 00:28:34.400]   Well, this only look at the action\n",
      "[00:28:34.400 --> 00:28:36.800]   that's taken by the policy, pi t minus 1,\n",
      "[00:28:36.800 --> 00:28:38.600]   because this is just amnestic.\n",
      "[00:28:38.600 --> 00:28:43.000]   But the problem is this argmax is also looking at all a.\n",
      "[00:28:43.000 --> 00:28:44.920]   So essentially, the computation of two steps\n",
      "[00:28:44.920 --> 00:28:49.120]   is roughly the same as just doing one pass of this Bellman\n",
      "[00:28:49.120 --> 00:28:49.880]  -optimal equation.\n",
      "[00:28:49.880 --> 00:28:52.240]   So you can think, just one iteration\n",
      "[00:28:52.240 --> 00:28:56.640]   is roughly equivalent to the entire value iteration algorithm\n",
      "[00:28:56.640 --> 00:28:57.840]   in this episodic setting.\n",
      "[00:28:57.840 --> 00:29:00.680]   So I would say the worst case performance, in theory,\n",
      "[00:29:00.680 --> 00:29:03.120]   it looks like value iteration is faster.\n",
      "[00:29:03.120 --> 00:29:05.400]   But I do see people in--\n",
      "[00:29:05.400 --> 00:29:09.420]   unlike arguing and saying, this is an in practice\n",
      "[00:29:09.420 --> 00:29:10.840]   policy iteration can be faster.\n",
      "[00:29:10.840 --> 00:29:12.920]   And I think it probably makes more sense\n",
      "[00:29:12.920 --> 00:29:16.040]   in the infinite horizon, this kind of setting,\n",
      "[00:29:16.040 --> 00:29:18.800]   where they can look at some stationary policy,\n",
      "[00:29:18.800 --> 00:29:21.040]   where this pi doesn't depends on h.\n",
      "[00:29:21.040 --> 00:29:27.000]   And so you don't need to do this for h times.\n",
      "[00:29:27.000 --> 00:29:28.760]   You just need to do this for one time.\n",
      "[00:29:28.760 --> 00:29:31.000]   But this is a little bit beyond this class.\n",
      "[00:29:31.000 --> 00:29:32.760]   I don't want to talk too much about it.\n",
      "[00:29:32.760 --> 00:29:34.760]   [VIDEO PLAYBACK]\n",
      "[00:29:34.760 --> 00:29:51.100]   [END PLAYBACK]\n",
      "[00:29:51.100 --> 00:29:51.600]   OK.\n",
      "[00:29:51.600 --> 00:30:17.360]   [END PLAYBACK]\n",
      "[00:30:17.360 --> 00:30:19.680]   So this essentially concludes what\n",
      "[00:30:19.680 --> 00:30:21.600]   I want to talk about for planning algorithm.\n",
      "[00:30:21.600 --> 00:30:23.560]   Of course, there's some other planning algorithm.\n",
      "[00:30:23.560 --> 00:30:26.040]   But for the purpose of this course,\n",
      "[00:30:26.040 --> 00:30:29.400]   I think we would like to examine more challenging setting\n",
      "[00:30:29.400 --> 00:30:31.920]   and develop guarantees for those kind of settings,\n",
      "[00:30:31.920 --> 00:30:34.320]   instead of talking about a lot of algorithm achieved\n",
      "[00:30:34.320 --> 00:30:37.200]   a similar objective.\n",
      "[00:30:37.200 --> 00:30:37.760]   OK.\n",
      "[00:30:37.760 --> 00:30:40.080]   So before we conclude the planning part,\n",
      "[00:30:40.080 --> 00:30:44.400]   I would like to also mention a little bit about the episodic\n",
      "[00:30:44.400 --> 00:30:47.360]   setting versus infinite horizon setting.\n",
      "[00:30:47.360 --> 00:30:48.760]   This provides some comment.\n",
      "[00:31:15.360 --> 00:31:17.680]   So things we're already talking about, like the policy\n",
      "[00:31:17.680 --> 00:31:19.480]   situation and some stationary policy.\n",
      "[00:31:19.480 --> 00:31:22.560]   So we will just comment about the difference between those two.\n",
      "[00:31:22.560 --> 00:31:26.400]   I think throughout this class, we will just use episodic setting.\n",
      "[00:31:26.400 --> 00:31:29.600]   I think the reason is, although we have additional index\n",
      "[00:31:29.600 --> 00:31:33.080]   of this little h, but everything, the underlying mechanism\n",
      "[00:31:33.080 --> 00:31:33.880]   is much clearer.\n",
      "[00:31:33.880 --> 00:31:35.960]   We just do, like, dynamic programming.\n",
      "[00:31:35.960 --> 00:31:38.040]   Well, in an infinite horizon, this kind of setting\n",
      "[00:31:38.040 --> 00:31:40.360]   will say a little bit.\n",
      "[00:31:40.360 --> 00:31:44.560]   I think the key quantity there is, instead of how many steps\n",
      "[00:31:44.560 --> 00:31:47.240]   you end the per episode, they're saying\n",
      "[00:31:47.240 --> 00:31:50.240]   like you have infinite number of steps.\n",
      "[00:31:50.240 --> 00:31:53.560]   But instead, you kind of discounted the future reward\n",
      "[00:31:53.560 --> 00:31:55.920]   by some discounting factor gamma.\n",
      "[00:31:55.920 --> 00:31:58.680]   This gamma is smaller than 1, and this is discount factor.\n",
      "[00:31:58.680 --> 00:32:12.400]   So basically, it will be much clearer what is this setting\n",
      "[00:32:12.400 --> 00:32:14.760]   if we look at the value function.\n",
      "[00:32:14.760 --> 00:32:18.200]   So essentially, the MDP in the infinite horizon\n",
      "[00:32:18.200 --> 00:32:20.840]   discounted setting is very similar to our setting, which\n",
      "[00:32:20.840 --> 00:32:26.200]   has a set of states, set of action and reward transition\n",
      "[00:32:26.200 --> 00:32:27.320]   probability.\n",
      "[00:32:27.320 --> 00:32:30.240]   And the only difference is now we no longer have a finite horizon.\n",
      "[00:32:30.240 --> 00:32:32.120]   We have a discounting factor, gamma.\n",
      "[00:32:35.160 --> 00:32:44.400]   And the value of a state and a policy pi\n",
      "[00:32:44.400 --> 00:32:46.840]   now is computed in the following way.\n",
      "[00:32:46.840 --> 00:32:51.080]   That is, I will take expectation over all the future\n",
      "[00:32:51.080 --> 00:32:53.080]   as a pair.\n",
      "[00:32:53.080 --> 00:32:55.680]   And we look at the summation of all the future reward.\n",
      "[00:32:55.680 --> 00:33:00.760]   By infinite horizon, that is, we're\n",
      "[00:33:00.760 --> 00:33:02.800]   looking at all the summation of future reward,\n",
      "[00:33:02.800 --> 00:33:05.280]   the summation from the very first step, h equal to 0,\n",
      "[00:33:05.280 --> 00:33:06.160]   or h equal to 1.\n",
      "[00:33:06.160 --> 00:33:07.480]   It doesn't matter much.\n",
      "[00:33:07.480 --> 00:33:10.120]   But we connect with summation over to the infinity,\n",
      "[00:33:10.120 --> 00:33:13.000]   or summation because it's infinite horizon.\n",
      "[00:33:13.000 --> 00:33:15.080]   And of course, for infinite horizon\n",
      "[00:33:15.080 --> 00:33:17.480]   thing, we cannot just summation something constant\n",
      "[00:33:17.480 --> 00:33:19.200]   because that will blow up.\n",
      "[00:33:19.200 --> 00:33:21.600]   So we need to discount the future reward.\n",
      "[00:33:21.600 --> 00:33:25.880]   That is, for any reward happens at h step in the future,\n",
      "[00:33:25.880 --> 00:33:30.360]   we will discount it by gamma to the power of h.\n",
      "[00:33:30.360 --> 00:33:33.560]   And then we can like summation.\n",
      "[00:33:33.560 --> 00:33:36.200]   And times the reward that we receive at h step.\n",
      "[00:33:36.200 --> 00:33:50.800]   Let's say this is 0 equal to minus.\n",
      "[00:33:50.800 --> 00:33:57.760]   And we know if the reward, for example,\n",
      "[00:33:57.760 --> 00:33:59.880]   is bounded in 0 and 1.\n",
      "[00:33:59.880 --> 00:34:01.280]   So the maximum reward you're going\n",
      "[00:34:01.280 --> 00:34:04.240]   to receive is summation gamma to the power of h.\n",
      "[00:34:04.240 --> 00:34:06.280]   And we know this is like geometric series\n",
      "[00:34:06.280 --> 00:34:09.600]   where with this factor, gamma is smaller than 1.\n",
      "[00:34:09.600 --> 00:34:11.280]   So this is clearly--\n",
      "[00:34:11.280 --> 00:34:13.360]   this summation is clearly converged to something.\n",
      "[00:34:13.360 --> 00:34:15.000]   And it won't blow up.\n",
      "[00:34:15.000 --> 00:34:16.400]   So everything is well defined.\n",
      "[00:34:16.400 --> 00:34:29.480]   And I think the beauty of infinite horizon setting\n",
      "[00:34:29.480 --> 00:34:34.880]   is we come possibly talking about a stationary policy,\n",
      "[00:34:34.880 --> 00:34:36.760]   that pi doesn't depends on h.\n",
      "[00:34:36.760 --> 00:34:38.920]   And also, the value doesn't depends on h.\n",
      "[00:34:38.920 --> 00:34:40.960]   So we can just consider everything\n",
      "[00:34:40.960 --> 00:34:43.080]   as something stationary, p is also stationary.\n",
      "[00:34:43.080 --> 00:34:52.920]   So we will first talk about how we're\n",
      "[00:34:52.920 --> 00:34:56.600]   going to relate those two settings.\n",
      "[00:34:56.600 --> 00:34:59.240]   Why we can talk about the episodic setting without lots\n",
      "[00:34:59.240 --> 00:34:59.960]   of generality.\n",
      "[00:34:59.960 --> 00:35:07.280]   The reason is essentially, although this\n",
      "[00:35:07.280 --> 00:35:09.720]   is an infinite horizon, this kind of setting,\n",
      "[00:35:09.720 --> 00:35:14.480]   but we can more or less count the maximum possible cumulative\n",
      "[00:35:14.480 --> 00:35:16.040]   reward after a step hitch.\n",
      "[00:35:16.040 --> 00:35:41.880]   [NON-ENGLISH SPEECH]\n",
      "[00:35:41.880 --> 00:35:45.720]   Since we kind of say this reward is always in 0 and 1\n",
      "[00:35:45.720 --> 00:35:50.000]   to binormalization, so we see this maximum possible reward\n",
      "[00:35:50.000 --> 00:35:55.480]   is after step hitch is basically just a summation,\n",
      "[00:35:55.480 --> 00:36:03.640]   h prime equal to h, 2 infinity, and gamma to the h prime.\n",
      "[00:36:03.640 --> 00:36:05.320]   This is already assuming I always\n",
      "[00:36:05.320 --> 00:36:08.520]   get the maximum reward that is 1 after step hitch.\n",
      "[00:36:08.520 --> 00:36:11.880]   So this is a maximum possible discounted cumulative reward\n",
      "[00:36:11.880 --> 00:36:14.360]   that we can receive.\n",
      "[00:36:14.360 --> 00:36:17.680]   Which we can easily compute this summation\n",
      "[00:36:17.680 --> 00:36:36.380]   is equal to gamma h 1 over gamma over 1.\n",
      "[00:36:36.380 --> 00:36:43.240]   I believe something like that.\n",
      "[00:36:43.240 --> 00:36:49.520]   [NON-ENGLISH SPEECH]\n",
      "[00:36:49.520 --> 00:36:50.960]   OK.\n",
      "[00:36:50.960 --> 00:36:55.200]   So we can clearly see because this gamma is something strictly\n",
      "[00:36:55.200 --> 00:36:58.200]   smaller than 0, or strictly smaller than 1.\n",
      "[00:36:58.200 --> 00:37:01.940]   So this is a cumulative reward after step\n",
      "[00:37:01.940 --> 00:37:04.640]   h is actually geometrically decreasing.\n",
      "[00:37:04.640 --> 00:37:07.240]   So it's decreasing extremely fast.\n",
      "[00:37:07.240 --> 00:37:11.600]   So we can essentially count how many steps--\n",
      "[00:37:11.600 --> 00:37:15.280]   so for example, if I want all the remaining possible cumulative\n",
      "[00:37:15.280 --> 00:37:18.360]   reward, it's smaller than epsilon.\n",
      "[00:37:18.360 --> 00:37:22.240]   It turns out you can essentially solve--\n",
      "[00:37:22.240 --> 00:37:26.120]   solve this as long as h is greater or equal to,\n",
      "[00:37:26.120 --> 00:37:34.480]   up to some constants, 1 over 1 minus gamma log 1 over 1\n",
      "[00:37:34.480 --> 00:37:37.120]   minus gamma times epsilon.\n",
      "[00:37:37.480 --> 00:37:42.460]   [NON-ENGLISH SPEECH]\n",
      "[00:37:42.460 --> 00:37:46.280]   So which, up to some logarithmic factor,\n",
      "[00:37:46.280 --> 00:37:51.120]   is equal to 1 over 1 minus gamma.\n",
      "[00:37:51.120 --> 00:37:52.840]   So this is up to log vector.\n",
      "[00:38:04.520 --> 00:38:09.000]   So what we are seeing is, after this amount of steps\n",
      "[00:38:09.000 --> 00:38:11.840]   in the infinite horizon setting, all the reward\n",
      "[00:38:11.840 --> 00:38:15.240]   summation together is very, very small, so epsilon.\n",
      "[00:38:15.240 --> 00:38:17.320]   And in a lot of scenario, like in the learning scenario,\n",
      "[00:38:17.320 --> 00:38:19.520]   we don't really care about this small difference epsilon\n",
      "[00:38:19.520 --> 00:38:21.400]   because we have statistical noise.\n",
      "[00:38:21.400 --> 00:38:24.320]   And in that kind of scenario, basically, we don't care.\n",
      "[00:38:24.320 --> 00:38:27.520]   And so what we're seeing is this infinite horizon\n",
      "[00:38:27.520 --> 00:38:31.320]   can be also viewed as some finite horizon setting,\n",
      "[00:38:31.320 --> 00:38:34.560]   where this is essentially the effective horizon.\n",
      "[00:38:34.560 --> 00:38:37.540]   [NON-ENGLISH SPEECH]\n",
      "[00:38:37.540 --> 00:38:41.500]   [NON-ENGLISH SPEECH]\n",
      "[00:38:41.500 --> 00:38:44.000]   [NON-ENGLISH SPEECH]\n",
      "[00:39:11.300 --> 00:39:27.500]   So what we are seeing is, up to a small arrow,\n",
      "[00:39:27.500 --> 00:39:44.140]   infinite horizon, discounted setting, can be treated\n",
      "[00:39:44.140 --> 00:40:05.740]   as a finite horizon, with effective--\n",
      "[00:40:05.740 --> 00:40:15.900]   [NON-ENGLISH SPEECH]\n",
      "[00:40:15.900 --> 00:40:22.900]   That is, h theta equal to o theta over 1 minus gamma.\n",
      "[00:40:22.900 --> 00:40:43.260]   So this basically set up the connection\n",
      "[00:40:43.260 --> 00:40:45.860]   between this infinite horizon and the finite horizon.\n",
      "[00:40:45.860 --> 00:40:50.500]   That is, at least, when we get the result of the finite horizon\n",
      "[00:40:50.500 --> 00:40:53.380]   setting, and up to some arrow, we can transfer the result\n",
      "[00:40:53.380 --> 00:40:54.460]   to the infinite horizon setting.\n",
      "[00:40:54.460 --> 00:41:00.900]   So the second thing is, why are we\n",
      "[00:41:00.900 --> 00:41:05.060]   talking about a finite horizon setting in this course?\n",
      "[00:41:05.060 --> 00:41:06.740]   I think it will be clear if we look\n",
      "[00:41:06.740 --> 00:41:11.860]   at the Bell-Mau automatic equation there, Bell-Mau\n",
      "[00:41:11.860 --> 00:41:13.620]   equation or Bell-Mau automatic equation.\n",
      "[00:41:13.620 --> 00:41:15.980]   We just look at here.\n",
      "[00:41:15.980 --> 00:41:34.100]   So Bell-Mau automatic equation in that kind of case\n",
      "[00:41:34.100 --> 00:41:41.300]   is like Q-star SA is equal to RSA plus.\n",
      "[00:41:41.300 --> 00:41:44.300]   Now we need to discount the value at the next situation.\n",
      "[00:41:44.300 --> 00:41:49.060]   So we're adding a gamma here, and then we're doing a pH,\n",
      "[00:41:49.060 --> 00:41:59.020]   and then we have a V-star SA.\n",
      "[00:41:59.020 --> 00:42:08.980]   And then V-star SA is equal to max A Q-star SA.\n",
      "[00:42:08.980 --> 00:42:09.980]   That's it.\n",
      "[00:42:09.980 --> 00:42:34.500]   So essentially the Bell-Mau channel equation\n",
      "[00:42:34.500 --> 00:42:37.780]   is we just drop the in-ax H, but we\n",
      "[00:42:37.780 --> 00:42:41.700]   have this kind of factor gamma here.\n",
      "[00:42:41.700 --> 00:42:45.220]   So then we can talk about the value iteration.\n",
      "[00:42:45.220 --> 00:42:47.460]   So we just are calling the episodic setting.\n",
      "[00:42:47.460 --> 00:42:53.580]   The value iteration essentially is\n",
      "[00:42:53.580 --> 00:42:56.220]   a random Bell-Mau automatic equation\n",
      "[00:42:56.220 --> 00:42:58.340]   by the number of programming, which\n",
      "[00:42:58.340 --> 00:43:02.780]   we run this equation from H equal to capital H to 1.\n",
      "[00:43:05.980 --> 00:43:17.740]   Well, in the infinite horizon setting,\n",
      "[00:43:17.740 --> 00:43:19.540]   there are essentially two possible ways.\n",
      "[00:43:19.540 --> 00:43:21.340]   One is that we can do this truncation,\n",
      "[00:43:21.340 --> 00:43:23.660]   and we can convert it to episodic setting.\n",
      "[00:43:23.660 --> 00:43:25.580]   And where we essentially need to consider\n",
      "[00:43:25.580 --> 00:43:28.980]   a non-stationary reward and a non-non-stationary policy\n",
      "[00:43:28.980 --> 00:43:30.820]   and a value again, and then everything\n",
      "[00:43:30.820 --> 00:43:32.380]   go back to the first setting.\n",
      "[00:43:32.380 --> 00:43:35.300]   Or we can directly use the stationary treatment\n",
      "[00:43:35.300 --> 00:43:38.620]   where instead of doing the normal programming from capital H\n",
      "[00:43:38.620 --> 00:43:41.060]   to 1, because there is no finite horizon, what\n",
      "[00:43:41.060 --> 00:43:43.940]   you can do is so-called like a fixed point iteration.\n",
      "[00:43:43.940 --> 00:43:55.500]   I would just intuitively explain what this is about.\n",
      "[00:43:55.500 --> 00:43:59.020]   So essentially, we run this Bell-Mau equation.\n",
      "[00:43:59.020 --> 00:44:03.100]   Like we do-- we have V-star, we compute the Q-star,\n",
      "[00:44:03.100 --> 00:44:04.860]   and we have Q-star, we compute the V-star,\n",
      "[00:44:04.860 --> 00:44:07.020]   and we repeat the process.\n",
      "[00:44:07.020 --> 00:44:08.460]   And by fixed point iteration means\n",
      "[00:44:08.460 --> 00:44:11.300]   like we were guaranteed after some iteration,\n",
      "[00:44:11.300 --> 00:44:14.060]   this process will converge to some value.\n",
      "[00:44:14.060 --> 00:44:16.820]   Because we use Q-star to update V-star--\n",
      "[00:44:16.820 --> 00:44:19.060]   V-star to update Q-star, Q-star to update V-star,\n",
      "[00:44:19.060 --> 00:44:21.420]   we might go back and forth in the very beginning,\n",
      "[00:44:21.420 --> 00:44:24.380]   and then eventually we're going to converge to something.\n",
      "[00:44:24.380 --> 00:44:25.860]   OK?\n",
      "[00:44:25.860 --> 00:44:28.700]   And this is essentially how equivalent\n",
      "[00:44:28.700 --> 00:44:31.100]   of this general programming in the infinite horizon setting,\n",
      "[00:44:31.100 --> 00:44:33.580]   I personally feel it's a little bit more complicated\n",
      "[00:44:33.580 --> 00:44:35.260]   to analyze that kind of thing.\n",
      "[00:44:35.260 --> 00:44:37.260]   And later, we have a more complicated setting,\n",
      "[00:44:37.260 --> 00:44:41.500]   like a function approximation or like a multi-agent scenario.\n",
      "[00:44:41.500 --> 00:44:43.740]   And by looking at this dynamic programming,\n",
      "[00:44:43.740 --> 00:44:46.580]   it's much clearer what everything is happening.\n",
      "[00:44:46.580 --> 00:44:48.460]   And it turns out a full fixed point iteration,\n",
      "[00:44:48.460 --> 00:44:56.300]   you can basically say it's up to some arrow epsilon.\n",
      "[00:44:56.300 --> 00:45:05.740]   So we will converge in some h-tilde effective horizon steps.\n",
      "[00:45:05.740 --> 00:45:14.700]   So if we run this fixed point iteration for h-tilde steps,\n",
      "[00:45:14.700 --> 00:45:16.540]   and eventually we will converge to something\n",
      "[00:45:16.540 --> 00:45:18.860]   up to some small epsilon, small epsilon arrow,\n",
      "[00:45:18.860 --> 00:45:21.860]   which again, intuitions like similar to dynamic programming.\n",
      "[00:45:22.580 --> 00:45:23.100]   OK.\n",
      "[00:45:23.100 --> 00:45:38.460]   Any questions about this part so far?\n",
      "[00:45:38.460 --> 00:45:41.380]   I guess I will just quickly go over the difference\n",
      "[00:45:41.380 --> 00:45:43.820]   between a similarity and difference\n",
      "[00:45:43.820 --> 00:45:47.020]   between this episodic setting and infinite horizon setting.\n",
      "[00:45:47.020 --> 00:45:48.500]   And for the later course, we will still\n",
      "[00:45:48.500 --> 00:45:52.260]   focus on an episodic setting.\n",
      "[00:45:52.260 --> 00:45:54.260]   [VIDEO PLAYBACK]\n",
      "[00:46:19.980 --> 00:46:24.540]   OK, so this finish, essentially, where\n",
      "[00:46:24.540 --> 00:46:27.180]   we want to talk about the planning.\n",
      "[00:46:27.180 --> 00:46:30.660]   Let's just recall the setting of planning.\n",
      "[00:46:30.660 --> 00:46:36.500]   Planning is so we know the environment that\n",
      "[00:46:36.500 --> 00:46:38.260]   is transitioning on the reward, and we\n",
      "[00:46:38.260 --> 00:46:40.620]   want to compute the pi star, optimal policy.\n",
      "[00:46:40.620 --> 00:46:46.380]   And in this setting, basically everything is already down.\n",
      "[00:46:46.380 --> 00:46:49.900]   And we have reasonably good solution.\n",
      "[00:46:49.900 --> 00:46:53.460]   And however, this is essentially why we call planning.\n",
      "[00:46:53.460 --> 00:46:57.660]   Well, the name of this course is called reinforcement learning.\n",
      "[00:46:57.660 --> 00:47:00.420]   So we still need to do the learning component.\n",
      "[00:47:00.420 --> 00:47:03.100]   And by doing learning, I think a lot of people\n",
      "[00:47:03.100 --> 00:47:06.940]   understand learning or define learning as we want to learn\n",
      "[00:47:06.940 --> 00:47:10.700]   optimal policy by interactions through the unknown environment,\n",
      "[00:47:10.700 --> 00:47:12.980]   by exploring the unknown environment.\n",
      "[00:47:12.980 --> 00:47:14.740]   So the very important thing is this\n",
      "[00:47:14.740 --> 00:47:19.100]   is, in practice, a lot of time this environment is unknown.\n",
      "[00:47:19.100 --> 00:47:30.460]   As a baby, we want to learn how to work,\n",
      "[00:47:30.460 --> 00:47:33.020]   or how to do some special skills.\n",
      "[00:47:33.020 --> 00:47:36.380]   We essentially explore this word, which is unknown to the baby,\n",
      "[00:47:36.380 --> 00:47:38.100]   or to a lot of people.\n",
      "[00:47:38.100 --> 00:47:41.420]   And in that kind of scenario, we need to do some learning.\n",
      "[00:47:41.820 --> 00:47:48.820]   By learning, we essentially means\n",
      "[00:47:48.820 --> 00:47:55.140]   we want to estimate both transition and reward using samples.\n",
      "[00:47:55.140 --> 00:48:06.540]   That is, I don't know the transition probability,\n",
      "[00:48:06.540 --> 00:48:09.500]   but I like visiting this state multiple times\n",
      "[00:48:09.500 --> 00:48:11.740]   and see what it states the transition to,\n",
      "[00:48:11.740 --> 00:48:13.620]   then what the next state is a transition to.\n",
      "[00:48:13.620 --> 00:48:16.380]   And we will essentially use some statistical tools\n",
      "[00:48:16.380 --> 00:48:20.980]   to estimate what is the transition probability and reward.\n",
      "[00:48:20.980 --> 00:48:23.380]   So when we talk about this learning setup,\n",
      "[00:48:23.380 --> 00:48:28.100]   we inevitably need to introduce some statistical tools\n",
      "[00:48:28.100 --> 00:48:33.180]   to talk about this, which essentially will\n",
      "[00:48:33.180 --> 00:48:36.820]   be the next topic that we're going to go over.\n",
      "[00:48:36.820 --> 00:48:41.220]   So let's first look at what kind of statistical questions\n",
      "[00:48:41.220 --> 00:48:42.220]   will be faced at.\n",
      "[00:48:42.220 --> 00:48:51.620]   Let's consider a very simple example.\n",
      "[00:49:00.900 --> 00:49:17.980]   The simple example is we can consider we want to estimate\n",
      "[00:49:17.980 --> 00:49:19.260]   the transition probability.\n",
      "[00:49:19.860 --> 00:49:23.500]   So we can consider, for example, this new state,\n",
      "[00:49:23.500 --> 00:49:31.620]   so we only have two possible new states.\n",
      "[00:49:31.620 --> 00:49:33.780]   And we are only looking at the probability\n",
      "[00:49:33.780 --> 00:49:35.420]   that we have two new states.\n",
      "[00:49:35.420 --> 00:49:42.900]   So a very special case is we can consider,\n",
      "[00:49:42.900 --> 00:49:45.660]   for example, this new state, we only have two possible new\n",
      "[00:49:45.660 --> 00:49:47.660]   states.\n",
      "[00:49:47.660 --> 00:49:50.740]   And we are only looking at the probability of starting\n",
      "[00:49:50.740 --> 00:49:57.980]   from one specific assay pair, like one fixed assay pair,\n",
      "[00:49:57.980 --> 00:50:00.020]   to two possible new state S prime.\n",
      "[00:50:15.380 --> 00:50:19.020]   This kind of problem is basically the same as, for example,\n",
      "[00:50:19.020 --> 00:50:23.780]   I want to look at some coin toes problem,\n",
      "[00:50:23.780 --> 00:50:25.700]   because we're starting from some fixed assay pair.\n",
      "[00:50:25.700 --> 00:50:28.220]   So let's just say that we only have one assay pair.\n",
      "[00:50:28.220 --> 00:50:31.180]   And we're looking at the transition to two new states.\n",
      "[00:50:31.180 --> 00:50:33.860]   And we're essentially similar to a coin toes,\n",
      "[00:50:33.860 --> 00:50:37.100]   where every time the new states has two possibilities.\n",
      "[00:50:37.100 --> 00:50:39.740]   Let's just talk about the coin toes here.\n",
      "[00:50:39.740 --> 00:50:41.980]   We have either hat or tail.\n",
      "[00:50:44.980 --> 00:50:47.300]   And let's say, for hat, we know in reality,\n",
      "[00:50:47.300 --> 00:50:49.020]   it's like with probability p.\n",
      "[00:50:49.020 --> 00:50:51.340]   And with probability for equal to tail,\n",
      "[00:50:51.340 --> 00:50:53.220]   it's probability with 1 minus p.\n",
      "[00:50:53.220 --> 00:50:55.580]   So this is like Bernoulli.\n",
      "[00:50:55.580 --> 00:50:59.420]   This is like Bernoulli random variable, with parameter p.\n",
      "[00:50:59.420 --> 00:51:13.780]   The question is here, we don't really know the parameter p.\n",
      "[00:51:14.580 --> 00:51:28.820]   We want to estimate it by looking at a lot of samples.\n",
      "[00:51:28.820 --> 00:51:33.180]   That is, x1, x2, and a lot of tail xn.\n",
      "[00:51:33.180 --> 00:51:38.620]   We will do an independent draw out of this Bernoulli random\n",
      "[00:51:38.620 --> 00:51:39.060]   variable.\n",
      "[00:51:39.060 --> 00:51:43.180]   And we want to use x1 to xn to estimate the parameter p.\n",
      "[00:51:43.180 --> 00:51:45.980]   Essentially, this is a problem of learning the environment,\n",
      "[00:51:45.980 --> 00:51:49.580]   where we look at many independent draw of the next states.\n",
      "[00:51:49.580 --> 00:51:52.100]   And we want to estimate what is probability to visit a state y,\n",
      "[00:51:52.100 --> 00:51:54.100]   and what is probability to visit a state 2.\n",
      "[00:51:54.100 --> 00:51:57.080]   [INAUDIBLE]\n",
      "[00:52:23.340 --> 00:52:39.020]   OK, and so we know by law of large number,\n",
      "[00:52:39.020 --> 00:52:43.740]   we know the average 1 over n summation i from 1 to n xi.\n",
      "[00:52:43.740 --> 00:52:52.500]   Let's call this hat is equal to a value of 1.\n",
      "[00:52:52.500 --> 00:52:59.140]   This tail is equal to like a equal to a value of 0.\n",
      "[00:52:59.140 --> 00:53:06.660]   And we know the summation of xi and taking average divided\n",
      "[00:53:06.660 --> 00:53:12.460]   by n is going to converge to this p as n go to infinity.\n",
      "[00:53:12.460 --> 00:53:19.420]   And for those of you who have learned the probability class,\n",
      "[00:53:19.420 --> 00:53:24.580]   we know this convergence is called almost surely convergence.\n",
      "[00:53:24.580 --> 00:53:29.180]   If you don't know, that doesn't matter,\n",
      "[00:53:29.180 --> 00:53:31.700]   almost surely convergence is with probability 1.\n",
      "[00:53:31.700 --> 00:53:44.700]   So this somehow says some phenomena\n",
      "[00:53:44.700 --> 00:53:46.980]   like when we have infinite samples.\n",
      "[00:53:46.980 --> 00:53:49.260]   However, to a learning community,\n",
      "[00:53:49.260 --> 00:53:51.460]   this might not be that satisfactory.\n",
      "[00:53:51.460 --> 00:53:54.580]   Because this essentially only tells you when n goes to infinity,\n",
      "[00:53:54.580 --> 00:53:57.220]   you don't really know how large is enough.\n",
      "[00:53:57.220 --> 00:54:00.180]   For example, whether a million sample is enough,\n",
      "[00:54:00.180 --> 00:54:01.740]   or whether a billion sample is enough,\n",
      "[00:54:01.740 --> 00:54:03.900]   like when this convergence is going to happen,\n",
      "[00:54:03.900 --> 00:54:08.380]   that is quite unknown by this law of large number.\n",
      "[00:54:08.380 --> 00:54:11.860]   So for this class, the learning theory course,\n",
      "[00:54:11.860 --> 00:54:13.940]   we care a lot about sample complexity,\n",
      "[00:54:13.940 --> 00:54:16.540]   because in practice, we never have like this infinite number\n",
      "[00:54:16.540 --> 00:54:17.380]   of samples.\n",
      "[00:54:17.380 --> 00:54:21.180]   So we really want to know what kind of sample complexity\n",
      "[00:54:21.180 --> 00:54:25.420]   or how many samples we need so that this estimation can happen\n",
      "[00:54:25.420 --> 00:54:28.100]   and so that we can accurately learn some transitions,\n",
      "[00:54:28.100 --> 00:54:31.180]   so that we can accurately learn some optimal policy.\n",
      "[00:54:31.180 --> 00:54:35.660]   So this is why we need to introduce a better tool.\n",
      "[00:54:35.660 --> 00:54:37.860]   So a sharper tool, OK, we can see.\n",
      "[00:54:37.860 --> 00:54:43.700]   Essentially, we need to not only say this will converge\n",
      "[00:54:43.700 --> 00:54:46.420]   to the parameter, like almost surely.\n",
      "[00:54:46.420 --> 00:54:55.860]   We want to have something so that we can say the 1 over n,\n",
      "[00:54:55.860 --> 00:55:02.860]   this average, summation from 1 to n xi, subtract by p.\n",
      "[00:55:02.860 --> 00:55:06.460]   We want to say this difference is upper bounded\n",
      "[00:55:06.460 --> 00:55:07.700]   by some function of n.\n",
      "[00:55:08.580 --> 00:55:17.700]   So hopefully we can say this n, this dysfunction is something\n",
      "[00:55:17.700 --> 00:55:22.220]   like a scale as 1 over square root of n, or 1 over n,\n",
      "[00:55:22.220 --> 00:55:24.900]   or something basically decreased with n.\n",
      "[00:55:24.900 --> 00:55:26.780]   So in that case, we're in some good shape,\n",
      "[00:55:26.780 --> 00:55:29.340]   because we can see when n goes to infinity,\n",
      "[00:55:29.340 --> 00:55:32.140]   like that is in a large sample scenario,\n",
      "[00:55:32.140 --> 00:55:35.940]   we're guaranteed this difference will be smaller and smaller,\n",
      "[00:55:35.940 --> 00:55:38.820]   and we also not only have this a sympathetic guarantee,\n",
      "[00:55:38.820 --> 00:55:40.580]   we have also non-sympathetic guarantee\n",
      "[00:55:40.580 --> 00:55:44.180]   that we can actually say for this less than epsilon,\n",
      "[00:55:44.180 --> 00:55:47.020]   we only need this number of n greater than something\n",
      "[00:55:47.020 --> 00:55:48.780]   depends on epsilon.\n",
      "[00:55:48.780 --> 00:55:53.620]   So this is what we want to develop over this class,\n",
      "[00:55:53.620 --> 00:55:56.340]   or maybe a bit of next class.\n",
      "[00:55:56.340 --> 00:55:59.500]   And for those who are familiar with probability,\n",
      "[00:55:59.500 --> 00:56:01.940]   this tool is called concentration inequality.\n",
      "[00:56:01.940 --> 00:56:18.660]   So it will be essentially the single most important mathematical\n",
      "[00:56:18.660 --> 00:56:22.340]   tool we're going to use when we analyze any learning algorithm.\n",
      "[00:56:22.340 --> 00:56:46.940]   OK, so this goes to the second part of the lecture.\n",
      "[00:56:46.940 --> 00:56:48.900]   We'll talk about the concentration inequalities.\n",
      "[00:56:48.900 --> 00:57:08.220]   So there are a lot of different concentration inequalities,\n",
      "[00:57:08.220 --> 00:57:12.420]   and we will only introduce a few concentration inequalities\n",
      "[00:57:12.420 --> 00:57:14.980]   that is most relevant to this course.\n",
      "[00:57:14.980 --> 00:57:16.980]   And essentially, we introduced four of them,\n",
      "[00:57:16.980 --> 00:57:19.140]   but three of them are kind of extension.\n",
      "[00:57:19.140 --> 00:57:24.060]   We will only briefly talk about what is the statement\n",
      "[00:57:24.060 --> 00:57:27.700]   and how we derive them from the original one,\n",
      "[00:57:27.700 --> 00:57:30.340]   and we will focus on the original one today.\n",
      "[00:57:30.340 --> 00:57:33.380]   So before we talk about this concentration inequality,\n",
      "[00:57:33.380 --> 00:57:37.180]   we first want to introduce a bit of notion, no notation,\n",
      "[00:57:37.180 --> 00:57:39.580]   so that we can facilitate our discussion.\n",
      "[00:57:39.580 --> 00:57:53.940]   So we'll talk about some big old notation.\n",
      "[00:57:53.940 --> 00:57:55.620]   So I think to introduce a big old notation,\n",
      "[00:57:55.620 --> 00:57:58.340]   because a lot of times, if we want\n",
      "[00:57:58.340 --> 00:58:01.540]   to do all the mathematical very rigorous that I'm talking about,\n",
      "[00:58:01.540 --> 00:58:05.140]   we talk very precisely about what is a constant like it's two or four,\n",
      "[00:58:05.140 --> 00:58:06.380]   we need a lot of computation.\n",
      "[00:58:06.380 --> 00:58:09.620]   And also, of course, a little bit of a big picture.\n",
      "[00:58:09.620 --> 00:58:12.140]   So a lot of scenario, I think we don't really\n",
      "[00:58:12.140 --> 00:58:16.060]   care about some precise constant, whether it's two or four,\n",
      "[00:58:16.060 --> 00:58:18.100]   or like 10 or something.\n",
      "[00:58:18.100 --> 00:58:19.740]   So we will use some big old notation\n",
      "[00:58:19.740 --> 00:58:22.100]   to hide the constant dependence, or sometimes even\n",
      "[00:58:22.100 --> 00:58:23.780]   the log dependence.\n",
      "[00:58:23.780 --> 00:58:25.980]   So we will essentially have a few notation that\n",
      "[00:58:25.980 --> 00:58:31.500]   is like big old, little, and this h.\n",
      "[00:58:31.500 --> 00:58:37.820]   And we also have omega dependence, like a capital omega, little omega.\n",
      "[00:58:37.820 --> 00:58:42.300]   And we will also have another set of notation that is old tilde.\n",
      "[00:58:42.300 --> 00:58:44.260]   This is only hiding the constant factor.\n",
      "[00:58:44.260 --> 00:58:46.140]   This is hiding the log factor.\n",
      "[00:58:46.140 --> 00:58:50.300]   We'll talk about rigorous definition here.\n",
      "[00:58:50.300 --> 00:58:53.580]   So basically, those set of notation, like tilde version,\n",
      "[00:58:53.580 --> 00:58:55.900]   non-tilde version, tilde is about hiding the log factor.\n",
      "[00:58:55.900 --> 00:58:57.220]   This doesn't have the log factor.\n",
      "[00:58:57.220 --> 00:58:59.260]   This only has a constant factor.\n",
      "[00:58:59.260 --> 00:59:01.300]   And always, essentially, less or equal to,\n",
      "[00:59:01.300 --> 00:59:04.100]   theta is equal to, and omega is greater or equal to.\n",
      "[00:59:04.100 --> 00:59:06.580]   And we will talk about the rigorous definition.\n",
      "[00:59:06.580 --> 00:59:13.020]   [INAUDIBLE]\n",
      "[00:59:13.020 --> 00:59:16.540]   The question is, the transition probability is stable over the time.\n",
      "[00:59:16.540 --> 00:59:20.180]   You're talking about whether pH depends on h.\n",
      "[00:59:20.180 --> 00:59:21.420]   Is that the question?\n",
      "[00:59:21.420 --> 00:59:25.180]   So in our episodic formulation, it doesn't necessarily\n",
      "[00:59:25.180 --> 00:59:26.860]   need to be stable over the time.\n",
      "[00:59:26.860 --> 00:59:29.700]   [INAUDIBLE]\n",
      "[00:59:29.700 --> 00:59:39.180]   So I think the way we do the learning\n",
      "[00:59:39.180 --> 00:59:42.100]   is we will play the game for multiple episodes.\n",
      "[00:59:42.100 --> 00:59:45.140]   So essentially, we'll play from the beginning and do h step.\n",
      "[00:59:45.140 --> 00:59:46.700]   That is one episode of the game.\n",
      "[00:59:46.700 --> 00:59:51.340]   Essentially, you imagine you learn to play super Mario\n",
      "[00:59:51.340 --> 00:59:53.580]   by not just playing it once.\n",
      "[00:59:53.580 --> 00:59:57.020]   You play it for 1,000 times, and you're going to learn it.\n",
      "[00:59:57.020 --> 00:59:59.460]   [INAUDIBLE]\n",
      "[00:59:59.460 --> 01:00:10.180]   OK, so I think what we are--\n",
      "[01:00:10.180 --> 01:00:13.780]   so the question is, essentially, we say p1.\n",
      "[01:00:13.780 --> 01:00:19.020]   We have p1, p2, and that are pH.\n",
      "[01:00:19.020 --> 01:00:23.460]   And this can be all different in episodic setting.\n",
      "[01:00:23.460 --> 01:00:26.260]   And the way we're going to learn this is, for example,\n",
      "[01:00:26.260 --> 01:00:28.860]   we play the first game of super Mario,\n",
      "[01:00:28.860 --> 01:00:33.340]   and we collect one sample, one trajectory, s1, a1, s2,\n",
      "[01:00:33.340 --> 01:00:35.940]   and dot a lot to your SHH.\n",
      "[01:00:35.940 --> 01:00:38.100]   This is like first trajectory.\n",
      "[01:00:38.100 --> 01:00:40.820]   And then we go through the second trajectory.\n",
      "[01:00:40.820 --> 01:00:46.460]   That is another s1, a1, s2, a2, dot a lot to your SHH.\n",
      "[01:00:46.460 --> 01:00:48.500]   This is like when I play the second game.\n",
      "[01:00:52.180 --> 01:00:54.460]   So in both trajectory--\n",
      "[01:00:54.460 --> 01:00:56.620]   and we can have multiple trajectories.\n",
      "[01:00:56.620 --> 01:00:59.420]   At different trajectories, those transition probability\n",
      "[01:00:59.420 --> 01:01:00.820]   will be the same.\n",
      "[01:01:00.820 --> 01:01:02.660]   So this will be the same transition probability\n",
      "[01:01:02.660 --> 01:01:06.220]   happening at the very last step of both trajectories.\n",
      "[01:01:06.220 --> 01:01:10.500]   [INAUDIBLE]\n",
      "[01:01:10.500 --> 01:01:12.660]   If we cannot restart the game, then we cannot learn it.\n",
      "[01:01:12.660 --> 01:01:14.340]   Of course, if we only have one trajectory,\n",
      "[01:01:14.340 --> 01:01:17.660]   then there is no way to estimate the transition that's true.\n",
      "[01:01:17.660 --> 01:01:19.220]   So in order to do statistical estimation,\n",
      "[01:01:19.220 --> 01:01:23.140]   you have to have multiple examples.\n",
      "[01:01:23.140 --> 01:01:25.940]   I think there are no settings.\n",
      "[01:01:25.940 --> 01:01:27.300]   You can use one sample to learn.\n",
      "[01:01:27.300 --> 01:01:30.460]   This is that it's impossible.\n",
      "[01:01:30.460 --> 01:01:37.140]   [INAUDIBLE]\n",
      "[01:01:37.140 --> 01:01:38.140]   Yes.\n",
      "[01:01:38.140 --> 01:01:39.500]   I think in that kind of scenario,\n",
      "[01:01:39.500 --> 01:01:42.060]   you need to assume there is some correlation\n",
      "[01:01:42.060 --> 01:01:44.180]   between this transition.\n",
      "[01:01:44.180 --> 01:01:46.740]   I think we won't talk about this in the class,\n",
      "[01:01:46.740 --> 01:01:51.060]   but essentially, people also study some out-of-the-zero\n",
      "[01:01:51.060 --> 01:01:53.620]   change of the P end in the setting.\n",
      "[01:01:53.620 --> 01:01:56.220]   When you say, if you only visited once,\n",
      "[01:01:56.220 --> 01:01:58.540]   then basically there are a lot of impossibility results.\n",
      "[01:01:58.540 --> 01:02:03.300]   You cannot say anything reasonable if P is changing\n",
      "[01:02:03.300 --> 01:02:04.820]   arbitrarily in the worst case.\n",
      "[01:02:04.820 --> 01:02:07.140]   You have to say it changes smoothly\n",
      "[01:02:07.140 --> 01:02:09.940]   or it has some correlation over the time\n",
      "[01:02:09.940 --> 01:02:11.260]   so that you can do the prediction.\n",
      "[01:02:11.260 --> 01:02:14.780]   For example, in a financial market,\n",
      "[01:02:14.780 --> 01:02:19.420]   you can say this is similar to the past history of some part.\n",
      "[01:02:19.420 --> 01:02:21.380]   We essentially give you some human prior\n",
      "[01:02:21.380 --> 01:02:24.020]   to establishing this transition similar to that kind\n",
      "[01:02:24.020 --> 01:02:25.620]   of transition in the past.\n",
      "[01:02:25.620 --> 01:02:27.980]   You cannot see anything strong if you say\n",
      "[01:02:27.980 --> 01:02:29.620]   there can be arbitrarily change.\n",
      "[01:02:29.620 --> 01:02:33.260]   And I only have one sample that's just not possible.\n",
      "[01:02:33.260 --> 01:02:33.980]   That's a good question.\n",
      "[01:02:33.980 --> 01:02:56.980]   [NOISE]\n",
      "[01:02:56.980 --> 01:02:58.980]   OK.\n",
      "[01:02:58.980 --> 01:03:03.780]   So, regular say, we will say we define the big O notation.\n",
      "[01:03:03.780 --> 01:03:19.380]   [NOISE]\n",
      "[01:03:19.380 --> 01:03:28.300]   We say that GX is less or equal to some capital\n",
      "[01:03:28.300 --> 01:03:36.740]   big O of Fx, where for generality we say this X is a vector in Rd.\n",
      "[01:03:36.740 --> 01:03:40.220]   [NOISE]\n",
      "[01:03:40.220 --> 01:03:41.420]   So this is not a scalar.\n",
      "[01:03:41.420 --> 01:03:44.460]   This is a vector in Rd.\n",
      "[01:03:44.460 --> 01:03:48.740]   We say this GX is a little of Fx if following happens.\n",
      "[01:03:48.740 --> 01:03:53.900]   That is, there exists some constant C that is greater or equal to 0.\n",
      "[01:03:53.900 --> 01:03:56.740]   And there exists also at a very large number.\n",
      "[01:03:56.740 --> 01:03:58.900]   And greater or equal to 0.\n",
      "[01:03:58.900 --> 01:04:09.340]   So that for any X, such that the smallest coordinate of X\n",
      "[01:04:09.340 --> 01:04:20.820]   is greater than n, then GX is less or equal to some constant times CX.\n",
      "[01:04:20.820 --> 01:04:22.860]   So informally, you can think this capital O is just\n",
      "[01:04:22.860 --> 01:04:25.820]   high to some constant factor C. Basically, this is saying\n",
      "[01:04:25.820 --> 01:04:31.020]   when X is a large enough with any coordinate of X is a large enough,\n",
      "[01:04:31.020 --> 01:04:35.500]   then we will have the relation that GX is less or equal to some constant times CX.\n",
      "[01:04:35.500 --> 01:04:40.060]   You can just enjoy, I think, this high some constant here.\n",
      "[01:04:40.060 --> 01:04:44.860]   And similarly, we'll also define some [NOISE]\n",
      "[01:04:44.860 --> 01:04:57.460]   tilde notation. Essentially, we will say if GX is less or\n",
      "[01:04:57.460 --> 01:05:12.900]   equal to bigger or tilde of Fx, if there exists some constant C\n",
      "[01:05:12.900 --> 01:05:25.860]   between 0 and for n large enough. And also, there exists some polynomial P\n",
      "[01:05:25.860 --> 01:05:36.500]   so that for any X, again for any X, so that the minimum of a coordinate\n",
      "[01:05:36.500 --> 01:05:44.980]   is greater or equal to n. That is a large enough. And we have GX less or equal to C\n",
      "[01:05:44.980 --> 01:05:56.300]   times Fx and times some polynomial times log Fx. So again, you can just think this\n",
      "[01:05:56.300 --> 01:06:01.140]   O tilde is a notation where high is not only the constant, but also some logarithmic\n",
      "[01:06:01.140 --> 01:06:25.300]   factor. And everything happens when X is large enough.\n",
      "[01:06:25.300 --> 01:06:39.780]   And similarly, we will define the little notation. Little essentially, we say GX\n",
      "[01:06:39.780 --> 01:06:47.340]   is less or equal to little of Fx. The following happens.\n",
      "[01:06:47.340 --> 01:06:51.220]   I think the intuitive understanding of little versus bigger or bigger or smaller than some\n",
      "[01:06:51.220 --> 01:06:56.260]   constant, but probably can be similar order. They are just up to some constant difference.\n",
      "[01:06:56.260 --> 01:07:01.980]   Little essentially, GX is strictly smaller than Fx when X is like large enough. So how\n",
      "[01:07:01.980 --> 01:07:08.260]   we describe it, essentially, we will say for any constant C, no matter how small it is,\n",
      "[01:07:08.260 --> 01:07:15.300]   there exists some n so that n when n is large enough, that for any X such that the\n",
      "[01:07:15.300 --> 01:07:29.020]   minimum I of xi is greater or equal to n, we have GX smaller than constant times CX.\n",
      "[01:07:29.020 --> 01:07:32.940]   So the major difference now is we are not saying there exists some constant. For example,\n",
      "[01:07:32.940 --> 01:07:36.980]   exists some constant can be like 2 or 4. But this is saying for any constant, no matter\n",
      "[01:07:36.980 --> 01:07:42.420]   how small it is, it can be like 0.1, it can be 0.001, it can be like a 10 to the minus\n",
      "[01:07:42.420 --> 01:07:47.900]   6 or something. But this is saying no matter how small a constant is, as long as I make\n",
      "[01:07:47.900 --> 01:07:54.300]   my X to be large enough, I can always make this GX smaller than some constant times Fx.\n",
      "[01:07:54.300 --> 01:08:00.780]   So this is much smaller than the previous big old case. So we can think this will directly\n",
      "[01:08:00.780 --> 01:08:23.060]   implies that GX over Fx goes through 0 for large X. So for x large enough, we can say\n",
      "[01:08:23.060 --> 01:08:34.700]   the ratio will eventually go through 0. So this is much smaller than the big old notation.\n",
      "[01:08:34.700 --> 01:08:39.900]   So we have already essentially, you can also say this little O is like also the log, parilog\n",
      "[01:08:39.900 --> 01:08:44.820]   addition has a parilog addition of parilog vector. So we have introduced the first set\n",
      "[01:08:44.820 --> 01:08:52.380]   of notation. So the remaining set is essentially we can just define it by capital O or little.\n",
      "[01:08:52.380 --> 01:09:04.060]   So we say Fx is greater or equal to omega GX. This is essentially large up to some constant.\n",
      "[01:09:04.060 --> 01:09:18.460]   This is equivalent to GX is smaller than the capital O, bigger of Fx. So bigger is less\n",
      "[01:09:18.460 --> 01:09:24.420]   or equal to up to some constant. Big old omega is greater or equal to up to some constant.\n",
      "[01:09:24.420 --> 01:09:35.940]   And finally, we see Fx is equal to theta GX if they are basically equal up to some constant\n",
      "[01:09:35.940 --> 01:09:47.220]   factor. So this is equivalent to say Fx is not only less than bigger of GX, but also\n",
      "[01:09:47.220 --> 01:09:51.220]   greater than bigger omega of GX.\n",
      "[01:09:51.220 --> 01:09:58.220]   So, we have a question. So, we have a question. So, we have a question. So, we have a question.\n",
      "[01:09:58.220 --> 01:10:05.220]   So, we have a question. So, we have a question. So, we have a question. So, we have a question.\n",
      "[01:10:05.220 --> 01:10:12.220]   So, we have a question. So, we have a question. So, we have a question. So, we have a question.\n",
      "[01:10:12.220 --> 01:10:41.220]   So, after introducing our notation, we are going to do a question. So, we have a question.\n",
      "[01:10:41.220 --> 01:10:51.220]   On notation, we will kind of start to talk a little bit about the concentration inequality.\n",
      "[01:10:51.220 --> 01:11:02.220]   So, just to recall that our objective is we want to have a much sharper bounds that basically tells us something about how close it is from the sample average.\n",
      "[01:11:02.220 --> 01:11:14.220]   That is the summation from 1 to n to its expectation. Like we see the expected value of xi.\n",
      "[01:11:14.220 --> 01:11:24.220]   So, we want this difference to be less or equal to Fm. So, this is what we call concentration inequality.\n",
      "[01:11:24.220 --> 01:11:36.220]   And it turns out that whether this inequality can happen really depends on like what is the relation between different xi and whether they have some dependency or something.\n",
      "[01:11:36.220 --> 01:11:46.220]   So, one of the very basic case we can start with considering inequality is we look at the independent random variable.\n",
      "[01:11:46.220 --> 01:11:51.220]   The first case is we say this is independent.\n",
      "[01:11:51.220 --> 01:12:02.220]   And this is a very important assumption because during independence, so we kind of expect the difference between the random variable to expectation is like some noise.\n",
      "[01:12:02.220 --> 01:12:08.220]   And then because the noise are independent, so they're going to fluctuate and eventually cancel out for a lot of times.\n",
      "[01:12:08.220 --> 01:12:28.220]   So, the whole reason why we can't have concentration. So, what we'll do the first part, that is the concentration inequality\n",
      "[01:12:28.220 --> 01:12:37.220]   for independent random variables.\n",
      "[01:12:37.220 --> 01:12:55.220]   Or just use the shot like Rv is like a random variables.\n",
      "[01:12:55.220 --> 01:13:01.220]   So, we directly introduce the key result today and we will leave the proof to the next lecture.\n",
      "[01:13:01.220 --> 01:13:09.220]   Now, the key result we're going to do this for is, we call it half-dincey inequality.\n",
      "[01:13:09.220 --> 01:13:27.220]   Let's do a theorem too.\n",
      "[01:13:27.220 --> 01:13:43.220]   So, the statement of theorem looks like the follows that we say let x1, x2, that allow to xn be independent random variables.\n",
      "[01:13:43.220 --> 01:13:47.220]   This is like the key assumption they're independent.\n",
      "[01:13:47.220 --> 01:14:00.220]   And the other structure is with probability 1.\n",
      "[01:14:00.220 --> 01:14:10.220]   We know xi's bounded ai to bi.\n",
      "[01:14:10.220 --> 01:14:17.220]   And this is essentially okay for our setting because a lot of times in reinforcement, we just consider bounded random variable.\n",
      "[01:14:17.220 --> 01:14:31.220]   For example, the value is like in 0 to h or something. So, we say the random variable is always bounded with probability 1.\n",
      "[01:14:31.220 --> 01:14:54.220]   So, let's just use the notation at i equal to bi minus ai, which is the range of xi.\n",
      "[01:14:54.220 --> 01:15:18.220]   Then for any t greater than 0, we see the probability of summation i from 1 to n of xi minus e xi.\n",
      "[01:15:18.220 --> 01:15:38.220]   Greater equal to t is less or equal to something exponential to the minus 2t square over summation i from 1 to n i square.\n",
      "[01:15:38.220 --> 01:16:06.220]   So,\n",
      "[01:16:06.220 --> 01:16:11.220]   briefly say what does this mean? Why this is called a concentration in quality?\n",
      "[01:16:11.220 --> 01:16:19.220]   We can think this whatever the distribution, we can look at some distribution of summation xi.\n",
      "[01:16:19.220 --> 01:16:28.220]   So, we know summation xi is has the mean, which is the mean, which is the expectation of summation xi.\n",
      "[01:16:28.220 --> 01:16:34.220]   And we expect this as a new random variable that's called v or something like z.\n",
      "[01:16:34.220 --> 01:16:40.220]   So, this is z has a mean, which is equal to this. And we expect some distribution probably something look like this.\n",
      "[01:16:40.220 --> 01:16:44.220]   I don't really know whether it's like this, but I expect something like this.\n",
      "[01:16:44.220 --> 01:16:55.220]   So, what this statement is saying is the probability of this z random variable exceeding the expectation by amount of t.\n",
      "[01:16:55.220 --> 01:17:04.220]   So, this is the difference of t. So, it says it will exceed the expectation by amount of t.\n",
      "[01:17:04.220 --> 01:17:09.220]   And look at all the probability that goes beyond t.\n",
      "[01:17:09.220 --> 01:17:15.220]   This is the probability of this random variable z is going to be greater than the expectation, at least by amount of t.\n",
      "[01:17:15.220 --> 01:17:35.220]   So, what we are saying is that this probability is exponentially small regarding to t.\n",
      "[01:17:35.220 --> 01:17:52.220]   In fact, it would decrease like something e to the minus t square or something like that.\n",
      "[01:17:52.220 --> 01:17:58.220]   So, this essentially says the probability of going out is like very, very small.\n",
      "[01:17:58.220 --> 01:18:08.220]   So, you can imagine like this is a, instead of some picture look like this, it might be something like a super sharp.\n",
      "[01:18:08.220 --> 01:18:14.220]   So, the probability is like very small. And so, most of the mass will be concentrated around like this region.\n",
      "[01:18:14.220 --> 01:18:18.220]   So, this is why it caught concentration inequality.\n",
      "[01:18:18.220 --> 01:18:28.220]   So, this is only one side of result, you can also directly flip it to the other side like this is less or equal to minus t and you get the same result.\n",
      "[01:18:28.220 --> 01:18:54.220]   So, basically it came most mass in the middle region.\n",
      "[01:18:54.220 --> 01:19:00.220]   And so, this is why we caught concentration.\n",
      "[01:19:00.220 --> 01:19:11.220]   And in the next lecture, we will further talk about what this country and inquiry talk about and we will also talk about how we are going to derive this concentration inequality.\n",
      "[01:19:11.220 --> 01:19:13.800]   (upbeat music)\n",
      "[01:19:13.800 --> 01:19:21.760]   [BLANK_AUDIO]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/n3WxDCvjlAs.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/n3WxDCvjlAs.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/n3WxDCvjlAs.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/n3WxDCvjlAs.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1276.99 ms\n",
      "whisper_print_timings:     fallbacks =   6 p /   1 h\n",
      "whisper_print_timings:      mel time =  2651.76 ms\n",
      "whisper_print_timings:   sample time = 25112.27 ms / 62636 runs (    0.40 ms per run)\n",
      "whisper_print_timings:   encode time =   360.33 ms /   198 runs (    1.82 ms per run)\n",
      "whisper_print_timings:   decode time =   603.83 ms /   290 runs (    2.08 ms per run)\n",
      "whisper_print_timings:   batchd time = 32067.71 ms / 61329 runs (    0.52 ms per run)\n",
      "whisper_print_timings:   prompt time = 10062.25 ms / 45232 runs (    0.22 ms per run)\n",
      "whisper_print_timings:    total time = 72603.55 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=YEOAn9FIvyI started\n",
      "YEOAn9FIvyI\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/YEOAn9FIvyI.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/YEOAn9FIvyI.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:21:20.95, start: 0.000000, bitrate: 290 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 190 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/YEOAn9FIvyI.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  152530kB time=01:21:20.94 bitrate= 256.0kbits/s speed=1.42e+03x    \n",
      "video:0kB audio:152530kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000050%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/YEOAn9FIvyI.wav' (78095209 samples, 4881.0 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.200]   [MUSIC]\n",
      "[00:00:03.200 --> 00:00:07.080]   >> Let's first recall what we have been talking about in the last lecture.\n",
      "[00:00:07.080 --> 00:00:11.600]   We have been talking about the basic MDP formulation,\n",
      "[00:00:11.600 --> 00:00:13.880]   just very briefly recall.\n",
      "[00:00:13.880 --> 00:00:15.280]   When we talk about MDP,\n",
      "[00:00:15.280 --> 00:00:17.680]   we talk about MDP as the following elements.\n",
      "[00:00:17.680 --> 00:00:22.200]   The first, we have a set of states and we also have a set of actions,\n",
      "[00:00:22.200 --> 00:00:27.960]   and we have a reward function and a transition probability transition matrix.\n",
      "[00:00:27.960 --> 00:00:32.840]   Finally, things we're always talking about the episodic setting in this class,\n",
      "[00:00:32.840 --> 00:00:35.760]   so we will have a finite horizon H.\n",
      "[00:00:35.760 --> 00:00:38.600]   Okay. This is the first thing we're talking about.\n",
      "[00:00:38.600 --> 00:00:45.320]   Then the next, we talk about what we call the environments and the policies and values.\n",
      "[00:00:45.320 --> 00:00:48.880]   So we know the environments.\n",
      "[00:00:48.880 --> 00:00:53.880]   We typically just means the elements P and R,\n",
      "[00:00:53.880 --> 00:00:59.280]   and we will first talk about the case of known environments today,\n",
      "[00:00:59.280 --> 00:01:02.320]   and later we will talk about the learning scenario where we're actually\n",
      "[00:01:02.320 --> 00:01:04.000]   exploring in the unknown environments.\n",
      "[00:01:04.000 --> 00:01:08.820]   So we actually need to learn those P and R from some data, from observations.\n",
      "[00:01:08.820 --> 00:01:12.240]   Then we also talk about the policy.\n",
      "[00:01:12.240 --> 00:01:16.720]   So policy is the first tricky part.\n",
      "[00:01:16.720 --> 00:01:19.320]   So we first talk about there are two kinds of policy.\n",
      "[00:01:19.320 --> 00:01:23.520]   One is we will spend a lot of time on that is called the Markov policy.\n",
      "[00:01:23.520 --> 00:01:32.560]   Markov policy can be viewed as I have a policy pi.\n",
      "[00:01:32.560 --> 00:01:38.520]   That is equal to a bunch of functions, we actually have H functions.\n",
      "[00:01:38.520 --> 00:01:45.320]   And each function is a map from states to either the action set or\n",
      "[00:01:45.320 --> 00:01:48.120]   the probability simple acts of action set.\n",
      "[00:01:48.120 --> 00:01:54.680]   Depends on whether it's deterministic policy or it's stochastic policy.\n",
      "[00:01:54.680 --> 00:01:57.440]   So we also have a general policy.\n",
      "[00:01:57.440 --> 00:02:01.280]   We say in general, we don't necessarily need to pick this special form of policy.\n",
      "[00:02:01.280 --> 00:02:10.920]   So the general policy is again, we have this pi is equal to,\n",
      "[00:02:10.920 --> 00:02:16.880]   we need to specify this policy, we just need to specify the policy at each step.\n",
      "[00:02:16.880 --> 00:02:23.440]   The only difference is now this policy can be a map from all the possible histories.\n",
      "[00:02:23.440 --> 00:02:30.280]   That is s times a to the power of h minus 1, and this is all possible histories.\n",
      "[00:02:30.280 --> 00:02:35.280]   Again, to the set of action or a set of distribution of action.\n",
      "[00:02:35.280 --> 00:02:46.360]   Okay, so in general, like you can think, this general policy is basically history dependent policy.\n",
      "[00:02:46.360 --> 00:02:49.840]   Well, this Markov policy is, you can think it's more like history independent.\n",
      "[00:02:49.840 --> 00:02:51.800]   It only depends on the current states, but\n",
      "[00:02:51.800 --> 00:02:54.200]   does not depends on the history before the current states.\n",
      "[00:02:54.200 --> 00:02:59.480]   And finally, we talk about the value in the last lecture.\n",
      "[00:02:59.480 --> 00:03:04.240]   So we'll just quickly talk about the VH.\n",
      "[00:03:04.240 --> 00:03:09.880]   So this is the value of, I would just first say this is like,\n",
      "[00:03:09.880 --> 00:03:14.160]   you should think this is a value of Markov policy.\n",
      "[00:03:14.160 --> 00:03:16.720]   We'll talk about the reason later.\n",
      "[00:03:16.720 --> 00:03:23.400]   So the value is defined as the expected cumulative rewards after step H.\n",
      "[00:03:23.400 --> 00:03:27.400]   So it's a summation of the rewards from each step to the final step.\n",
      "[00:03:27.400 --> 00:03:38.680]   Conditioning on the current state S is equal to S, okay?\n",
      "[00:03:38.680 --> 00:03:43.480]   We also talk about the Q value, which is a function of SA,\n",
      "[00:03:43.480 --> 00:03:45.560]   which is essentially the same thing.\n",
      "[00:03:45.560 --> 00:03:49.040]   But a condition on the current step, the state is S and the action is A.\n",
      "[00:03:49.040 --> 00:03:59.160]   So one thing I want to emphasize a little bit is this value, okay?\n",
      "[00:03:59.160 --> 00:04:07.320]   So because we have two different policies, we have Markov policy and the general policy.\n",
      "[00:04:07.320 --> 00:04:21.760]   Clearly, this VH is defined for Markov policy.\n",
      "[00:04:21.760 --> 00:04:35.600]   This definition is well defined for all H and S, for all states and the steps.\n",
      "[00:04:35.600 --> 00:04:38.280]   Okay, this has no problem.\n",
      "[00:04:38.280 --> 00:04:41.920]   But the only thing I want to be careful is when we written this kind of definition,\n",
      "[00:04:41.920 --> 00:04:47.720]   and when we talk about later steps or later states,\n",
      "[00:04:47.720 --> 00:04:52.480]   we actually realized that for general policy, it's not very well defined.\n",
      "[00:04:52.480 --> 00:04:58.000]   I think the reason is this kind of already assumes whatever the cumulative\n",
      "[00:04:58.000 --> 00:05:03.320]   rewards in the future only depends on the current state S, okay?\n",
      "[00:05:03.320 --> 00:05:05.240]   It doesn't depend on the history.\n",
      "[00:05:05.240 --> 00:05:08.280]   Well, on the other hand, a general policy depends on history.\n",
      "[00:05:08.280 --> 00:05:12.400]   So for general policy, you cannot necessarily write a value only depends on current state,\n",
      "[00:05:12.400 --> 00:05:14.560]   but also need to depend on entire history.\n",
      "[00:05:14.560 --> 00:05:21.360]   Okay, so in general, like this VH should depend on the entire history.\n",
      "[00:05:21.360 --> 00:05:48.680]   Okay, I will use this charge to specify the S1, A1, S2, A2, but that's your SH.\n",
      "[00:05:48.680 --> 00:05:53.120]   Okay, so we can no longer just use this definition.\n",
      "[00:05:53.120 --> 00:05:57.760]   However, we noticed at the very first step, V1 is still defined the same.\n",
      "[00:05:57.760 --> 00:06:04.400]   Because in the very first step, we don't have any further history.\n",
      "[00:06:04.400 --> 00:06:11.400]   So in the V1 case, in the first step, we can still use exactly this definition, okay?\n",
      "[00:06:11.400 --> 00:06:27.360]   So, finally, I think in the very last part of the last lecture, we talk about\n",
      "[00:06:27.360 --> 00:06:34.360]   Objection File File.\n",
      "[00:06:34.360 --> 00:06:56.800]   We want to define the optimal policy pi by star, which maximizes\n",
      "[00:06:56.800 --> 00:07:05.720]   the value at the initial states, which essentially at the beginning state of the game.\n",
      "[00:07:05.720 --> 00:07:10.080]   And let's just fully consider, without a lot of generality, consider we have the fixed\n",
      "[00:07:10.080 --> 00:07:16.920]   initial states case.\n",
      "[00:07:16.920 --> 00:07:20.400]   We already said, if you are not happy with fixed initial states, you can think initial\n",
      "[00:07:20.400 --> 00:07:24.200]   states is simple from some distribution, but all the proof, all the analysis we're going\n",
      "[00:07:24.200 --> 00:07:26.880]   to talk about today, easily extend to that kind of scenario.\n",
      "[00:07:26.880 --> 00:07:30.880]   So for simplicity, we just say we have a fixed initial states.\n",
      "[00:07:30.880 --> 00:07:34.960]   And for chess, essentially, we always start with the same configuration, which is initial\n",
      "[00:07:34.960 --> 00:07:35.960]   states.\n",
      "[00:07:35.960 --> 00:07:40.480]   And this is essentially kind of say, if you are playing some policy pi, and what is your\n",
      "[00:07:40.480 --> 00:07:48.600]   winning rate by using that kind of policy.\n",
      "[00:07:48.600 --> 00:07:49.600]   Okay.\n",
      "[00:07:49.600 --> 00:07:51.000]   So we stopped here in the last lecture.\n",
      "[00:07:51.000 --> 00:07:55.880]   So today, we will continue talking about how we're going to compute the value and also\n",
      "[00:07:55.880 --> 00:08:01.440]   what's the relation, how we compute the value and how we compute the optimal policy.\n",
      "[00:08:01.440 --> 00:08:06.600]   I think the first thing I want to do is think we kind of raise this general policy issue.\n",
      "[00:08:06.600 --> 00:08:12.720]   It seems like a very complicated, if we always need to talk about macro policy and general\n",
      "[00:08:12.720 --> 00:08:13.720]   policy.\n",
      "[00:08:13.720 --> 00:08:21.000]   And in the last lecture, I kind of already kind of sad that in the MDPS scenario, in\n",
      "[00:08:21.000 --> 00:08:25.760]   a single agent, like this Markov decision process, we can without loss of generality\n",
      "[00:08:25.760 --> 00:08:28.080]   talking about a Markov policy only.\n",
      "[00:08:28.080 --> 00:08:31.880]   So we don't need to go to the complexity of talking about this general policy.\n",
      "[00:08:31.880 --> 00:08:33.720]   So what is this reason?\n",
      "[00:08:33.720 --> 00:08:41.800]   So this is the first proposition we're going to talk about today.\n",
      "[00:08:41.800 --> 00:09:03.240]   In one, we're going to say for any general policy, pi, there exists\n",
      "[00:09:03.240 --> 00:09:15.160]   a macro policy mu\n",
      "[00:09:15.160 --> 00:09:23.320]   so that the value of the entire game, that is starting at the first step with initial\n",
      "[00:09:23.320 --> 00:09:37.200]   states is equal.\n",
      "[00:09:37.200 --> 00:09:40.240]   Just want to send a check that the macro policy can define this.\n",
      "[00:09:40.240 --> 00:09:44.200]   And for general policy, the later step is more complicated depending on your history,\n",
      "[00:09:44.200 --> 00:09:47.840]   but even the first step, you can still define it in terms of the initial states.\n",
      "[00:09:47.840 --> 00:09:52.160]   So both quantities are well defined.\n",
      "[00:09:52.160 --> 00:09:58.680]   And this essentially says, if my final goal is I just want to maximize my winning probability\n",
      "[00:09:58.680 --> 00:10:02.440]   or something, I want to achieve the highest value.\n",
      "[00:10:02.440 --> 00:10:04.840]   Without loss of generality, we can talk about the macro policy.\n",
      "[00:10:04.840 --> 00:10:08.960]   Because for whatever the value can be achieved by general policy, I can essentially construct\n",
      "[00:10:08.960 --> 00:10:15.400]   some other macro policy that may depend on this general policy and achieve the same value.\n",
      "[00:10:15.400 --> 00:10:38.040]   So this immediately implies that there exists at least an optimal policy,\n",
      "[00:10:38.040 --> 00:10:59.720]   and this mark off.\n",
      "[00:10:59.720 --> 00:11:02.560]   Any questions about the logic here?\n",
      "[00:11:02.560 --> 00:11:09.600]   If not a question, I think we will first spend some time, prove this proposition one.\n",
      "[00:11:09.600 --> 00:11:14.160]   Since this is a mathematical foundation, I think a lot of mathematical courses are constructed\n",
      "[00:11:14.160 --> 00:11:15.160]   in this way.\n",
      "[00:11:15.160 --> 00:11:19.280]   We first talk about the proposition, which proposition are theorem or lama, which is\n",
      "[00:11:19.280 --> 00:11:24.920]   the important thing, like carry the meaning of a high-level idea of what is going on.\n",
      "[00:11:24.920 --> 00:11:33.160]   And then we talk about mathematical proofs to support this kind of claim.\n",
      "[00:11:33.160 --> 00:11:38.560]   So the proof of this proposition is relatively easy if you are very familiar with this kind\n",
      "[00:11:38.560 --> 00:11:40.360]   of notation and probability.\n",
      "[00:11:40.360 --> 00:11:43.440]   So we'll just do the proof for now.\n",
      "[00:11:43.440 --> 00:11:50.320]   And a lot of proof in the reinforced modeling is by induction because we need to prove first\n",
      "[00:11:50.320 --> 00:11:54.280]   step is cracked and second step is cracked and those kind of things.\n",
      "[00:11:54.280 --> 00:11:56.360]   So we'll just do it here.\n",
      "[00:11:56.360 --> 00:12:04.640]   So let's first write out what is the value of this complicated, like a general policy\n",
      "[00:12:04.640 --> 00:12:05.720]   prime.\n",
      "[00:12:05.720 --> 00:12:07.360]   So we start with some general policy.\n",
      "[00:12:07.360 --> 00:12:10.120]   We want to essentially, we say there exists a mark off policy.\n",
      "[00:12:10.120 --> 00:12:11.400]   We will prove by construction.\n",
      "[00:12:11.400 --> 00:12:15.460]   So we'll construct some mark off policy that depends on this general policy so that we\n",
      "[00:12:15.460 --> 00:12:18.960]   can achieve the same value.\n",
      "[00:12:18.960 --> 00:12:22.160]   So let's first write out a definition of this value.\n",
      "[00:12:22.160 --> 00:12:29.320]   We already kind of say here is summation of h equal to 1 to capital H. We kind of put\n",
      "[00:12:29.320 --> 00:12:39.240]   a summation outside and inside we have the expectation over pi and the reward at h step\n",
      "[00:12:39.240 --> 00:12:50.320]   h, h, condition on the first status as 1.\n",
      "[00:12:50.320 --> 00:12:51.640]   This is just basic definition.\n",
      "[00:12:51.640 --> 00:12:55.640]   We only think we did is like exchange the expectation of an summation which we already\n",
      "[00:12:55.640 --> 00:13:00.960]   know is we can do it.\n",
      "[00:13:00.960 --> 00:13:05.960]   So the second thing is we explicitly write out what is this expectation.\n",
      "[00:13:05.960 --> 00:13:12.880]   So the expectation essentially we just summation over, so here the only randomness in this\n",
      "[00:13:12.880 --> 00:13:19.080]   reward is just this sh and h, those are the random variables.\n",
      "[00:13:19.080 --> 00:13:23.280]   So the expectation essentially is just summation over all the possible values of those random\n",
      "[00:13:23.280 --> 00:13:27.240]   variables and times the probability of those random variables.\n",
      "[00:13:27.240 --> 00:13:35.360]   So we summation over all the possible values of sh, h.\n",
      "[00:13:35.360 --> 00:13:46.360]   And we say we use the probability pi h of sh, h, condition on s1.\n",
      "[00:13:46.360 --> 00:13:56.160]   This is the probability of regions sh, follow the policy pi at step h and times the reward\n",
      "[00:13:56.160 --> 00:13:58.480]   of sh, h.\n",
      "[00:13:58.480 --> 00:14:14.400]   So I just want to say this is the probability.\n",
      "[00:14:14.400 --> 00:14:42.720]   Because sh, h at a step h, if starting from s1 and the follow\n",
      "[00:14:42.720 --> 00:15:12.440]   pi 1, follow positive pi.\n",
      "[00:15:12.440 --> 00:15:18.600]   So and also things we always just consider, we kind of start from the fixed initial states\n",
      "[00:15:18.600 --> 00:15:19.600]   s1.\n",
      "[00:15:19.600 --> 00:15:40.400]   So for simplicity, to simplify the notation, we will use this pH pi sh, to directly just\n",
      "[00:15:40.400 --> 00:15:50.520]   means it's conditional s1, because this is like fixed initial states, this is fixed.\n",
      "[00:15:50.520 --> 00:15:55.760]   So when it's clear from the compact, let's just always say if we don't, even if we don't\n",
      "[00:15:55.760 --> 00:16:00.960]   anything, we actually just mean we conditional, we fix this initial states.\n",
      "[00:16:00.960 --> 00:16:04.400]   So let's just simplify this notation.\n",
      "[00:16:04.400 --> 00:16:07.680]   So what do we learn from this equation?\n",
      "[00:16:07.680 --> 00:16:13.800]   This essentially is the value is just equal to the reward of each possible state action\n",
      "[00:16:13.800 --> 00:16:19.000]   pair times the probability of possibly visit those kind of state action pair, and then\n",
      "[00:16:19.000 --> 00:16:22.400]   we summation of all of them together.\n",
      "[00:16:22.400 --> 00:16:32.640]   And this not only holds for pi, also holds for mu.\n",
      "[00:16:32.640 --> 00:16:36.480]   Essentially this holds for any policy, not only just the particular general positive\n",
      "[00:16:36.480 --> 00:16:41.760]   pi but also holds for whatever the marginal policy we're going to construct for mu.\n",
      "[00:16:41.760 --> 00:16:54.480]   So this implies if finally we just want to prove v1 pi s1 is equal to v1 mu s1, there's\n",
      "[00:16:54.480 --> 00:17:02.080]   another way we can prove this statement, which essentially we just need to prove the probability\n",
      "[00:17:02.080 --> 00:17:12.280]   of reaching any state action pair at h step if follow a policy pi is the same as the probability\n",
      "[00:17:12.280 --> 00:17:23.960]   of reaching any state action pair at the step h for a policy mu for any sah.\n",
      "[00:17:23.960 --> 00:17:27.160]   Because we can see on the right hand side, there's only one thing that depends on the\n",
      "[00:17:27.160 --> 00:17:31.680]   policy that is this probability, everything else is independent of policy.\n",
      "[00:17:31.680 --> 00:17:36.440]   So that's why as long as we can prove the visitation, this is also called the visitation\n",
      "[00:17:36.440 --> 00:17:41.120]   measure, as long as this visitation measure is the same for different policy, then the\n",
      "[00:17:41.120 --> 00:17:46.720]   value is the same.\n",
      "[00:17:46.720 --> 00:17:53.480]   So the way we're going to construct this Markov policy to match the value is basically\n",
      "[00:17:53.480 --> 00:17:58.560]   by this criterion, we will construct our Markov policy so that we will match the visitation\n",
      "[00:17:58.560 --> 00:18:11.700]   measure, we'll match the probability of visiting any possible state action pair.\n",
      "[00:18:11.700 --> 00:18:15.160]   Okay.\n",
      "[00:18:15.160 --> 00:18:26.000]   So in order to prove this, let's first, we'll do this proof by construction.\n",
      "[00:18:26.000 --> 00:18:31.920]   So the idea is we're going to construct some policy mu so that this is true.\n",
      "[00:18:31.920 --> 00:18:35.960]   Just remember, in order to construct some Markov policy, all we need to do is we construct\n",
      "[00:18:35.960 --> 00:18:38.520]   the Markov policy at each step.\n",
      "[00:18:38.520 --> 00:18:45.560]   So we essentially need to specify what is the value of this Markov policy mu h at h step.\n",
      "[00:18:45.560 --> 00:18:53.160]   That is the probability of taking action A, conditional current state as, let's say\n",
      "[00:18:53.160 --> 00:18:59.280]   in general, this is like stochastic policy.\n",
      "[00:18:59.280 --> 00:19:02.280]   So this is what we want to construct.\n",
      "[00:19:02.280 --> 00:19:11.320]   And what we have here is we have some Markov, general policy, that Pi h, which kind of depends\n",
      "[00:19:11.320 --> 00:19:21.240]   on the action A and also the entire history, tau h, where the tau h is equal to S1 A1 that\n",
      "[00:19:21.240 --> 00:19:23.320]   is equal to S h.\n",
      "[00:19:23.320 --> 00:19:24.320]   Okay.\n",
      "[00:19:24.320 --> 00:19:30.520]   So this depends on the entire history.\n",
      "[00:19:30.520 --> 00:19:36.480]   So a simple idea of connecting those two policies, like we want to match this probability,\n",
      "[00:19:36.480 --> 00:19:41.200]   is how about we just do some average thing.\n",
      "[00:19:41.200 --> 00:19:44.720]   Because on the left-hand side, we only depend on current state and current action.\n",
      "[00:19:44.720 --> 00:19:47.960]   We're on the right-hand side, we depend on the entire history.\n",
      "[00:19:47.960 --> 00:19:52.160]   So one thing we can get rid of this history is we're just taking expectation over those\n",
      "[00:19:52.160 --> 00:19:54.000]   histories.\n",
      "[00:19:54.000 --> 00:19:56.600]   So this is essentially the idea we're going to do.\n",
      "[00:19:56.600 --> 00:20:04.400]   So we're going to say we will construct this policy, which is equal to taking average over\n",
      "[00:20:04.400 --> 00:20:12.120]   the general policy and the condition on this sh equal to S.\n",
      "[00:20:12.120 --> 00:20:17.800]   That is, the right-hand side originally depends on A, S1 A1, S2 A2 tau h.\n",
      "[00:20:17.800 --> 00:20:22.080]   Then we're going to just fix the last state equal to S, A is equal to A, and I'm going\n",
      "[00:20:22.080 --> 00:20:38.040]   to take an expectation over everything.\n",
      "[00:20:38.040 --> 00:20:43.960]   So this is essentially some straightforward idea we can think of.\n",
      "[00:20:43.960 --> 00:20:50.160]   And then our left is we need to test whether our construction actually delivers something\n",
      "[00:20:50.160 --> 00:21:08.020]   like this, actually delivers this property like this.\n",
      "[00:21:08.020 --> 00:21:12.240]   So first thing is we need to, since this is a conditional expectation, we need to write\n",
      "[00:21:12.240 --> 00:21:22.040]   the mathematical definition or like how we're going to compute this conditional expectation.\n",
      "[00:21:22.040 --> 00:21:26.480]   So we know the conditional expectation is just equal to the probability of this joint\n",
      "[00:21:26.480 --> 00:21:31.540]   probability over the probability of this whatever we condition now.\n",
      "[00:21:31.540 --> 00:21:40.080]   So we can write this equation, which is equal to on the denominator, that is the probability.\n",
      "[00:21:40.080 --> 00:21:46.320]   We follow in a pi to reach S in each step.\n",
      "[00:21:46.320 --> 00:21:49.320]   So this is a probability of what we condition now.\n",
      "[00:21:49.320 --> 00:21:54.360]   And then on the top, on numerators, the joint probability, which we can write it out, that\n",
      "[00:21:54.360 --> 00:22:06.480]   is equal to summation of all tau h, so that SH equal to S, and then the probability of\n",
      "[00:22:06.480 --> 00:22:24.320]   visiting tau h times the pi, which of A given tau h.\n",
      "[00:22:24.320 --> 00:22:28.560]   So this is essentially the definition of this conditional expectation, or we just compute\n",
      "[00:22:28.560 --> 00:22:29.800]   it out.\n",
      "[00:22:29.800 --> 00:22:37.340]   And the only thing that relies on is we require the denominator to be non-zero, otherwise\n",
      "[00:22:37.340 --> 00:22:46.120]   this is like L defined.\n",
      "[00:22:46.120 --> 00:22:51.880]   So that means we need to fix something if this is equal to zero, so what are we going\n",
      "[00:22:51.880 --> 00:22:52.880]   to do?\n",
      "[00:22:52.880 --> 00:22:55.920]   So it turns out that when this is equal to zero, it doesn't matter because when we follow\n",
      "[00:22:55.920 --> 00:22:58.920]   the policy pi, we will never visit that kind of state.\n",
      "[00:22:58.920 --> 00:23:02.640]   So it doesn't really matter what kind of policy you play on that state.\n",
      "[00:23:02.640 --> 00:23:13.360]   So you can just be arbitrary policy.\n",
      "[00:23:13.360 --> 00:23:34.240]   Or for the states that the probability is equal to zero.\n",
      "[00:23:34.240 --> 00:23:41.120]   So this gives a more rigorous way of defining what is this micro policy to be, and all the\n",
      "[00:23:41.120 --> 00:23:43.200]   remaining things we just need to check.\n",
      "[00:23:43.200 --> 00:24:07.360]   We generate the same visitation measure.\n",
      "[00:24:07.360 --> 00:24:14.440]   Any questions so far?\n",
      "[00:24:14.440 --> 00:24:15.440]   Okay.\n",
      "[00:24:15.440 --> 00:24:30.480]   So now it remains we will use induction to prove this stopper claim that is by our construction\n",
      "[00:24:30.480 --> 00:24:35.680]   of the mill, the mill and pi will generate the same visitation measure over all state\n",
      "[00:24:35.680 --> 00:24:41.320]   action and the step.\n",
      "[00:24:41.320 --> 00:24:45.520]   And the approach we're going to do induction is essentially we're going to argue in the\n",
      "[00:24:45.520 --> 00:24:47.120]   very first step, this is true.\n",
      "[00:24:47.120 --> 00:24:49.800]   And then the second step, this is true, and then the third step is true.\n",
      "[00:24:49.800 --> 00:24:55.000]   So we're going to do induction over H. Specifically, we're going to do the following.\n",
      "[00:24:55.000 --> 00:25:13.840]   So we will first argue that pH mu s is equal to pH pi s.\n",
      "[00:25:13.840 --> 00:25:22.040]   If this is true, then this is going to imply pH mu s a, that is the probability of visiting\n",
      "[00:25:22.040 --> 00:25:31.400]   SA at H step is also equal to pH pi, a following policy pi, a visiting SA.\n",
      "[00:25:31.400 --> 00:25:43.720]   And then we're going to use this to imply that pH plus one mu s is equal to pH pi s.\n",
      "[00:25:43.720 --> 00:25:49.720]   So we're going to prove first step and a second step.\n",
      "[00:25:49.720 --> 00:25:51.640]   So this is how we're going to do induction.\n",
      "[00:25:51.640 --> 00:26:00.040]   That is for NH, I'm going to prove that this claim will improve, it will imply this claim.\n",
      "[00:26:00.040 --> 00:26:04.760]   And the second step is prove this claim will also imply this claim.\n",
      "[00:26:04.760 --> 00:26:15.320]   And then we can do induction from the first step because we can check the very basic case.\n",
      "[00:26:15.320 --> 00:26:19.360]   So the basic case, essentially, we just need to check at the very first step.\n",
      "[00:26:19.360 --> 00:26:22.560]   Because first step, we're doing the fixed initial states.\n",
      "[00:26:22.560 --> 00:26:27.360]   So actually, the probability of visiting SA is kind of irrelevant to the policy.\n",
      "[00:26:27.360 --> 00:26:33.120]   So we immediately know this is true.\n",
      "[00:26:33.120 --> 00:26:46.240]   This is true things as one is fixed.\n",
      "[00:26:46.240 --> 00:26:51.520]   So all remains is just so we prove one and two, like y, one towards two.\n",
      "[00:26:51.520 --> 00:26:57.520]   And one to essentially followed by some calculation of basic probability.\n",
      "[00:26:57.520 --> 00:27:03.840]   So we'll first say the pH mu will prove one first.\n",
      "[00:27:03.840 --> 00:27:06.960]   So essentially, we want to first look at this pH mu SA.\n",
      "[00:27:06.960 --> 00:27:08.360]   This is what we want to prove.\n",
      "[00:27:08.360 --> 00:27:11.800]   This is equal to this.\n",
      "[00:27:11.800 --> 00:27:20.440]   And this we know because it's micro-policy, it's equal to probability of visiting state\n",
      "[00:27:20.440 --> 00:27:30.480]   S at edge staff, and times the probability of taking some action A, given S.\n",
      "[00:27:30.480 --> 00:27:32.520]   This is essentially the probability.\n",
      "[00:27:32.520 --> 00:27:35.920]   This is essentially the stochastic policy where the probability of taking some action\n",
      "[00:27:35.920 --> 00:27:44.880]   A condition S.\n",
      "[00:27:44.880 --> 00:27:55.680]   And finally, we know this is equal to p pi of S, and the mu h A given S. This is by induction\n",
      "[00:27:55.680 --> 00:28:10.960]   hypothesis.\n",
      "[00:28:10.960 --> 00:28:39.960]   So on the other hand, we can also start from the other side we want to prove, which is\n",
      "[00:28:39.960 --> 00:28:47.240]   the probability of reaching some state of action SA pair at step H, follow the general\n",
      "[00:28:47.240 --> 00:28:52.000]   policy pi.\n",
      "[00:28:52.000 --> 00:28:57.320]   And this we need to be slightly careful, and this we can no longer do this decomposition.\n",
      "[00:28:57.320 --> 00:29:02.080]   Essentially we need to summation over all the possible history, so that the final step\n",
      "[00:29:02.080 --> 00:29:13.800]   is equal to S, and we see the probability of h following policy pi to visit those history\n",
      "[00:29:13.800 --> 00:29:18.240]   and action pair.\n",
      "[00:29:18.240 --> 00:29:23.040]   By doing this decomposition is because now we can do the similar decomposition, that\n",
      "[00:29:23.040 --> 00:29:28.560]   we can decouple the A from this trajectory pi, because now the general policy actually\n",
      "[00:29:28.560 --> 00:29:30.880]   depends on the entire history.\n",
      "[00:29:30.880 --> 00:29:41.000]   So this is equal to summation over tau h SH equal to S, and we say this is equal to the\n",
      "[00:29:41.000 --> 00:29:50.120]   probability of following policy pi to visit some trajectory tau H times the general probability\n",
      "[00:29:50.120 --> 00:30:04.240]   of taking some action A condition on tau h.\n",
      "[00:30:04.240 --> 00:30:09.560]   So this is essentially also everything we can do on this side.\n",
      "[00:30:09.560 --> 00:30:16.540]   So we can see if we want this two to be equal, then we have to make some policy pi to satisfy\n",
      "[00:30:16.540 --> 00:30:18.600]   some special condition.\n",
      "[00:30:18.600 --> 00:30:25.400]   Unfortunately we already say our construction of pi is equal to this formula.\n",
      "[00:30:25.400 --> 00:30:30.920]   And we can directly plug this in, and then we realize when we do this, this kind of cancel\n",
      "[00:30:30.920 --> 00:30:36.880]   out with a denominator, and whatever on the numerator is exactly equal to this.\n",
      "[00:30:36.880 --> 00:30:50.320]   So this is essentially those two are equal by construction of mu.\n",
      "[00:30:50.320 --> 00:30:53.840]   That is our construction seems like to have some reason, and which the precise reason\n",
      "[00:30:53.840 --> 00:30:56.400]   is to make this two equal to each other.\n",
      "[00:30:56.400 --> 00:31:06.840]   So that our first step essentially holds.\n",
      "[00:31:06.840 --> 00:31:10.760]   I think this is a bit more about mathematical calculation, but the whole intuition is like\n",
      "[00:31:10.760 --> 00:31:14.880]   we just want to have some construction of our policy, which is average so that we're\n",
      "[00:31:14.880 --> 00:31:28.680]   more this guarantee this is true.\n",
      "[00:31:28.680 --> 00:31:36.280]   Now the first step is true, all we need to do is a second step.\n",
      "[00:31:36.280 --> 00:31:51.920]   Actually, we also start from probability of visiting some S in the next step, H+1 step.\n",
      "[00:31:51.920 --> 00:32:01.640]   Okay, let's just say this is H+1, so that we'll make it clear.\n",
      "[00:32:01.640 --> 00:32:10.840]   And we know this is obviously equal to summation over all SHH, the probability of visiting\n",
      "[00:32:10.840 --> 00:32:25.880]   or the tuple of SHH, SHH+1.\n",
      "[00:32:25.880 --> 00:32:39.520]   And in general, we can say this is equal to SHH, the probability of mu, that essentially\n",
      "[00:32:39.520 --> 00:32:44.640]   we decoupled this thing, that is equal to the probability of visiting SH+1, conditioned\n",
      "[00:32:44.640 --> 00:33:02.840]   on SHH, times the probability of visiting SHH.\n",
      "[00:33:02.840 --> 00:33:06.080]   Nothing very fancy.\n",
      "[00:33:06.080 --> 00:33:13.600]   I think the most important part is now we realize this thing.\n",
      "[00:33:13.600 --> 00:33:31.000]   This thing essentially is the transition in MDP, so essentially we already say the transition\n",
      "[00:33:31.000 --> 00:33:37.040]   structure actually incorporates a Markov structure, isn't it?\n",
      "[00:33:37.040 --> 00:33:44.120]   So the key thing to observe is this thing only depends on transition P, but it does not\n",
      "[00:33:44.120 --> 00:34:03.200]   depends on mu.\n",
      "[00:34:03.200 --> 00:34:17.680]   So essentially we can get rid of this mu dependence here.\n",
      "[00:34:17.680 --> 00:34:22.680]   And this is essentially the key step where we kind of use this Markov property, so that\n",
      "[00:34:22.680 --> 00:34:35.720]   we can say we don't need the general policy.\n",
      "[00:34:35.720 --> 00:34:41.920]   So the remaining steps become relatively straightforward that we say this is equal to,\n",
      "[00:34:41.920 --> 00:34:47.000]   so first we want to use the induction hypothesis, where the induction hypothesis says P mule\n",
      "[00:34:47.000 --> 00:34:51.440]   is equal to pi on SHH, so we can replace this mu by pi.\n",
      "[00:34:51.440 --> 00:35:04.920]   So this is equal to summation SHH of SH+1 given the current step state in SH, action\n",
      "[00:35:04.920 --> 00:35:30.160]   H, and times we just replace this by pi, this is by induction hypothesis.\n",
      "[00:35:30.160 --> 00:35:35.920]   And finally, we can also do the same argument that this thing doesn't depend on pi, so it\n",
      "[00:35:35.920 --> 00:35:39.160]   doesn't matter if we put pi here or not, they are the same thing.\n",
      "[00:35:39.160 --> 00:35:54.920]   So we can also just add a pi here, and then this becomes pH pi of SHH, SH+1, which is exactly\n",
      "[00:35:54.920 --> 00:36:12.240]   just equal to, sorry, pH plus 1, let's just say pH, which plus 1, pi and SH+1.\n",
      "[00:36:12.240 --> 00:36:30.040]   And this is finished in hard proof.\n",
      "[00:36:30.040 --> 00:36:35.800]   So after you get used to this notation of MDP and how the probability is going to generate\n",
      "[00:36:35.800 --> 00:36:38.880]   each other, and those proofs are going to be a piece of cake.\n",
      "[00:36:38.880 --> 00:36:44.880]   And only important idea is how we're going to construct the Markov policy that's going\n",
      "[00:36:44.880 --> 00:36:52.120]   to essentially match the same value of general policy is just by averaging over all the history.\n",
      "[00:36:52.120 --> 00:37:15.640]   I have a question about this part so far.\n",
      "[00:37:15.640 --> 00:37:27.080]   >> I'm just thinking, sir, that second part of the group is saying, oh, if they've got\n",
      "[00:37:27.080 --> 00:37:46.060]   the same probability of getting to say F and action A, then they always have the same\n",
      "[00:37:46.060 --> 00:37:49.520]   probability of getting to state H plus 1, yes, exactly.\n",
      "[00:37:49.520 --> 00:37:54.880]   So don't worry if you kind of feel some steps missing here, and I guess the important part\n",
      "[00:37:54.880 --> 00:38:00.360]   is about the claim, like we realize now that we don't need to look at the general policy\n",
      "[00:38:00.360 --> 00:38:05.880]   at all if we only care about MDP and we can without loss of generality only talk about\n",
      "[00:38:05.880 --> 00:38:07.160]   Markov policy.\n",
      "[00:38:07.160 --> 00:38:12.360]   So from now on, in the odd MDP part, we will just focus on the Markov policy.\n",
      "[00:38:12.360 --> 00:38:26.900]   So whenever we talk about policy, we actually mean Markov policy.\n",
      "[00:38:26.900 --> 00:38:32.600]   So before we go move forward, I want to talk a little bit about some notation I'm probably\n",
      "[00:38:32.600 --> 00:38:41.760]   going to be used in this lecture.\n",
      "[00:38:41.760 --> 00:38:45.240]   Well, for now, I think we just consider everything as a tabular setting.\n",
      "[00:38:45.240 --> 00:38:49.720]   So we consider we have some finite states and a finite action.\n",
      "[00:38:49.720 --> 00:38:55.680]   Like the state set, carnality is equal to S, and carnality of action set is equal to A.\n",
      "[00:38:55.680 --> 00:38:58.560]   Let's for now say something like that.\n",
      "[00:38:58.560 --> 00:39:06.120]   And then we use this value of some state at H stuff.\n",
      "[00:39:06.120 --> 00:39:12.880]   This is like some single scalar, real value of single scalar.\n",
      "[00:39:12.880 --> 00:39:20.240]   And sometimes we will use some notation like this V pi H without indicating what is S.\n",
      "[00:39:20.240 --> 00:39:24.000]   You can think this is a function, but you can also think this is a vector.\n",
      "[00:39:24.000 --> 00:39:28.160]   So this is a vector is in like the S dimensional vector.\n",
      "[00:39:28.160 --> 00:39:32.000]   You can essentially this is a vector so that the first dimension is like the value on the\n",
      "[00:39:32.000 --> 00:39:33.000]   first state.\n",
      "[00:39:33.000 --> 00:39:38.800]   Second dimension is the second coordinate is the value on the second states.\n",
      "[00:39:38.800 --> 00:39:48.800]   So similarly, you can also consider the Q value as a vector of S, A dimension.\n",
      "[00:39:48.800 --> 00:39:58.360]   Because essentially each state action pair correspond to one coordinate of this vector.\n",
      "[00:39:58.360 --> 00:40:03.240]   And finally, we will also use some succinct notation in reinforcement learning because\n",
      "[00:40:03.240 --> 00:40:05.720]   we're going to use this a lot of time.\n",
      "[00:40:05.720 --> 00:40:17.400]   So a lot of times we're going to face some expectation of the value of the next step\n",
      "[00:40:17.400 --> 00:40:26.480]   as prime conditional S, A.\n",
      "[00:40:26.480 --> 00:40:31.640]   Let's say this is the H step.\n",
      "[00:40:31.640 --> 00:40:38.240]   And this thing we know by definition is equal to taking expectation that S prime is generating\n",
      "[00:40:38.240 --> 00:40:43.040]   from the probability of like a transition probability, a condition on a current state\n",
      "[00:40:43.040 --> 00:40:47.280]   action and it goes to the next states and then V S prime.\n",
      "[00:40:47.280 --> 00:40:56.040]   So the complicated part about this notation is like we have this dummy variable S prime\n",
      "[00:40:56.040 --> 00:40:57.040]   and S prime.\n",
      "[00:40:57.040 --> 00:41:01.160]   Like we need to write it twice and which makes this notation a bit more complicated.\n",
      "[00:41:01.160 --> 00:41:06.200]   So for a lot of times we will just write a succinct notation where we just use this\n",
      "[00:41:06.200 --> 00:41:24.560]   P H V and depends on S A.\n",
      "[00:41:24.560 --> 00:41:33.360]   So we define this notation to essentially surprise the dependence on S prime.\n",
      "[00:41:33.360 --> 00:41:39.400]   So it really is just taking expectation of the value at the next step, conditional on\n",
      "[00:41:39.400 --> 00:41:40.920]   the current state action.\n",
      "[00:41:40.920 --> 00:41:45.360]   So we can read it as we start from the current state action and we take a transition to the\n",
      "[00:41:45.360 --> 00:41:49.240]   next step and then we evaluate the value on the next states.\n",
      "[00:41:49.240 --> 00:42:16.640]   So this is the notion we are going to use.\n",
      "[00:42:16.640 --> 00:42:28.280]   Essentially you can think we start from the current state an action and we take a transition\n",
      "[00:42:28.280 --> 00:42:32.520]   to the next states and it will take an average over value on the next states.\n",
      "[00:42:32.520 --> 00:42:34.240]   This is like essentially the definition.\n",
      "[00:42:34.240 --> 00:42:38.000]   That's start from the current state action and then we go to the next states.\n",
      "[00:42:38.000 --> 00:42:42.160]   We go to the next states and take an expectation on the value there.\n",
      "[00:42:42.160 --> 00:42:46.360]   So that I think this notation is a thing so we don't need to write the dummy variable\n",
      "[00:42:46.360 --> 00:42:54.920]   as prime to make it more succinct.\n",
      "[00:42:54.920 --> 00:42:57.400]   Okay.\n",
      "[00:42:57.400 --> 00:43:03.560]   So after we did all the cleanup work and now let's get to the interesting part.\n",
      "[00:43:03.560 --> 00:43:27.760]   So the first part is about a policy evaluation.\n",
      "[00:43:27.760 --> 00:43:30.600]   So policy evaluation asks this kind of question.\n",
      "[00:43:30.600 --> 00:43:36.680]   That is I think the first step, just remember the goal of reinforcement learning is we want\n",
      "[00:43:36.680 --> 00:43:39.920]   to find a policy so that the maximizes the value.\n",
      "[00:43:39.920 --> 00:43:42.280]   So the policy evaluation is essentially the first step.\n",
      "[00:43:42.280 --> 00:43:55.680]   It basically asks a question that given the environment P that is transition, R is a reward\n",
      "[00:43:55.680 --> 00:44:07.160]   and more importantly the policy pie, this is fixed policy.\n",
      "[00:44:07.160 --> 00:44:20.320]   How to compute v1 pie s1 essentially the value we care about.\n",
      "[00:44:20.320 --> 00:44:21.840]   Okay.\n",
      "[00:44:21.840 --> 00:44:27.000]   So the value of the policy pie from the initial states.\n",
      "[00:44:27.000 --> 00:44:29.400]   This by its name is like policy evaluation.\n",
      "[00:44:29.400 --> 00:44:34.560]   I want to evaluate how good this policy pie is and the evaluation criteria is by looking\n",
      "[00:44:34.560 --> 00:44:41.680]   at the value.\n",
      "[00:44:41.680 --> 00:44:46.520]   So in order to talk about this we first just record all the definition that we wrote in\n",
      "[00:44:46.520 --> 00:44:51.680]   the last lecture because now we can just purely focus on the market policy so everything\n",
      "[00:44:51.680 --> 00:44:53.820]   here is about market policy.\n",
      "[00:44:53.820 --> 00:45:03.120]   And we know the value of market policy can be written as expectation over all the summation\n",
      "[00:45:03.120 --> 00:45:16.960]   of all the future reward up to h and r h prime, s h prime, h prime, conditional s h equal\n",
      "[00:45:16.960 --> 00:45:37.720]   to s and the Q h pie Q h essay is equal to expectation over pie summation from h prime\n",
      "[00:45:37.720 --> 00:45:48.360]   over h and r h prime, s h prime, h prime, the only difference is the condition on the\n",
      "[00:45:48.360 --> 00:46:03.960]   current state is equal to s and action is equal to a.\n",
      "[00:46:03.960 --> 00:46:28.120]   So the question is again, how are we going to compute this initial value?\n",
      "[00:46:28.120 --> 00:46:34.400]   So there is a naive idea, a naive approach to compute this value at initial states is\n",
      "[00:46:34.400 --> 00:46:42.440]   basically just by expanding the entire probability by definition of expectation we can say because\n",
      "[00:46:42.440 --> 00:46:46.400]   this is expectation over the entire history, entire trajectory.\n",
      "[00:46:46.400 --> 00:46:53.120]   So we can summation over all the possible trajectories and the probability of those trajectories.\n",
      "[00:46:53.120 --> 00:47:09.080]   So one naive approach is we say we can do v1 pie s1 is equal to summation of all the\n",
      "[00:47:09.080 --> 00:47:20.560]   possible trajectory tau h, where this possible trajectory is like s1 a1 about r tau s h h.\n",
      "[00:47:20.560 --> 00:47:25.880]   And then we are going to say we look at the, we can show the probability of generating\n",
      "[00:47:25.880 --> 00:47:34.080]   this trajectory conditional s1 and then we do the summation of cumulative reward s equal\n",
      "[00:47:34.080 --> 00:47:49.160]   to 1 to h r h, s h h. This is essentially the reward of tau h, total reward of tau h.\n",
      "[00:47:49.160 --> 00:47:54.040]   So we essentially just say the probability of generating this tau h times the reward\n",
      "[00:47:54.040 --> 00:47:55.040]   of tau h.\n",
      "[00:47:55.040 --> 00:47:59.720]   This is the definition of the value.\n",
      "[00:47:59.720 --> 00:48:04.920]   However we immediately spot this approach of computation is like very expensive.\n",
      "[00:48:04.920 --> 00:48:05.920]   It's not very feasible.\n",
      "[00:48:05.920 --> 00:48:19.680]   The reason is this tau h has, this tau h has essentially s a to the power of h entries.\n",
      "[00:48:19.680 --> 00:48:25.900]   So we need to take a summation over this exponentially large number of entries.\n",
      "[00:48:25.900 --> 00:48:42.140]   This is exponential in h, which in general is like really bad if we just want to handle\n",
      "[00:48:42.140 --> 00:48:44.460]   some reasonably long sequence.\n",
      "[00:48:44.460 --> 00:48:49.940]   And this is already like extremely large and probably cannot fit in a computer memory.\n",
      "[00:48:49.940 --> 00:48:56.060]   So how is, is there any better way we can handle compute this, to evaluate this value?\n",
      "[00:48:56.060 --> 00:48:59.940]   So the secret is again, lies in the Markov structure.\n",
      "[00:48:59.940 --> 00:49:03.540]   We studied the Markov MDP for a reason just because it's Markov.\n",
      "[00:49:03.540 --> 00:49:08.300]   So a lot of things can be simplified and it can be more feasible.\n",
      "[00:49:08.300 --> 00:49:14.180]   So the way we're going to introduce essentially the key idea is by using dynamic programming.\n",
      "[00:49:14.180 --> 00:49:17.380]   So we will elaborate more later.\n",
      "[00:49:17.380 --> 00:49:21.860]   So we first will introduce the so-called Bellman equation, which is the most important, one\n",
      "[00:49:21.860 --> 00:49:34.020]   of the most important concepts in reinforcement learning.\n",
      "[00:49:34.020 --> 00:49:39.300]   So instead of just computing this value by brute force, we look at some intermediate\n",
      "[00:49:39.300 --> 00:49:51.700]   relation between this intermediate, like the value in the middle steps, VH and the QH.\n",
      "[00:49:51.700 --> 00:49:57.300]   So it turns out you can derive Bellman equation by directly expanding this definition.\n",
      "[00:49:57.300 --> 00:50:04.220]   And by this definition, you can already see this Q pi, the relation between Q pi and V\n",
      "[00:50:04.220 --> 00:50:10.380]   pi, V pi is probably just taking average of Q pi SA and taking an inner product of the\n",
      "[00:50:10.380 --> 00:50:15.260]   policy of taking some action in condition as essentially Bellman equation is an executive\n",
      "[00:50:15.260 --> 00:50:21.140]   relation that accepts the connection between the Q pi and the V pi at different steps.\n",
      "[00:50:21.140 --> 00:50:26.700]   So I would just directly write out the results, but you can check both equations is true based\n",
      "[00:50:26.700 --> 00:50:29.420]   on this definition.\n",
      "[00:50:29.420 --> 00:50:43.300]   So Bellman equation says VH pi S is equal to summation over A QH pi SA.\n",
      "[00:50:43.300 --> 00:50:52.300]   So the Q value of starting SA, and then times the probability of taking action A, given\n",
      "[00:50:52.300 --> 00:51:01.100]   as if all the policy pi.\n",
      "[00:51:01.100 --> 00:51:03.220]   This is relatively straightforward.\n",
      "[00:51:03.220 --> 00:51:08.100]   If you can look at the definition, this is like, this will take an expectation over\n",
      "[00:51:08.100 --> 00:51:14.420]   A H. Well, this will not take an expectation over A H because it's already specified.\n",
      "[00:51:14.420 --> 00:51:20.980]   And then expectation over A H, like by expanding out is like exactly like this.\n",
      "[00:51:20.980 --> 00:51:37.460]   And then the next equation is Q H pi SA is equal to R H. So the value if starting from\n",
      "[00:51:37.460 --> 00:51:44.140]   some state action pair at H step is equal to the immediate value I'm going to receive\n",
      "[00:51:44.140 --> 00:51:48.100]   at this state action pair and plus the future value.\n",
      "[00:51:48.100 --> 00:51:54.460]   And the future value is equal to the expectation, taking expectation over the next states, that\n",
      "[00:51:54.460 --> 00:52:05.260]   is a sample from this transition probability and V H plus 1 pi S prime.\n",
      "[00:52:05.260 --> 00:52:22.460]   And I just remember we will use simplifying notation later as this P H V H plus 1 pi SA\n",
      "[00:52:22.460 --> 00:52:35.980]   and 2 mean this.\n",
      "[00:52:35.980 --> 00:52:41.220]   So again, as I promised, this is directly by like a definition of this Q H and pi H, and\n",
      "[00:52:41.220 --> 00:52:42.220]   you can check offline.\n",
      "[00:52:42.220 --> 00:52:43.220]   This is true.\n",
      "[00:52:43.220 --> 00:52:53.020]   By just expanding the probability.\n",
      "[00:52:53.020 --> 00:52:57.500]   With this one, you can think it's a matrix vector modification.\n",
      "[00:52:57.500 --> 00:53:04.420]   This V as I said is a vector, and you can think this is a like transition is a matrix,\n",
      "[00:53:04.420 --> 00:53:25.300]   which is like SA by S dimension.\n",
      "[00:53:25.300 --> 00:53:28.820]   So why Bellman equation is kind of important?\n",
      "[00:53:28.820 --> 00:53:35.780]   Essentially, it's a relation between different values, but it also gives a recursive formula\n",
      "[00:53:35.780 --> 00:53:38.500]   of value at different steps.\n",
      "[00:53:38.500 --> 00:53:44.220]   And I think the important part is this actually kind of already directly give us a dynamic\n",
      "[00:53:44.220 --> 00:53:50.740]   programming way of computing the value, doing the policy evaluation.\n",
      "[00:53:50.740 --> 00:54:18.780]   So we'll just do the-- essentially, we will just compute V1 pi S1 by dynamic programming.\n",
      "[00:54:18.780 --> 00:54:23.420]   So let's just write out how we're going to compute this now, given this Bellman equation.\n",
      "[00:54:23.420 --> 00:54:40.580]   So what we can do is we can initialize the H plus 1 pi S equal to 0 for OS.\n",
      "[00:54:40.580 --> 00:54:44.580]   So we can think this is some imaginary stuff, because we already stop at H step.\n",
      "[00:54:44.580 --> 00:54:47.780]   So we really don't have anything like at H plus 1 step.\n",
      "[00:54:47.780 --> 00:54:53.820]   So why not just set the value equal to 0, because there is no reward at H plus 1 step\n",
      "[00:54:53.820 --> 00:54:54.820]   and later.\n",
      "[00:54:54.820 --> 00:54:59.540]   So we can just make this as an initial condition.\n",
      "[00:54:59.540 --> 00:55:06.780]   And then we can do the number of programming for H, little H, equal to H, H minus 1, dot\n",
      "[00:55:06.780 --> 00:55:14.740]   dot dot, till 1.\n",
      "[00:55:14.740 --> 00:55:27.420]   We can compute the Q pi S A, H. That is equal to R H S A. Essentially, we just use the Bellman\n",
      "[00:55:27.420 --> 00:55:28.420]   equation.\n",
      "[00:55:28.420 --> 00:55:43.060]   I'm just copying it here for clarity plus 1 pi S A is for any S A.\n",
      "[00:55:43.060 --> 00:56:01.620]   And V H pi S is equal to summation of A in action set, Q H pi S A times pi H of A given\n",
      "[00:56:01.620 --> 00:56:07.900]   S is for any S.\n",
      "[00:56:07.900 --> 00:56:16.060]   So this way we can imagine, we start from the V H plus 1, a capital H plus 1.\n",
      "[00:56:16.060 --> 00:56:20.300]   And then we use this equation to compute a Q capital H, and then use this equation to\n",
      "[00:56:20.300 --> 00:56:32.540]   compute a V capital H, and then Q capital H minus 1, V capital H minus 1.\n",
      "[00:56:32.540 --> 00:57:00.660]   So if we illustrate this in the figure, that is, we start with some V H plus 1, which\n",
      "[00:57:00.660 --> 00:57:10.100]   is S dimensional thing, V H plus 1, if you look at this as a vector, V H plus 1 pi is\n",
      "[00:57:10.100 --> 00:57:11.740]   S dimensional thing.\n",
      "[00:57:11.740 --> 00:57:21.020]   And then use the first equation, we will get to the Q H pi, which is S by A dimensional\n",
      "[00:57:21.020 --> 00:57:24.020]   thing.\n",
      "[00:57:24.020 --> 00:57:29.380]   So we will just use some computation here.\n",
      "[00:57:29.380 --> 00:57:44.780]   And then we will use this a Q H to get to the V H pi, which is the S dimensional thing.\n",
      "[00:57:44.780 --> 00:57:49.660]   And then we just use the standard programming goal onwards, and essentially we will go back\n",
      "[00:57:49.660 --> 00:58:00.780]   to the V Y, which is also S dimensional thing, and we just write one more here, Q H minus\n",
      "[00:58:00.780 --> 00:58:18.100]   1 pi, and if S by A, we have a lot of connections.\n",
      "[00:58:18.100 --> 00:58:21.900]   So it's very easy to verify by using this dimensional programming approach, we no longer\n",
      "[00:58:21.900 --> 00:58:25.740]   suffer this exponential complexity in computing V 1.\n",
      "[00:58:25.740 --> 00:58:47.820]   So now, this dynamic programming gives polynomial complexity to compute V 1, and this is a\n",
      "[00:58:47.820 --> 00:58:50.620]   polynomial image.\n",
      "[00:58:50.620 --> 00:58:54.820]   Essentially, we can think it would make longer the entire computation and it just grows linear\n",
      "[00:58:54.820 --> 00:59:20.780]   with respect to which.\n",
      "[00:59:20.780 --> 00:59:24.220]   Any question up to now?\n",
      "[00:59:24.220 --> 00:59:26.660]   So the question is the answer is very straightforward.\n",
      "[00:59:26.660 --> 00:59:33.780]   So to ask how to evaluate the value given a fixed policy pi, all we need to do is just\n",
      "[00:59:33.780 --> 00:59:49.340]   do the grammar programming and using the Bellman equation.\n",
      "[00:59:49.340 --> 00:59:53.300]   So if no problem, we have already talked about the policy evaluation.\n",
      "[00:59:53.300 --> 00:59:57.820]   And the next thing is, of course, we're not just happy about evaluating a single policy\n",
      "[00:59:57.820 --> 01:00:17.660]   and our ultimate goal is we want to find the optimal policy.\n",
      "[01:00:17.660 --> 01:00:29.420]   So the question is how we're going to find this argmax, V 1 pi of S1.\n",
      "[01:00:29.420 --> 01:00:40.020]   This is defined as the optimal policy.\n",
      "[01:00:40.020 --> 01:00:45.180]   So one straightforward idea we can probably think of is, OK, we already have this Bellman\n",
      "[01:00:45.180 --> 01:00:50.740]   equation to evaluate some fixed policy pi, and we have some Bellman equation type of\n",
      "[01:00:50.740 --> 01:00:57.500]   thing to do the optimal policy.\n",
      "[01:00:57.500 --> 01:01:03.180]   So one naive idea is we think this entire Bellman equation, if we look at the second\n",
      "[01:01:03.180 --> 01:01:06.860]   equation, it actually does not really depends on pi.\n",
      "[01:01:06.860 --> 01:01:11.180]   It can be anything because the pi only appears in the value.\n",
      "[01:01:11.180 --> 01:01:14.660]   In terms of computation, there's nothing related to pi.\n",
      "[01:01:14.660 --> 01:01:20.380]   So pi only appears in the first equation, where this is taking average over the probability\n",
      "[01:01:20.380 --> 01:01:22.700]   that indicated by pi.\n",
      "[01:01:22.700 --> 01:01:28.060]   So if you want to do the optimal policy, why not we just focus on all the probability\n",
      "[01:01:28.060 --> 01:01:33.300]   on the action that generates the highest Q value instead of doing some average.\n",
      "[01:01:33.300 --> 01:01:38.980]   So what about every time I just do the max over A instead of doing some average over A?\n",
      "[01:01:38.980 --> 01:01:57.060]   This is what we call a Bellman optimality equation.\n",
      "[01:01:57.060 --> 01:02:01.460]   So it's again a set of equations, which the only difference is that we're going to change\n",
      "[01:02:01.460 --> 01:02:04.980]   this average to the maximum.\n",
      "[01:02:04.980 --> 01:02:16.380]   So the first equation is V star S is equal to max over A QH star S A. And the second\n",
      "[01:02:16.380 --> 01:02:19.220]   is exactly the same.\n",
      "[01:02:19.220 --> 01:02:40.660]   Q star S A is equal to R S A plus PV star S A. So this is our gas in terms of an optimal\n",
      "[01:02:40.660 --> 01:02:51.580]   policy with a set of equations and how we're going to get a policy here.\n",
      "[01:02:51.580 --> 01:02:58.020]   So we can read from there, the policy essentially is identified by this taking average according\n",
      "[01:02:58.020 --> 01:03:01.220]   to the probability that is sampled from pi.\n",
      "[01:03:01.220 --> 01:03:05.340]   So this equation kind of naturally gives a policy that is essentially taking the max\n",
      "[01:03:05.340 --> 01:03:06.900]   over this Q star.\n",
      "[01:03:06.900 --> 01:03:22.420]   So we'll just define this policy, pi star H A given S. This is my policy.\n",
      "[01:03:22.420 --> 01:03:30.540]   My gas to be the optimal policy, which is equal to the indicator of A equal to R max\n",
      "[01:03:30.540 --> 01:03:48.340]   A of Q star S A. Essentially, I'm just going to put all my probability into the action which\n",
      "[01:03:48.340 --> 01:03:52.540]   maximizes the Q star.\n",
      "[01:03:52.540 --> 01:03:58.260]   And if there are some like tie, tie, I just do some arbitrary tie breaking, I can just\n",
      "[01:03:58.260 --> 01:04:22.740]   focus on one particular action that I'll show you for the maximum.\n",
      "[01:04:22.740 --> 01:04:35.100]   So, nothing very difficult, but as I said, everything is my gas.\n",
      "[01:04:35.100 --> 01:04:40.460]   I think I just modified the Bellman equation and my gas, my policy is like this.\n",
      "[01:04:40.460 --> 01:04:46.860]   So the first thing I noticed is we first defined this V star.\n",
      "[01:04:46.860 --> 01:04:51.380]   I don't even know whether this value is corresponding to any policy.\n",
      "[01:04:51.380 --> 01:04:52.380]   This is a value for any policy.\n",
      "[01:04:52.380 --> 01:04:53.940]   I don't know.\n",
      "[01:04:53.940 --> 01:05:04.540]   But we can first argue that V star H is actually just equal to V H pi star.\n",
      "[01:05:04.540 --> 01:05:19.060]   And Q H star is equal to Q H pi star.\n",
      "[01:05:19.060 --> 01:05:27.980]   It's easy to prove this because the Bellman equation, because pi star is some particular\n",
      "[01:05:27.980 --> 01:05:41.460]   policy and so pi stars, you can think the value of pi star satisfy the Bellman equation.\n",
      "[01:05:41.460 --> 01:05:45.740]   And if you write out the Bellman equation of pi star, you can realize this is exactly\n",
      "[01:05:45.740 --> 01:05:56.180]   equal to the Bellman automatic equation, like they are, they look exactly the same.\n",
      "[01:05:56.180 --> 01:06:07.200]   So, the first thing is we convince ourselves when we write out this Bellman automatic equation,\n",
      "[01:06:07.200 --> 01:06:10.020]   this actually corresponds to the value of this particular policy.\n",
      "[01:06:10.020 --> 01:06:14.820]   This is not some arbitrary thing, it's kind of corresponding to some particular, that\n",
      "[01:06:14.820 --> 01:06:21.820]   particular policy.\n",
      "[01:06:21.820 --> 01:06:29.700]   And the second thing we're going to claim, proposition two, is I'm going to claim this\n",
      "[01:06:29.700 --> 01:06:35.460]   is not my wild guess, actually this is in particular indeed the optimal policy.\n",
      "[01:06:35.460 --> 01:06:47.020]   So, let's say pi star is the optimal policy.\n",
      "[01:06:47.020 --> 01:07:15.900]   I think we kind of, because we're doing like theoretical work, theoretical, like essential\n",
      "[01:07:15.900 --> 01:07:21.380]   foundation, so we want to make sure the logic is clear, like we don't have like some cyclic\n",
      "[01:07:21.380 --> 01:07:26.580]   reasoning in the middle, but like in the end, after you kind of knows everything, you just\n",
      "[01:07:26.580 --> 01:07:30.700]   know this is like Bellman automatic equation, this is like equal to the Bellman equation\n",
      "[01:07:30.700 --> 01:07:38.500]   of pi star and everything is like pretty straightforward.\n",
      "[01:07:38.500 --> 01:07:49.620]   So, now we will prove this claim, so in order to prove this claim, we actually will prove\n",
      "[01:07:49.620 --> 01:07:54.820]   even stronger claim and where this claim is like a special case of my stronger claim.\n",
      "[01:07:54.820 --> 01:08:08.380]   So, we will actually prove that the pi star h of s, this is like the value of pi star\n",
      "[01:08:08.380 --> 01:08:20.380]   at step h to state s, is actually equal to max of all the Markov policy, pi in Markov\n",
      "[01:08:20.380 --> 01:08:44.340]   v h, pi s for any h and s.\n",
      "[01:08:44.340 --> 01:08:56.700]   Okay, I just want to explain a little bit difference between those two claims.\n",
      "[01:08:56.700 --> 01:09:01.660]   I think the original optimal policy only says you need to, this is only defined, you need\n",
      "[01:09:01.660 --> 01:09:07.500]   to maximize the initial value, initial value at the initial states.\n",
      "[01:09:07.500 --> 01:09:24.980]   Well, this essentially says pi star is the optimal policy for all states and all h\n",
      "[01:09:24.980 --> 01:09:38.500]   steps simultaneously, so that means like if you look at different states, the optimal\n",
      "[01:09:38.500 --> 01:09:42.100]   policy is actually not different policy, they are actually the exact same policy which\n",
      "[01:09:42.100 --> 01:09:47.140]   is the pi star we define here, this is a much stronger claim, but you can think of one\n",
      "[01:09:47.140 --> 01:09:51.420]   very special case is I just pick h, h equal to one as equal to the initial states and\n",
      "[01:09:51.420 --> 01:09:58.140]   this gives a, what I want to claim in the proposition two.\n",
      "[01:09:58.140 --> 01:10:19.620]   Okay, so again we will do induction, so the only difference of this induction is we no\n",
      "[01:10:19.620 --> 01:10:23.660]   longer do induction from the first step to the last step and we will do the opposite\n",
      "[01:10:23.660 --> 01:10:28.820]   direction because this is optimal policy, so we will first argue the last step is optimal,\n",
      "[01:10:28.820 --> 01:10:38.820]   then the second and last step is optimal and then going backwards.\n",
      "[01:10:38.820 --> 01:10:46.180]   So the essentially logic we are going to do is we are going to argue this v h as equal\n",
      "[01:10:46.180 --> 01:11:00.660]   to max pi v pi h as this is what we want to say in here and we will argue this by we\n",
      "[01:11:00.660 --> 01:11:18.080]   first say first step, q pi star as a is actually equal to the max pi q pi h as a and the second\n",
      "[01:11:18.080 --> 01:11:30.760]   step is we will use the next step to argue this that is v h plus one, pi star, s is equal\n",
      "[01:11:30.760 --> 01:11:48.420]   to max pi v h pi s is for any s, for any s a h a this is also for any s okay, so we\n",
      "[01:11:48.420 --> 01:11:54.480]   induction backwards, we will argue if the value at h plus one step is already the optimal\n",
      "[01:11:54.480 --> 01:11:59.880]   one then going backwards at h step q value is optimal then v value is also optimal.\n",
      "[01:11:59.880 --> 01:12:29.840]   Okay, let us always start from the base case.\n",
      "[01:12:29.840 --> 01:12:35.080]   This case is the final step and I think we say that we can use the dummy step which is\n",
      "[01:12:35.080 --> 01:12:41.040]   h plus one step, in the h plus one step there is no reward going forward, so we can immediately\n",
      "[01:12:41.040 --> 01:12:56.320]   say this is true because there like really no policy in the future and everything is\n",
      "[01:12:56.320 --> 01:13:07.540]   just equal to zero.\n",
      "[01:13:07.540 --> 01:13:29.460]   So we will quickly do one and two, let us do this one first, first we look at the max\n",
      "[01:13:29.460 --> 01:13:46.980]   pi q h pi as a and by bounding equation we know this is equal to r as a and because\n",
      "[01:13:46.980 --> 01:13:51.500]   r does not involve pi so we no longer need to take a max over this pi and we can push\n",
      "[01:13:51.500 --> 01:14:03.620]   maxing inside, we say this is a max pi over expectation s prime sample from q s a and\n",
      "[01:14:03.620 --> 01:14:24.460]   v h plus one pi as prime.\n",
      "[01:14:24.460 --> 01:14:30.940]   I think the important thing is now we want to essentially we want to switch the order\n",
      "[01:14:30.940 --> 01:14:37.860]   of the max and expectation, so in general like let us just be more concrete we want\n",
      "[01:14:37.860 --> 01:14:54.260]   to essentially do max pi summation over s prime p s prime given as a h and v h plus\n",
      "[01:14:54.260 --> 01:15:07.020]   one s prime and in general we cannot swap this max and summation so in general if we\n",
      "[01:15:07.020 --> 01:15:18.280]   look have this max fx plus gx like we had two x thing and we want to take the max, in\n",
      "[01:15:18.280 --> 01:15:34.120]   general this is a will be smaller than the max of fx and plus max of gx okay just because\n",
      "[01:15:34.120 --> 01:15:39.080]   this max can be achieved at different place, if this max at different place then this is\n",
      "[01:15:39.080 --> 01:15:55.560]   strictly less than that however if we have that arg max of fx is equal to arg max of\n",
      "[01:15:55.560 --> 01:16:03.700]   gx that is there max can be achieved simultaneously then we actually have this equation that\n",
      "[01:16:03.700 --> 01:16:14.500]   is max x of fx plus gx is precisely equal to max x\n",
      "[01:16:14.500 --> 01:16:40.420]   and max gx okay and this is essentially what we can do here because by induction hypothesis\n",
      "[01:16:40.420 --> 01:16:47.500]   we can like say the pi star actually achieve the maximum policy simultaneously for all\n",
      "[01:16:47.500 --> 01:16:55.940]   the state s for all the next state s that is the maximizing policy here for pi is always\n",
      "[01:16:55.940 --> 01:17:02.440]   pi star no matter what s prime is so we can say this is actually equal to we push the\n",
      "[01:17:02.440 --> 01:17:16.240]   maximization inside because all the maxes achieve simultaneous at a pi star so this\n",
      "[01:17:16.240 --> 01:17:42.800]   is just equal to bh plus 1 pi star s prime and finally we realize this is just a Bellman\n",
      "[01:17:42.800 --> 01:18:07.280]   equation for pi star and this is equal to QH pi star s a okay I will just do like two\n",
      "[01:18:07.280 --> 01:18:12.440]   more minutes to finish the proof I think similarly we can also just argue the same\n",
      "[01:18:12.440 --> 01:18:24.280]   thing that is the we start from the max H bH pi s this again by Bellman equation is equal\n",
      "[01:18:24.280 --> 01:18:47.320]   to max pi summation a QH pi s a of pi a given s and this is a h so the important part is\n",
      "[01:18:47.320 --> 01:18:52.380]   that we can realize that we can we can like separate this pi because this pi is essentially\n",
      "[01:18:52.380 --> 01:18:58.480]   the collection of all the possible future policies so we have this we can take a max\n",
      "[01:18:58.480 --> 01:19:09.520]   over pi H here and max over all the future policies summation a QH pi s a and a pi H\n",
      "[01:19:09.520 --> 01:19:15.180]   a given s and the other thing is realize this Q value actually only depends on the policy\n",
      "[01:19:15.180 --> 01:19:19.060]   in the future so it doesn't depends on the current policy of current step because current\n",
      "[01:19:19.060 --> 01:19:30.000]   step we're already taking some action a so this only depends on pi H plus 1 2 pi cap\n",
      "[01:19:30.000 --> 01:19:36.680]   to H and again we can use the same argument that because we achieve the max at the same\n",
      "[01:19:36.680 --> 01:19:59.140]   place so we can essentially swap this max inside and which more to skip what we want\n",
      "[01:19:59.140 --> 01:20:06.200]   and it's just equal to max over pi H summation a because we can push the maximization over\n",
      "[01:20:06.200 --> 01:20:18.320]   all the future policy inside and it becomes the Q star s a and pi H a given s and finally\n",
      "[01:20:18.320 --> 01:20:23.020]   we know the maximization of the policy achieved at so we put a property to one for the maximizing\n",
      "[01:20:23.020 --> 01:20:37.460]   action so this is just equal to the max over a Q star s a Q pi star and which is equal\n",
      "[01:20:37.460 --> 01:20:52.700]   to V H pi star s I think this finish the proof that we accurate as pi star is an optimum\n",
      "[01:20:52.700 --> 01:20:58.580]   policy in next lecture we'll continue talking about some planning algorithm essentially\n",
      "[01:20:58.580 --> 01:21:02.940]   what is the algorithm we're going to compute the optimum policy and then we're going to\n",
      "[01:21:02.940 --> 01:21:07.220]   start talk about a little bit more challenging setting where the environment is unknown and\n",
      "[01:21:07.220 --> 01:21:13.100]   we need to how we going to develop some tools to use data to estimate this environment to\n",
      "[01:21:13.100 --> 01:21:15.640]   learn that okay thanks\n",
      "[01:21:15.640 --> 01:21:17.440]   the next lecture.\n",
      "[01:21:17.440 --> 01:21:20.020]   (upbeat music)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/YEOAn9FIvyI.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/YEOAn9FIvyI.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/YEOAn9FIvyI.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/YEOAn9FIvyI.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1279.87 ms\n",
      "whisper_print_timings:     fallbacks =   3 p /   1 h\n",
      "whisper_print_timings:      mel time =  2775.02 ms\n",
      "whisper_print_timings:   sample time = 21974.72 ms / 56959 runs (    0.39 ms per run)\n",
      "whisper_print_timings:   encode time =   375.24 ms /   210 runs (    1.79 ms per run)\n",
      "whisper_print_timings:   decode time =   828.53 ms /   436 runs (    1.90 ms per run)\n",
      "whisper_print_timings:   batchd time = 28837.54 ms / 55460 runs (    0.52 ms per run)\n",
      "whisper_print_timings:   prompt time = 10703.61 ms / 47526 runs (    0.23 ms per run)\n",
      "whisper_print_timings:    total time = 67248.55 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n",
      "Downloading video https://www.youtube.com/watch?v=QwTBtKWmA0M started\n",
      "QwTBtKWmA0M\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/QwTBtKWmA0M.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/QwTBtKWmA0M.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Google\n",
      "  Duration: 01:20:59.91, start: 0.000000, bitrate: 285 kb/s\n",
      "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 640x360 [SAR 1:1 DAR 16:9], 185 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 95 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/QwTBtKWmA0M.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : ISO Media file produced by Google Inc.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  151872kB time=01:20:59.91 bitrate= 256.0kbits/s speed=1.36e+03x    \n",
      "video:0kB audio:151872kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000050%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/QwTBtKWmA0M.wav' (77758613 samples, 4859.9 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:03.120]   [MUSIC]\n",
      "[00:00:03.120 --> 00:00:05.760]   >> Hello everyone, I'm Chijing,\n",
      "[00:00:05.760 --> 00:00:07.960]   I mean from ECE departments.\n",
      "[00:00:07.960 --> 00:00:11.240]   So this year we will be working together on\n",
      "[00:00:11.240 --> 00:00:14.560]   this course called Foundations of Reinforcement Learning.\n",
      "[00:00:14.560 --> 00:00:16.480]   It's called Foundations but it's really\n",
      "[00:00:16.480 --> 00:00:19.760]   the mathematical foundations of Reinforcement Learning.\n",
      "[00:00:19.760 --> 00:00:23.760]   As you can see and this course will be videotaped,\n",
      "[00:00:23.760 --> 00:00:27.320]   but it's basically for like open lecture and\n",
      "[00:00:27.320 --> 00:00:31.160]   for external students and you are still very\n",
      "[00:00:31.160 --> 00:00:34.000]   welcome to ask questions if you have anything you're unclear,\n",
      "[00:00:34.000 --> 00:00:37.600]   like feel free to ask and they will be all very valuable.\n",
      "[00:00:37.600 --> 00:00:41.880]   We'll first do a very brief intro of the course and most of\n",
      "[00:00:41.880 --> 00:00:44.320]   the course because it's like a mathematical foundation.\n",
      "[00:00:44.320 --> 00:00:46.280]   Most of the course will do blackboard,\n",
      "[00:00:46.280 --> 00:00:49.100]   like derivation and on the concepts,\n",
      "[00:00:49.100 --> 00:00:50.880]   but it's just the very first lecture,\n",
      "[00:00:50.880 --> 00:00:53.480]   we'll do a slide like slides.\n",
      "[00:00:53.480 --> 00:00:55.800]   Do a overview.\n",
      "[00:00:55.800 --> 00:00:59.100]   So first I want to talk about,\n",
      "[00:00:59.100 --> 00:01:01.320]   this is like the foundations of Reinforcement Learning.\n",
      "[00:01:01.320 --> 00:01:04.360]   I think in before like digging into\n",
      "[00:01:04.360 --> 00:01:06.880]   the details of what is the mathematical foundations,\n",
      "[00:01:06.880 --> 00:01:09.920]   we first should have something in mind like what are\n",
      "[00:01:09.920 --> 00:01:13.520]   the angle or what are the applications\n",
      "[00:01:13.520 --> 00:01:16.200]   we will develop like this Reinforcement Learning for.\n",
      "[00:01:16.200 --> 00:01:18.680]   I think the modern Reinforcement Learning essentially has been\n",
      "[00:01:18.680 --> 00:01:22.480]   applied to a wide range of areas, a lot of applications.\n",
      "[00:01:22.480 --> 00:01:24.600]   I would just say something like very famous and\n",
      "[00:01:24.600 --> 00:01:28.640]   have been successful, like partially successful in the field.\n",
      "[00:01:28.640 --> 00:01:31.400]   I think the first thing you can probably say is like robotics,\n",
      "[00:01:31.400 --> 00:01:34.080]   like manipulation, if you have ever saw like recently,\n",
      "[00:01:34.080 --> 00:01:36.080]   like Google and Berkeley,\n",
      "[00:01:36.080 --> 00:01:39.360]   a lot of institute has announced this RT2 essentially,\n",
      "[00:01:39.360 --> 00:01:42.880]   they can use like Reinforcement Learning and also some other methodology\n",
      "[00:01:42.880 --> 00:01:47.480]   to train robots to manipulate a lot of\n",
      "[00:01:47.480 --> 00:01:52.520]   different objects and across different type of robots.\n",
      "[00:01:52.520 --> 00:01:56.060]   And robotics, I think I would say is a top one,\n",
      "[00:01:56.060 --> 00:01:58.360]   major application of Reinforcement Learning.\n",
      "[00:01:58.360 --> 00:02:02.000]   And the second thing is like the StarCraft or\n",
      "[00:02:02.000 --> 00:02:04.240]   like this strategic game, video games,\n",
      "[00:02:04.240 --> 00:02:08.360]   or I think it's like a prototype of this Reinforcement Learning.\n",
      "[00:02:08.360 --> 00:02:12.640]   You probably have already heard of this RFR star,\n",
      "[00:02:12.640 --> 00:02:16.000]   which has been already several years.\n",
      "[00:02:16.000 --> 00:02:18.240]   Like a deep Google DeepMind,\n",
      "[00:02:18.240 --> 00:02:20.680]   they train a very good superhuman AI.\n",
      "[00:02:20.680 --> 00:02:24.760]   They're able to like beat the world champion on this complicated video games,\n",
      "[00:02:24.760 --> 00:02:28.440]   where there are a lot of like strategy involved in this kind of scenario.\n",
      "[00:02:28.440 --> 00:02:31.720]   And the last two applications you probably all well known,\n",
      "[00:02:31.720 --> 00:02:33.520]   the first one is autonomous driving.\n",
      "[00:02:33.520 --> 00:02:37.280]   And the second one is like a very, very hot last year chat GBT.\n",
      "[00:02:37.280 --> 00:02:41.000]   And they claim also to use some technology about Reinforcement Learning\n",
      "[00:02:41.000 --> 00:02:46.520]   from human feedback in the last stage to fine tune this like large language model.\n",
      "[00:02:46.520 --> 00:02:49.560]   So if you look at all those applications,\n",
      "[00:02:49.560 --> 00:02:52.360]   what are the common things over those applications?\n",
      "[00:02:52.360 --> 00:02:57.000]   It really like all those complications involve to make decisions.\n",
      "[00:02:57.000 --> 00:03:00.840]   And not only just to make a single decision, but to make a sequence of decisions.\n",
      "[00:03:00.840 --> 00:03:03.360]   So it's like sequential decision making problem.\n",
      "[00:03:03.360 --> 00:03:06.000]   So I would say the Reinforcement Learning is really the framework,\n",
      "[00:03:06.000 --> 00:03:10.240]   which is designed to handle those sequential decision making problems.\n",
      "[00:03:10.240 --> 00:03:15.000]   Okay, so those are like defensive applications.\n",
      "[00:03:15.000 --> 00:03:18.960]   But this semester we will dig more into like this foundations.\n",
      "[00:03:18.960 --> 00:03:23.240]   So we will look more for what is underlying those applications,\n",
      "[00:03:23.240 --> 00:03:28.840]   like what is the mathematical principles that's going on like behind those applications.\n",
      "[00:03:28.840 --> 00:03:31.040]   So in order to talk about Reinforcement Learning,\n",
      "[00:03:31.040 --> 00:03:35.760]   I think all those applications can be abstracted into that this like diagram.\n",
      "[00:03:35.760 --> 00:03:39.760]   Where basically we have agents and we also have an environment.\n",
      "[00:03:39.760 --> 00:03:44.880]   Agent is essentially the learner or the AI we want to build, which will take some action.\n",
      "[00:03:44.880 --> 00:03:48.000]   And this action will affect how environment is going to change\n",
      "[00:03:48.000 --> 00:03:52.800]   or transition to the next stage where the next stage we call like states.\n",
      "[00:03:52.800 --> 00:03:56.360]   So we will still states to describe the environment.\n",
      "[00:03:56.360 --> 00:03:58.080]   And in addition to states,\n",
      "[00:03:58.080 --> 00:04:00.760]   environment will also give us the reward.\n",
      "[00:04:00.760 --> 00:04:04.120]   So we can basically look at this chess game as an example,\n",
      "[00:04:04.120 --> 00:04:08.440]   where essentially the environment is this like chess board.\n",
      "[00:04:08.440 --> 00:04:12.840]   And the agent is like we, for example, we want to play this white pieces.\n",
      "[00:04:12.840 --> 00:04:16.680]   And the action we're going to take is like we're going to move specifically pieces\n",
      "[00:04:16.680 --> 00:04:18.200]   to some other location.\n",
      "[00:04:18.200 --> 00:04:20.320]   And the environment will change to the next stage,\n",
      "[00:04:20.320 --> 00:04:23.800]   which is essentially the configuration of this chess board.\n",
      "[00:04:23.800 --> 00:04:27.920]   And the reward you can think of reward is just like eventually if you win the game,\n",
      "[00:04:27.920 --> 00:04:29.240]   you get a reward one.\n",
      "[00:04:29.240 --> 00:04:32.560]   And if you lose, you get a reward zero, okay.\n",
      "[00:04:32.560 --> 00:04:36.240]   And in general, Reinforcement can be formulated as a final good policy\n",
      "[00:04:36.240 --> 00:04:37.880]   to maximize a cumulative reward.\n",
      "[00:04:37.880 --> 00:04:45.920]   So this is like a brief, very, very brief introduction\n",
      "[00:04:45.920 --> 00:04:50.160]   of what is Reinforcement about from the application and from like a little bit\n",
      "[00:04:50.160 --> 00:04:53.600]   of like the model underlines it.\n",
      "[00:04:53.600 --> 00:04:57.720]   So we will, I think we'll briefly talk about what will be covered in this course\n",
      "[00:04:57.720 --> 00:05:00.200]   about this Reinforcement Learning and will not be covered.\n",
      "[00:05:00.200 --> 00:05:04.960]   So first we will cover some mathematical models,\n",
      "[00:05:04.960 --> 00:05:07.280]   like underlying the Reinforcement Learning.\n",
      "[00:05:07.280 --> 00:05:11.600]   Like I think a big part of this course will be devoted\n",
      "[00:05:11.600 --> 00:05:14.120]   to this so-called Markov decision process.\n",
      "[00:05:14.120 --> 00:05:18.880]   We will actually introduce this model later in this class.\n",
      "[00:05:18.880 --> 00:05:25.080]   But essentially, we'll like very regularly formulate how this sequential process goes\n",
      "[00:05:25.080 --> 00:05:30.600]   on and what a kind of quantity governs like how this stays sort of the environment\n",
      "[00:05:30.600 --> 00:05:32.120]   like move forwards.\n",
      "[00:05:32.120 --> 00:05:35.200]   And we will also consider a lot of different settings like simulator\n",
      "[00:05:35.200 --> 00:05:38.800]   and the setting we require exploration and also the setting we require function\n",
      "[00:05:38.800 --> 00:05:40.520]   approximation or something like that.\n",
      "[00:05:40.520 --> 00:05:44.800]   Which essentially is, we will also like essentially identify the challenges\n",
      "[00:05:44.800 --> 00:05:50.080]   and also talk about the strategies, how to solve those challenges.\n",
      "[00:05:50.080 --> 00:05:53.560]   I think challenges involve like we will do sequential decision making.\n",
      "[00:05:53.560 --> 00:05:56.760]   So we'll handle a lot of like temporal correlation.\n",
      "[00:05:56.760 --> 00:06:01.160]   We'll also talk about how to balance exploration versus exploitation,\n",
      "[00:06:01.160 --> 00:06:04.000]   which is a very important topic in like the online decision making\n",
      "[00:06:04.000 --> 00:06:07.840]   or online reinforcement learning, where we need to collect samples\n",
      "[00:06:07.840 --> 00:06:09.840]   or collect data by ourselves.\n",
      "[00:06:09.840 --> 00:06:13.920]   We'll also talk about how to handle like the large states in the sense\n",
      "[00:06:13.920 --> 00:06:18.320]   like classical reinforcement learning always talk about we have finite states\n",
      "[00:06:18.320 --> 00:06:22.800]   but the real application like video games usually have like millions of states\n",
      "[00:06:22.800 --> 00:06:26.720]   and how we're going to use function approximation to handle those challenges.\n",
      "[00:06:26.720 --> 00:06:32.680]   And finally, we will also spend the last part of the lecture of the semester talking\n",
      "[00:06:32.680 --> 00:06:37.840]   about how to do like multi-agent reinforcement learning like the multi-agency part,\n",
      "[00:06:37.840 --> 00:06:39.480]   collaboration and the competition.\n",
      "[00:06:39.480 --> 00:06:43.720]   And finally, how we going to handle some partial observability like in a sense,\n",
      "[00:06:43.720 --> 00:06:48.680]   some states may not be fully observable and we need to infer what is underlying states\n",
      "[00:06:48.680 --> 00:06:51.840]   from the limited observation.\n",
      "[00:06:51.840 --> 00:06:56.720]   So we'll talk about model, we'll talk about challenge and how to solve those challenges.\n",
      "[00:06:56.720 --> 00:07:02.040]   And also very important part of the course will be devoted on like designing algorithms\n",
      "[00:07:02.040 --> 00:07:07.600]   or talk about what kind of algorithm would work for those kind of to solve those challenges.\n",
      "[00:07:07.600 --> 00:07:12.320]   And the algorithms including the classical important like algorithm like value iteration,\n",
      "[00:07:12.320 --> 00:07:16.600]   curating and also function approximation like fit accuracy and basically all the\n",
      "[00:07:16.600 --> 00:07:19.920]   important reinforcement algorithm.\n",
      "[00:07:19.920 --> 00:07:25.080]   We'll also talk about some additional algorithm component which is not considered\n",
      "[00:07:25.080 --> 00:07:28.320]   just directly like planning like the curating type of algorithm\n",
      "[00:07:28.320 --> 00:07:33.560]   but more like to help those like core reinforcement algorithms to success.\n",
      "[00:07:33.560 --> 00:07:37.240]   For example, like we talk about exploration versus exploitation.\n",
      "[00:07:37.240 --> 00:07:42.160]   So in addition to like value iteration, curating those like classical reinforcement algorithm,\n",
      "[00:07:42.160 --> 00:07:44.160]   we also need to do some exploration.\n",
      "[00:07:44.160 --> 00:07:47.120]   So what is like a principle way to do exploration?\n",
      "[00:07:47.120 --> 00:07:51.320]   And this course will essentially like leverage so-called optimism principle\n",
      "[00:07:51.320 --> 00:07:55.720]   and design some like upper confidence bound algorithm and combine it with standard reinforcement\n",
      "[00:07:55.720 --> 00:08:00.480]   algorithm and show how those algorithm will perform and.\n",
      "[00:08:00.480 --> 00:08:05.200]   And also we will also talk about some offline scenario where we have a lot of data already available.\n",
      "[00:08:05.200 --> 00:08:08.960]   And we can use instead of optimism like a pessimism.\n",
      "[00:08:08.960 --> 00:08:13.760]   So those are the algorithms and algorithm design part.\n",
      "[00:08:13.760 --> 00:08:18.200]   And finally also the most important part of the course in addition to models and algorithm\n",
      "[00:08:18.200 --> 00:08:20.200]   we'll also talk about theoretical guarantees.\n",
      "[00:08:20.200 --> 00:08:25.040]   So this is really like the mathematical foundation part which really differentiate this course\n",
      "[00:08:25.040 --> 00:08:29.840]   from a lot of other more empirical version of the machine learning course or reinforcement\n",
      "[00:08:29.840 --> 00:08:36.240]   learning course, essentially we care about use mathematics to provide guarantees to algorithm\n",
      "[00:08:36.240 --> 00:08:42.280]   say it always probably gonna find the quantity or the optimal policy or the quantity we want\n",
      "[00:08:42.280 --> 00:08:47.080]   to find for like a wide range class of applications.\n",
      "[00:08:47.080 --> 00:08:50.160]   So what type of theoretical guarantees will talk about in this class?\n",
      "[00:08:50.160 --> 00:08:54.080]   I think I would say mostly we talk about two type of theoretical guarantees.\n",
      "[00:08:54.080 --> 00:08:57.080]   The first type is like the sample efficiency.\n",
      "[00:08:57.080 --> 00:09:03.400]   So efficiency essentially says how many samples, how many, like this is the official question.\n",
      "[00:09:03.400 --> 00:09:07.960]   Like how many samples we need to learn the optimal policy of the, for example, autonomous\n",
      "[00:09:07.960 --> 00:09:09.880]   driving or robotics.\n",
      "[00:09:09.880 --> 00:09:14.280]   So in terms of samples essentially like how many games we need to play like in the sense\n",
      "[00:09:14.280 --> 00:09:21.360]   of if we do the Starcraft as application or like how many trajectories we need to do like\n",
      "[00:09:21.360 --> 00:09:23.080]   in terms of robotics.\n",
      "[00:09:23.080 --> 00:09:27.760]   So classical algorithms or like I would say state of art existing algorithm always require\n",
      "[00:09:27.760 --> 00:09:33.720]   like a massive number of samples easily go to like millions or sometimes even like hundreds\n",
      "[00:09:33.720 --> 00:09:35.520]   of millions samples.\n",
      "[00:09:35.520 --> 00:09:39.960]   And for a lot of applications like for example autonomous driving, like collecting samples\n",
      "[00:09:39.960 --> 00:09:44.640]   can be really expensive like you need to spend all gas and on those kind of locations and\n",
      "[00:09:44.640 --> 00:09:49.200]   also it can be time consuming like one trajectory will take 10 minutes or half an hour or something\n",
      "[00:09:49.200 --> 00:09:50.360]   like that.\n",
      "[00:09:50.360 --> 00:09:54.360]   So in this kind of scenario, how to reduce sample complexity is like a very important\n",
      "[00:09:54.360 --> 00:09:57.600]   topics for like those kind of occasions.\n",
      "[00:09:57.600 --> 00:10:01.800]   So that's why our theory would, I think one very important thing like centered on the sample\n",
      "[00:10:01.800 --> 00:10:06.680]   complexity really talking about what kind of sample complexity we can achieve and whether\n",
      "[00:10:06.680 --> 00:10:11.080]   those are optimal, whether there are some better algorithm that can achieve like better\n",
      "[00:10:11.080 --> 00:10:13.920]   sample complexity.\n",
      "[00:10:13.920 --> 00:10:18.680]   The second part, second kind of guarantees I will talk about is the computational efficiency.\n",
      "[00:10:18.680 --> 00:10:24.800]   So I think I would say like in general machine learning is just statistics and computer science.\n",
      "[00:10:24.800 --> 00:10:27.920]   So statistics are like corresponding to the sample complexity and computer science more\n",
      "[00:10:27.920 --> 00:10:31.200]   than corresponding to like computational efficiency.\n",
      "[00:10:31.200 --> 00:10:36.320]   And state of art models usually like trains in weeks and a month, if you already know like\n",
      "[00:10:36.320 --> 00:10:42.120]   those big projects are a first are they need like the entire Google DeepMind cluster and\n",
      "[00:10:42.120 --> 00:10:48.040]   they actually trees like a several weeks which basically in academia we can never like we\n",
      "[00:10:48.040 --> 00:10:50.320]   cannot really do something like that.\n",
      "[00:10:50.320 --> 00:10:56.080]   So how we can reduce this computational efficiency and eventually make it more manageable or\n",
      "[00:10:56.080 --> 00:11:02.680]   more like more accessible to like academia or like even the daily computing that kind\n",
      "[00:11:02.680 --> 00:11:03.680]   of thing.\n",
      "[00:11:03.680 --> 00:11:09.280]   So this is also another important topics in our theory.\n",
      "[00:11:09.280 --> 00:11:13.520]   So those are things we will talk about in this class and more importantly we will just\n",
      "[00:11:13.520 --> 00:11:18.800]   do everything very regularly in math and using proofs.\n",
      "[00:11:18.800 --> 00:11:22.760]   And we will also talk about what we will not talk about in this class and because we really\n",
      "[00:11:22.760 --> 00:11:29.880]   devoted to this math and like the proofs I think including some coding projects or everything\n",
      "[00:11:29.880 --> 00:11:35.240]   would make this course like too big and too much.\n",
      "[00:11:35.240 --> 00:11:42.720]   So we will not talk about anything about coding some real algorithms like for some like benchmarks\n",
      "[00:11:42.720 --> 00:11:49.640]   or we will not have like any collab for this course.\n",
      "[00:11:49.640 --> 00:11:55.120]   Just to summarize and I would like to say this is like a really advanced graduate level\n",
      "[00:11:55.120 --> 00:11:57.680]   course on mathematical foundations for reinforcement learning.\n",
      "[00:11:57.680 --> 00:12:02.600]   So this supposed to not be like some intro level like your first course in machine learning\n",
      "[00:12:02.600 --> 00:12:06.720]   or something I don't think that would be a good idea.\n",
      "[00:12:06.720 --> 00:12:10.880]   And I think we talk about we will cover mathematical models, we will talk about different settings,\n",
      "[00:12:10.880 --> 00:12:15.200]   challenges, we will talk about algorithms and algorithm designs and the most importantly\n",
      "[00:12:15.200 --> 00:12:18.700]   we will talk about theoretical analysis and theoretical guarantees in terms of both sample\n",
      "[00:12:18.700 --> 00:12:22.840]   complexity and computational complexity and we will do everything in math.\n",
      "[00:12:22.840 --> 00:12:28.560]   And finally there is no coding or implementation and we will talk a little bit about some deep\n",
      "[00:12:28.560 --> 00:12:33.640]   power algorithms but mostly like from the algorithm formulation part and why it works\n",
      "[00:12:33.640 --> 00:12:36.440]   will not implement those deep power algorithms.\n",
      "[00:12:36.440 --> 00:12:42.600]   So if you really want to like learn some implementation I would say probably this is not the course\n",
      "[00:12:42.600 --> 00:12:45.520]   for it.\n",
      "[00:12:45.520 --> 00:12:49.600]   And finally I will want to talk a little bit about the prerequisite for like because we\n",
      "[00:12:49.600 --> 00:12:55.400]   were establishing this like mathematical foundations so we definitely need some mathematical backgrounds.\n",
      "[00:12:55.400 --> 00:13:00.840]   And I will say most of the course will be essentially self-contained which requires some like undergraduate\n",
      "[00:13:00.840 --> 00:13:05.360]   entry level like mathematics for this course.\n",
      "[00:13:05.360 --> 00:13:09.560]   So definitely like the basic calculus and linear algebra are very important and we will\n",
      "[00:13:09.560 --> 00:13:14.360]   use a lot of this course or this gradient like integral or something like that.\n",
      "[00:13:14.360 --> 00:13:17.960]   And I think one more thing very important is like the probability and statistics.\n",
      "[00:13:17.960 --> 00:13:19.720]   I think undergraduate level is fine.\n",
      "[00:13:19.720 --> 00:13:24.360]   We don't really need the graduate level probability or statistics but I think you at least need\n",
      "[00:13:24.360 --> 00:13:29.720]   to know like all the concepts of like what is probability, conditional probability, marginal\n",
      "[00:13:29.720 --> 00:13:35.480]   probability and like expectation that kind of stuff, okay.\n",
      "[00:13:35.480 --> 00:13:42.360]   And I think some exposure or some knowledge of the stochastic process like Markov chain\n",
      "[00:13:42.360 --> 00:13:48.280]   would be helpful but it's not necessary like we essentially will cover that kind of probability\n",
      "[00:13:48.280 --> 00:13:49.600]   here.\n",
      "[00:13:49.600 --> 00:13:55.160]   And finally I would say some intro level previous if you have take some intro level course to\n",
      "[00:13:55.160 --> 00:13:59.160]   machine learning I think those will be recommended it's not like hard to require.\n",
      "[00:13:59.160 --> 00:14:03.320]   But I think usually I would personally say like because we're doing the foundations usually\n",
      "[00:14:03.320 --> 00:14:07.280]   it's very good to have a target application in mind.\n",
      "[00:14:07.280 --> 00:14:14.380]   So have some sense what we're actually doing like besides like those mathematical derivations\n",
      "[00:14:14.380 --> 00:14:18.760]   we have like some intuition of what kind of application it is and what we are talking\n",
      "[00:14:18.760 --> 00:14:20.300]   about using this math.\n",
      "[00:14:20.300 --> 00:14:24.120]   So I would say the intro course is really recommended if you have not taken this course\n",
      "[00:14:24.120 --> 00:14:28.360]   and if you're taking this course as like the first machine learning course that will\n",
      "[00:14:28.360 --> 00:14:32.160]   be a little bit challenging, okay.\n",
      "[00:14:32.160 --> 00:14:38.240]   I think this is essentially the everything I want to say about some brief overview of\n",
      "[00:14:38.240 --> 00:14:39.480]   the course.\n",
      "[00:14:39.480 --> 00:14:42.760]   Any questions so far?\n",
      "[00:14:42.760 --> 00:14:51.520]   Okay, if no questions we will talk about some logistics of this course, okay.\n",
      "[00:14:51.520 --> 00:14:59.640]   And you have already know the time and sorry maybe that's a little bit bigger, how about\n",
      "[00:14:59.640 --> 00:15:00.640]   this?\n",
      "[00:15:00.640 --> 00:15:09.960]   Yeah, you already know the, we already know the time and the location of this course and\n",
      "[00:15:09.960 --> 00:15:17.080]   I'm the instructor and Weng Ha is the TA for this course.\n",
      "[00:15:17.080 --> 00:15:21.280]   And our office hour will be announced this week.\n",
      "[00:15:21.280 --> 00:15:25.720]   And as we already said the content of this course is really just a mathematical foundation\n",
      "[00:15:25.720 --> 00:15:30.360]   of reinforcement learning and we're mostly doing theorems and proofs.\n",
      "[00:15:30.360 --> 00:15:34.560]   And about the grade of this course and this course essentially we'll just have five problem\n",
      "[00:15:34.560 --> 00:15:40.320]   sets and one final exams that will consist of all the grades and the five problem sets\n",
      "[00:15:40.320 --> 00:15:46.080]   will consist of 60 percentage of the grades so like 12 percentage each and a final exam\n",
      "[00:15:46.080 --> 00:15:49.680]   will take up the 40 percentage of the entire grade.\n",
      "[00:15:49.680 --> 00:15:54.360]   And our policy is like we hope there's no late homeworks and if there's really something\n",
      "[00:15:54.360 --> 00:15:59.480]   emergent and please like send me email at least several days earlier and like several\n",
      "[00:15:59.480 --> 00:16:04.120]   days prior to the deadline like this don't send me like the last minute that like email\n",
      "[00:16:04.120 --> 00:16:07.960]   saying like I want some extension and maybe I didn't see the email and I didn't reply\n",
      "[00:16:07.960 --> 00:16:11.140]   and say something about that.\n",
      "[00:16:11.140 --> 00:16:18.200]   And as you already saw like we will videotape the lecture and we will also update, there\n",
      "[00:16:18.200 --> 00:16:25.320]   are some preliminary version of the lecture notes which is scribe, scribe notes into 20\n",
      "[00:16:25.320 --> 00:16:30.200]   and you can look at the course like if you will go to, I think this course website is\n",
      "[00:16:30.200 --> 00:16:36.000]   on my homepage and if you go to teaching they are like easy 5 to 4 and they are like previous\n",
      "[00:16:36.000 --> 00:16:40.360]   version in 2020 version there are a lot of like lecture notes.\n",
      "[00:16:40.360 --> 00:16:46.120]   I think the big part of the course will based on those lecture notes and it will also post\n",
      "[00:16:46.120 --> 00:16:53.480]   updated lecture notes for this semester which will fix some typos and make things cleaner.\n",
      "[00:16:53.480 --> 00:17:05.400]   And we will also post video tapes after the class, probably several days after the class.\n",
      "[00:17:05.400 --> 00:17:09.160]   And I just want to talk a little bit about syllabus like what do we plan to do for this\n",
      "[00:17:09.160 --> 00:17:16.360]   semester and this is mostly similar to previous years but with some little changes in certain\n",
      "[00:17:16.360 --> 00:17:17.880]   topics.\n",
      "[00:17:17.880 --> 00:17:24.880]   The first thing is we will have 5 homework so starting from homework 2 will essentially\n",
      "[00:17:24.880 --> 00:17:28.760]   have each homework like a deal in every 2 weeks.\n",
      "[00:17:28.760 --> 00:17:34.880]   So that will consist of 5 homework and I think the course will divide it into 2 parts.\n",
      "[00:17:34.880 --> 00:17:41.320]   The first part is like really basics of reinforcement learning and I want to talk about all prospective\n",
      "[00:17:41.320 --> 00:17:47.320]   reinforcement learning in some like very clean setting so we can understand those each perspective\n",
      "[00:17:47.320 --> 00:17:50.000]   very cleanly and very clearly.\n",
      "[00:17:50.000 --> 00:17:54.600]   So I think the first half of the semester will be focused on the tabular MDP.\n",
      "[00:17:54.600 --> 00:17:59.240]   By tabular MDP really means the Markov decision process which has like a finite number of\n",
      "[00:17:59.240 --> 00:18:01.680]   states and a finite number of actions.\n",
      "[00:18:01.680 --> 00:18:06.080]   So the first week we will talk about some intros, we will talk about some basics and\n",
      "[00:18:06.080 --> 00:18:10.120]   planning algorithm essentially how to find optimal policy based on the transition and\n",
      "[00:18:10.120 --> 00:18:11.760]   the rewards.\n",
      "[00:18:11.760 --> 00:18:18.080]   And starting from the second week we will then talk more about learning perspective which\n",
      "[00:18:18.080 --> 00:18:24.520]   essentially we need to learn the MDP or learn the transition and rewards from the data.\n",
      "[00:18:24.520 --> 00:18:29.480]   Where whenever we talk about learning we need some statistics and the probability tools.\n",
      "[00:18:29.480 --> 00:18:33.080]   So from the second week starting from second week we will first talk about the mathematical\n",
      "[00:18:33.080 --> 00:18:40.200]   tools which require to do this analysis which is concentration inequality and some like\n",
      "[00:18:40.200 --> 00:18:42.920]   union bound something like that.\n",
      "[00:18:42.920 --> 00:18:47.120]   And then we will just jump to the direct, mostly important like different settings and how\n",
      "[00:18:47.120 --> 00:18:49.160]   we're going to establish the guarantees.\n",
      "[00:18:49.160 --> 00:18:54.080]   The first setting is called generative models or simulator which is really the easier setting\n",
      "[00:18:54.080 --> 00:18:58.160]   for reinforcement learning like in terms of learning perspective.\n",
      "[00:18:58.160 --> 00:19:02.200]   And then we will talk about two part which is online learning and offline reinforcement\n",
      "[00:19:02.200 --> 00:19:03.200]   learning.\n",
      "[00:19:03.200 --> 00:19:05.960]   Online reinforcement learning we will talk about this like tradeoff between exploration\n",
      "[00:19:05.960 --> 00:19:10.840]   versus exploitation and we will talk about the optimism principle how this is really\n",
      "[00:19:10.840 --> 00:19:13.720]   helpful for online reinforcement learning.\n",
      "[00:19:13.720 --> 00:19:18.720]   And offline reinforcement on the contrary will do the pessimism.\n",
      "[00:19:18.720 --> 00:19:23.360]   And in the middle we will also talk about the minimax lower bound essentially as we said\n",
      "[00:19:23.360 --> 00:19:28.920]   in the previous slides one very important thing is sample complexity and we will provide\n",
      "[00:19:28.920 --> 00:19:33.160]   guarantees on how many samples we need to learn for specific algorithm.\n",
      "[00:19:33.160 --> 00:19:36.760]   And the other thing very important is we also know like whether this algorithm we design\n",
      "[00:19:36.760 --> 00:19:40.480]   is already the optimal whether there is some better algorithm can achieve like better sample\n",
      "[00:19:40.480 --> 00:19:42.720]   complexity or better combination efficiency.\n",
      "[00:19:42.720 --> 00:19:46.520]   So the lower bound essentially is developed to answer those questions we will see for\n",
      "[00:19:46.520 --> 00:19:52.120]   certain class of algorithm certain number of sample complexity is like really required\n",
      "[00:19:52.120 --> 00:19:56.840]   there is really no algorithm can circumvent that kind of sample complexity.\n",
      "[00:19:56.840 --> 00:20:04.000]   So this is like to say some algorithm is tied some algorithm is optimal.\n",
      "[00:20:04.000 --> 00:20:09.320]   And the second part of the lecture really after the spring break we will talk about\n",
      "[00:20:09.320 --> 00:20:11.800]   more advanced topics okay.\n",
      "[00:20:11.800 --> 00:20:15.960]   Some advanced topics including like we want to do policy optimization which is like more\n",
      "[00:20:15.960 --> 00:20:21.040]   similar to the practice like PPO algorithm and how we accept some guarantees for those\n",
      "[00:20:21.040 --> 00:20:23.780]   policy optimization type of algorithm.\n",
      "[00:20:23.780 --> 00:20:28.060]   And then we will talk about the challenge of like how to handle in the large state space\n",
      "[00:20:28.060 --> 00:20:32.160]   which is really beyond those classical tabular setting where we assume we have finite number\n",
      "[00:20:32.160 --> 00:20:33.700]   of states.\n",
      "[00:20:33.700 --> 00:20:38.260]   And so in this large state setting we will talk about like if for example if we have infinite\n",
      "[00:20:38.260 --> 00:20:42.060]   number of states and how we going to handle those kind of scenario which is really the\n",
      "[00:20:42.060 --> 00:20:47.660]   key concept is so called a function approximation will approximate value or policy like use some\n",
      "[00:20:47.660 --> 00:20:51.740]   smooth functions where we will talk about linear function approximation and general function\n",
      "[00:20:51.740 --> 00:20:52.980]   approximation.\n",
      "[00:20:52.980 --> 00:20:57.460]   Those are like some abstract concepts which just give you some brief idea of what we are\n",
      "[00:20:57.460 --> 00:20:58.460]   going to talk about.\n",
      "[00:20:58.460 --> 00:21:02.040]   If you don't know this concept it doesn't matter we will talk about those concepts when\n",
      "[00:21:02.040 --> 00:21:03.540]   we go there.\n",
      "[00:21:03.540 --> 00:21:08.460]   And the final three week of the course will be devoted into some like other challenges\n",
      "[00:21:08.460 --> 00:21:12.660]   in reinforcement and like the two weeks we will talk about some game theoretical perspective\n",
      "[00:21:12.660 --> 00:21:18.220]   like multi-agent reinforcement learning what are the mathematical models we can study for\n",
      "[00:21:18.220 --> 00:21:21.900]   multi-agent reinforcement learning and what is algorithm and guarantees.\n",
      "[00:21:21.900 --> 00:21:26.220]   And the final week will be devoted to partially observable scenario where we will talk about\n",
      "[00:21:26.220 --> 00:21:32.780]   partially observable MDP and what is algorithm and guarantees, okay.\n",
      "[00:21:32.780 --> 00:21:36.800]   Any questions about this like brief syllabus and the schedule?\n",
      "[00:21:36.800 --> 00:21:37.800]   Yes?\n",
      "[00:21:37.800 --> 00:21:45.980]   Final exam is take home, sorry, the question is whether final exam is take home or in-person\n",
      "[00:21:45.980 --> 00:21:51.740]   it's just take home and usually we will spend like several days and you can spend as much\n",
      "[00:21:51.740 --> 00:21:58.140]   as time you want for taking home exam, okay.\n",
      "[00:21:58.140 --> 00:22:04.700]   And finally I just want to briefly talk about some reference readings like besides the lectures\n",
      "[00:22:04.700 --> 00:22:05.700]   we gave.\n",
      "[00:22:05.700 --> 00:22:09.540]   I think the first draft is really what I would recommend it.\n",
      "[00:22:09.540 --> 00:22:18.900]   I think it's probably the most relevant readings like complementary to this lecture and we'll\n",
      "[00:22:18.900 --> 00:22:24.300]   talk about I think probably not much about multi-agent and partially observable but all\n",
      "[00:22:24.300 --> 00:22:31.660]   the previous parts I think have some similar type like chapters talking about some similar\n",
      "[00:22:31.660 --> 00:22:36.060]   contents and I think a lot of formulation will be slightly different like they will\n",
      "[00:22:36.060 --> 00:22:41.500]   use infinite horizon thing and we will usually use like finite horizon formulation but I\n",
      "[00:22:41.500 --> 00:22:47.540]   think more or less it gives you some alternative view point on like a roughly similar type\n",
      "[00:22:47.540 --> 00:22:52.780]   of contents so I think this will be the most relevant readings if you really want to like\n",
      "[00:22:52.780 --> 00:22:58.020]   read some other materials but I think most like for the most part of lecture will be\n",
      "[00:22:58.020 --> 00:23:02.940]   self-contained we really don't require a lot of like additional reading and also some\n",
      "[00:23:02.940 --> 00:23:07.820]   other very good textbook I would recommend like including the reinforcement learning\n",
      "[00:23:07.820 --> 00:23:14.180]   and introduction from Richard Sutton and Bartle I think this is really the intro to reinforcement\n",
      "[00:23:14.180 --> 00:23:19.940]   learning book I also read this book like when I first started learning reinforcement learning\n",
      "[00:23:19.940 --> 00:23:26.420]   I think the book has a lot of like application formulation and the algorithm didn't talk\n",
      "[00:23:26.420 --> 00:23:33.820]   a lot about the analysis or guarantees but I think as an intro level like a textbook\n",
      "[00:23:33.820 --> 00:23:38.780]   this is really awesome and I think also two additional book from Charba which has a lot\n",
      "[00:23:38.780 --> 00:23:45.140]   of bandits and like techniques in this book which I would also recommend and they spend\n",
      "[00:23:45.140 --> 00:23:50.740]   it out with them tour and Charba and if you really want to like do this research especially\n",
      "[00:23:50.740 --> 00:23:55.420]   like graduate level research in reinforcement learning and I think one very important part\n",
      "[00:23:55.420 --> 00:24:01.340]   is like a basic case like a bandit this bandit algorithm book really gets very comprehensive\n",
      "[00:24:01.340 --> 00:24:06.260]   on like every different settings and how you're gonna design algorithm and what kind of guarantees\n",
      "[00:24:06.260 --> 00:24:14.820]   you can have for different scenarios and the last two will be some complementary readings\n",
      "[00:24:14.820 --> 00:24:20.540]   for the second week material about concentration in quality where we'll essentially as we introduce\n",
      "[00:24:20.540 --> 00:24:27.340]   some newest probability tools and to study those MDP and learning MDP problems and if\n",
      "[00:24:27.340 --> 00:24:31.140]   you feel you want to learn a bit more about concentration in quality you can read like\n",
      "[00:24:31.140 --> 00:24:40.920]   those to like survey or textbooks. There are some really the course already I think this\n",
      "[00:24:40.920 --> 00:24:45.460]   course is like I've been giving this course this is my third time giving this course and\n",
      "[00:24:45.460 --> 00:24:49.860]   the initially we designed this course in 2020 and since then there are already a lot of\n",
      "[00:24:49.860 --> 00:24:54.580]   like reinforcement course happening at different universities including like UIUC I think this\n",
      "[00:24:54.580 --> 00:25:00.540]   is Cornell this is MIT so different course have like different flavors I believe the first\n",
      "[00:25:00.540 --> 00:25:07.020]   two courses are mostly about like the first draft and Dylan and as I were writing this\n",
      "[00:25:07.020 --> 00:25:11.740]   course will be more in terms of statistical flavor they will they will introduce like\n",
      "[00:25:11.740 --> 00:25:16.700]   more advanced there there will be a little bit more advanced tools I think our person\n",
      "[00:25:16.700 --> 00:25:22.500]   will feel a little bit too complicated and I will briefly mention those and we will not\n",
      "[00:25:22.500 --> 00:25:32.300]   focus on this in this class and finally like Alekagro and Alex Scavins and I also give this\n",
      "[00:25:32.300 --> 00:25:37.780]   like benefits and reinforcement learning course and they are also like some more empirical\n",
      "[00:25:37.780 --> 00:25:41.980]   version or practical version of the course for example like Sergey gives deep reinforcement\n",
      "[00:25:41.980 --> 00:25:46.980]   learning course at UC Berkeley and there I am sure there are like a bunch of other like\n",
      "[00:25:46.980 --> 00:25:51.420]   deep reinforcement course I think at Berkeley at Princeton this semester we will also have\n",
      "[00:25:51.420 --> 00:25:57.540]   like intro to reinforcement in course which targeted like both graduate I think junior\n",
      "[00:25:57.540 --> 00:26:03.780]   graduate level and a lot of undergraduate level which will talk a lot about like empirical\n",
      "[00:26:03.780 --> 00:26:10.340]   like some case study and some some real applications I think if those are like what interest you\n",
      "[00:26:10.340 --> 00:26:22.020]   can also like look at those materials okay and that's all about the logic six. Any questions?\n",
      "[00:26:22.020 --> 00:26:34.100]   Yes, we will actually upload all the homeworks in Canvas and also the your solution was\n",
      "[00:26:34.100 --> 00:26:43.220]   it should be also uploaded to like Canvas yeah thank you okay if you know other questions\n",
      "[00:26:43.220 --> 00:26:58.140]   we will directly start the lecture.\n",
      "[00:26:58.140 --> 00:27:05.140]   Okay.\n",
      "[00:27:05.140 --> 00:27:15.140]   Okay.\n",
      "[00:27:15.140 --> 00:27:20.140]   Okay.\n",
      "[00:27:20.140 --> 00:27:25.140]   Okay.\n",
      "[00:27:25.140 --> 00:27:31.140]   Okay.\n",
      "[00:27:31.140 --> 00:27:35.140]   Okay, so let's directly jump into, like,\n",
      "[00:27:35.140 --> 00:27:37.140]   the mathematical foundations.\n",
      "[00:27:37.140 --> 00:27:44.140]   So, first talk about the MDP formulations.\n",
      "[00:27:44.140 --> 00:27:50.140]   Okay.\n",
      "[00:27:50.140 --> 00:27:58.140]   So, the MDP really stands for the Markov decision process.\n",
      "[00:27:58.140 --> 00:28:07.140]   As we mentioned, in the intro, I think this is, like,\n",
      "[00:28:07.140 --> 00:28:10.140]   the mathematical model, which is underlined, essentially,\n",
      "[00:28:10.140 --> 00:28:12.140]   everything we're going to study for reinforcement learning.\n",
      "[00:28:12.140 --> 00:28:15.140]   And we'll spend most of the semester, like,\n",
      "[00:28:15.140 --> 00:28:18.140]   focusing on, like, learning different perspectives\n",
      "[00:28:18.140 --> 00:28:23.140]   and different settings of this MDP.\n",
      "[00:28:23.140 --> 00:28:24.140]   Okay.\n",
      "[00:28:24.140 --> 00:28:27.140]   So, we'll take some steps to eventually introduce this MDP.\n",
      "[00:28:27.140 --> 00:28:28.140]   Okay.\n",
      "[00:28:28.140 --> 00:28:30.140]   So, we'll first talk about, as we already said,\n",
      "[00:28:30.140 --> 00:28:33.140]   the reinforcement learning, in general, it says this diagram.\n",
      "[00:28:33.140 --> 00:28:34.140]   We have the learner.\n",
      "[00:28:34.140 --> 00:28:38.140]   We have the agents.\n",
      "[00:28:38.140 --> 00:28:49.140]   And we also have the environments.\n",
      "[00:28:49.140 --> 00:28:53.140]   And agents is essentially the learner or the decision maker.\n",
      "[00:28:53.140 --> 00:28:56.140]   It's going to take some action to affect the environment.\n",
      "[00:28:56.140 --> 00:29:05.140]   And the environment is going to send some feedback to the agents.\n",
      "[00:29:05.140 --> 00:29:07.140]   Okay.\n",
      "[00:29:07.140 --> 00:29:10.140]   And in the intro slides, we kind of say this feedback\n",
      "[00:29:10.140 --> 00:29:13.140]   into, including, like, state, standard reward.\n",
      "[00:29:13.140 --> 00:29:15.140]   And we'll talk about those things later on.\n",
      "[00:29:15.140 --> 00:29:17.140]   But just in general, we have the, like,\n",
      "[00:29:17.140 --> 00:29:21.140]   this interaction loop.\n",
      "[00:29:21.140 --> 00:29:24.140]   So, in order to introduce this MDP formulation,\n",
      "[00:29:24.140 --> 00:29:30.140]   I would like to first talk about some easier, like, model,\n",
      "[00:29:30.140 --> 00:29:34.140]   which basically is some special case of MDP.\n",
      "[00:29:34.140 --> 00:29:38.140]   And we were starting from there, and we adding one by,\n",
      "[00:29:38.140 --> 00:29:41.140]   a little bit, a by a little bit, and eventually we go to the MDP.\n",
      "[00:29:41.140 --> 00:29:43.140]   And we'll see what it's about.\n",
      "[00:29:43.140 --> 00:29:46.140]   So, I think the first basic model I want to talk about\n",
      "[00:29:46.140 --> 00:29:57.140]   is called a multi-arm bandit.\n",
      "[00:29:57.140 --> 00:30:01.140]   So, what is a multi-arm bandit problem?\n",
      "[00:30:01.140 --> 00:30:04.140]   Essentially, imagine you're going to a casino.\n",
      "[00:30:04.140 --> 00:30:15.140]   And your face, the way it's the so-called a K-arm bandit machine.\n",
      "[00:30:15.140 --> 00:30:17.140]   So, what is this kind of bandit machine?\n",
      "[00:30:17.140 --> 00:30:20.140]   Essentially, you have, you are represented with K-arms.\n",
      "[00:30:20.140 --> 00:30:24.140]   This is, like, arm 1, this is arm 2, and this is arm 3.\n",
      "[00:30:24.140 --> 00:30:27.140]   And in order, you have, like, arm K.\n",
      "[00:30:27.140 --> 00:30:31.140]   Let's say they're connected by a single machine.\n",
      "[00:30:31.140 --> 00:30:35.140]   And so, every time you require to pull one of the arm,\n",
      "[00:30:35.140 --> 00:30:37.140]   and you're getting some reward, essentially,\n",
      "[00:30:37.140 --> 00:30:40.140]   you're getting some coin out of this bandit machine.\n",
      "[00:30:40.140 --> 00:30:45.140]   So, let's suppose we are under this model, so that when I pull the arm 1,\n",
      "[00:30:45.140 --> 00:30:48.140]   I will get some stochastic reward.\n",
      "[00:30:48.140 --> 00:30:53.140]   Essentially, the amount of coin I'm going to get is sampled from some Gaussian distribution.\n",
      "[00:30:53.140 --> 00:30:59.140]   This is, like, Gaussian distribution.\n",
      "[00:30:59.140 --> 00:31:13.140]   With some reward 1, R1 is the meaning of the Gaussian distribution.\n",
      "[00:31:13.140 --> 00:31:19.140]   And the sigma square is the standard, like, the variance of the Gaussian distribution.\n",
      "[00:31:19.140 --> 00:31:24.140]   And this is, like, I'm going to get some stochastic reward from the first arm\n",
      "[00:31:24.140 --> 00:31:28.140]   if I pull the first arm, and the stochastic reward is going to sample from the\n",
      "[00:31:28.140 --> 00:31:32.140]   first Gaussian distribution with mean R1 and variance sigma square.\n",
      "[00:31:32.140 --> 00:31:37.140]   And if I pull the arm 2, I'm going to get a stochastic reward that is sampled from\n",
      "[00:31:37.140 --> 00:31:41.140]   Gaussian distribution with mean R2 and sigma square as a variance.\n",
      "[00:31:41.140 --> 00:31:46.140]   And similarly, if I'm going to sample from, if I'm going to pull the last arm,\n",
      "[00:31:46.140 --> 00:31:51.140]   arm K, I'm going to get a stochastic reward which is sampled from Gaussian distribution\n",
      "[00:31:51.140 --> 00:32:00.140]   with mean RK and sigma square.\n",
      "[00:32:00.140 --> 00:32:06.140]   So in this case, you can imagine, every time I pull an arm, I'm getting some stochastic reward.\n",
      "[00:32:06.140 --> 00:32:11.140]   And the final goal of this multi-arm bandit.\n",
      "[00:32:11.140 --> 00:32:16.140]   One of the goal is we want to identify the best arm.\n",
      "[00:32:16.140 --> 00:32:35.140]   Essentially, we know, because this is, like, each reward is sampled from Gaussian,\n",
      "[00:32:35.140 --> 00:32:41.140]   and which best arm can be defined as the arm with highest mean reward.\n",
      "[00:32:41.140 --> 00:32:46.140]   And so for this kind of problem, we can see this is, we can call this as, like,\n",
      "[00:32:46.140 --> 00:32:52.140]   a multi-arm bandit problem where we can specify the multi-arm bandit problem by two elements.\n",
      "[00:32:52.140 --> 00:32:58.140]   One is the action set.\n",
      "[00:32:58.140 --> 00:33:07.140]   This is an action set, which in our case is essentially the 1 to K.\n",
      "[00:33:07.140 --> 00:33:25.140]   And also, we also require to specify the reward in terms of this is, like, the reward distribution.\n",
      "[00:33:25.140 --> 00:33:34.140]   So those two elements identify what is the multi-arm bandit problem.\n",
      "[00:33:34.140 --> 00:33:41.140]   So we're starting the very basic setting and to make it slightly more complicated for the next setting.\n",
      "[00:33:41.140 --> 00:33:47.140]   So the next setting we'll talk about is so-called a contextual multi-arm bandit.\n",
      "[00:33:47.140 --> 00:34:07.140]   Or people just call it a contextual bandit.\n",
      "[00:34:07.140 --> 00:34:10.140]   So what is a contextual multi-arm bandit about?\n",
      "[00:34:10.140 --> 00:34:16.140]   So this multi-arm bandit just says I have one K-arm bandit question.\n",
      "[00:34:16.140 --> 00:34:23.140]   So suppose connection of a multi-arm bandit kind of tells you, instead of having just one machine,\n",
      "[00:34:23.140 --> 00:34:28.140]   I have like a multiple, like, this K-arm bandit machine.\n",
      "[00:34:28.140 --> 00:34:37.140]   So for example, I have a first machine that is, like, for example, this is like a blue bandit machine.\n",
      "[00:34:37.140 --> 00:34:45.140]   We also have a red bandit machine that is also like a K-arm bandit machine.\n",
      "[00:34:45.140 --> 00:34:58.140]   We also have a yellow bandit machine.\n",
      "[00:34:58.140 --> 00:35:18.140]   So the contextual multi-arm bandit has the following interaction protocols.\n",
      "[00:35:18.140 --> 00:35:32.140]   So if you interact with this like a connection multi-arm bandit, in K rounds, so for each round K from one, two to a bunch of numbers.\n",
      "[00:35:32.140 --> 00:35:59.140]   So environment will first pick a context S. And in our example, this context S is just like whether it's blue or red or yellow.\n",
      "[00:35:59.140 --> 00:36:22.140]   And then the agents will take an action.\n",
      "[00:36:22.140 --> 00:36:32.140]   And finally, the environment will reveal the stochastic reward.\n",
      "[00:36:32.140 --> 00:37:01.140]   [ Writing on Board ]\n",
      "[00:37:01.140 --> 00:37:08.140]   So essentially, I have some environment that is playing with me.\n",
      "[00:37:08.140 --> 00:37:14.140]   And he will first ask me, oh, you should first play the blue bandit machine, and then I will put one arm.\n",
      "[00:37:14.140 --> 00:37:23.140]   And then he next tells me, you can only play like the red bandit machine, and then we'll put another arm from the red bandit machine.\n",
      "[00:37:23.140 --> 00:37:52.140]   So the goal in this scenario, of course, is to identify the best arm for each content, for each context.\n",
      "[00:37:52.140 --> 00:38:04.140]   Or sometimes later, in MDP, we'll use the language card as a state.\n",
      "[00:38:04.140 --> 00:38:12.140]   Essentially, what I want to learn is, if you'll tell me to play the blue bandit machine, I want to know which arm is the best for blue bandit machine.\n",
      "[00:38:12.140 --> 00:38:18.140]   And on the other hand, I also want to know which arm is best for the red, and which arm is best for the yellow one.\n",
      "[00:38:18.140 --> 00:38:28.140]   And in this case, we can essentially mathematically describe what is the object of this, like identifying the best arm for each context.\n",
      "[00:38:28.140 --> 00:38:32.140]   We can call it optimal policy.\n",
      "[00:38:32.140 --> 00:38:45.140]   Essentially, this is equivalent to an optimal policy, where how we define the policy.\n",
      "[00:38:45.140 --> 00:39:03.140]   The policy is defined as a map, which we map each context, where the set of contexts is S to the action set A.\n",
      "[00:39:03.140 --> 00:39:16.140]   This is a deterministic policy.\n",
      "[00:39:16.140 --> 00:39:25.140]   And sometimes, only in the very last part of the lecture, like in some multi-agent scenario, it's also beneficial to consider the randomized policy,\n",
      "[00:39:25.140 --> 00:39:33.140]   where essentially it's not only just a map states to action set, but a map states to some simplex over action.\n",
      "[00:39:33.140 --> 00:39:36.140]   There's a probability of simplex, probability of space over action.\n",
      "[00:39:36.140 --> 00:39:40.140]   Essentially, you will map states to our probability here.\n",
      "[00:39:40.140 --> 00:39:43.140]   For example, I will have a 0.5 probability to play action.\n",
      "[00:39:43.140 --> 00:40:02.140]   The 0.5 probability to play action tool. This is like the probability simplex over action.\n",
      "[00:40:02.140 --> 00:40:05.140]   And this is for randomized policy.\n",
      "[00:40:05.140 --> 00:40:26.140]   So, causal policy.\n",
      "[00:40:26.140 --> 00:40:34.140]   So, we haven't said here, like a contextual bandwidth, multi-ambended, can be essentially specified by the same thing.\n",
      "[00:40:34.140 --> 00:40:52.140]   And we have a reward distribution, and the additional thing we have is the complex set.\n",
      "[00:40:52.140 --> 00:40:59.140]   So, we can think of this second model. The contextual multi-ambended essentially has a state version of the multi-ambended.\n",
      "[00:40:59.140 --> 00:41:10.140]   It's not only just a single-banded machine, but it has a different-banded machine in it, and the different-banded machine is described by this state or the complex.\n",
      "[00:41:10.140 --> 00:41:15.140]   Okay. Any questions so far about this model?\n",
      "[00:41:15.140 --> 00:41:25.140]   I want to introduce this model, because also later in the later phase of this course, like when we developed some very complex solutions for reinforcement learning,\n",
      "[00:41:25.140 --> 00:41:31.140]   or reasonable complex solution for reinforcement learning, it's always good to keep in mind, like those are the special case.\n",
      "[00:41:31.140 --> 00:41:45.140]   And if you feel very difficult to understand some concepts for MDP of reinforcement learning, it will be very beneficial to go back to a simple case to understand how it works first for like multi-ambended and connection multi-ambended.\n",
      "[00:41:45.140 --> 00:41:52.140]   And then think about how it works for MDP, and then you have a deeper understanding of everything.\n",
      "[00:41:52.140 --> 00:41:56.140]   Okay. So, yes, please.\n",
      "[00:41:56.140 --> 00:42:01.140]   Oh, so wait, in the C-mount, does R have like a larger size than in the normal?\n",
      "[00:42:01.140 --> 00:42:08.140]   Yes. The question is, in the contextual multi-ambended, is R like, is R different from the multi-ambended?\n",
      "[00:42:08.140 --> 00:42:13.140]   Yes. I think this multi-ambended is just, this R is a reward distribution of every arm.\n",
      "[00:42:13.140 --> 00:42:19.140]   And now this R needs to be the reward distribution for every arm and every different machine, a different connection.\n",
      "[00:42:19.140 --> 00:42:24.140]   So, it's larger.\n",
      "[00:42:24.140 --> 00:42:27.140]   Yes.\n",
      "[00:42:27.140 --> 00:42:33.140]   What is the state space, should it just the previous waterfall?\n",
      "[00:42:33.140 --> 00:42:38.140]   Yes, state space, in this example is just the blue, red, and yellow.\n",
      "[00:42:38.140 --> 00:42:50.140]   You can show it different random machines like the state space.\n",
      "[00:42:50.140 --> 00:42:57.140]   Okay. So, finally, we'll talk about the mark of decision process.\n",
      "[00:42:57.140 --> 00:43:17.140]   So, we will use a lot of notation for it.\n",
      "[00:43:17.140 --> 00:43:20.140]   So, it has some contact set or state set as.\n",
      "[00:43:20.140 --> 00:43:29.140]   It has actions at A and the reward. And in addition, it has some transition and H.\n",
      "[00:43:29.140 --> 00:43:34.140]   We'll talk about what are those later. Okay.\n",
      "[00:43:34.140 --> 00:43:39.140]   So, I think the most important thing this mark of decision process is trying to model.\n",
      "[00:43:39.140 --> 00:43:49.140]   In addition to this contextual multi-ambended, is some temporal correlation.\n",
      "[00:43:49.140 --> 00:44:07.140]   Essentially, we want to say the action would have some influence\n",
      "[00:44:07.140 --> 00:44:21.140]   on the future states. Essentially, we are now no longer just repeatedly play the same connection\n",
      "[00:44:21.140 --> 00:44:34.140]   but where essentially states will be like transition to some specific way which is actually depends on what kind of action you take.\n",
      "[00:44:34.140 --> 00:44:47.140]   So, let's first talk about the protocols.\n",
      "[00:44:47.140 --> 00:44:51.140]   So, it's an interaction protocol.\n",
      "[00:44:51.140 --> 00:45:04.140]   In action protocol it's kind of similar, we have different episodes for episodes.\n",
      "[00:45:04.140 --> 00:45:08.140]   I think usually it's great if we think about some type of applications.\n",
      "[00:45:08.140 --> 00:45:15.140]   I think the card target application I have sometimes in mind is like for example, we are playing super Mario or we are playing some chess.\n",
      "[00:45:15.140 --> 00:45:23.140]   Essentially, each episode is like one pass of the game. You can think of one episode is like we play from the start initial states\n",
      "[00:45:23.140 --> 00:45:33.140]   till eventually the super Mario indicates like the player died or something and we start another game that is like one episode.\n",
      "[00:45:33.140 --> 00:45:44.140]   And so think about super Mario how it goes. The first is the environment.\n",
      "[00:45:44.140 --> 00:45:58.140]   We will reveal initial states as one.\n",
      "[00:45:58.140 --> 00:46:08.140]   And starting from initial states we need to make a sequence of decisions and then the state of the environment will transition to a bunch of like states in the future.\n",
      "[00:46:08.140 --> 00:46:25.140]   So, we have another for loop inside. So, this is for step H from one dot dot two capital H.\n",
      "[00:46:25.140 --> 00:46:29.140]   So, imagine we are playing super Mario for like H steps.\n",
      "[00:46:29.140 --> 00:46:39.140]   So, at every step agents will pick some action.\n",
      "[00:46:39.140 --> 00:46:50.140]   H is the action for H step. We will use H as subscript H as denoted like the action the agent takes for H steps.\n",
      "[00:46:50.140 --> 00:47:08.140]   And the environment will transition to the next states as H plus one.\n",
      "[00:47:08.140 --> 00:47:15.140]   And the gift reward.\n",
      "[00:47:15.140 --> 00:47:20.140]   Orange.\n",
      "[00:47:20.140 --> 00:47:25.140]   So, essentially after we take some action we can immediately see the effect of the action.\n",
      "[00:47:25.140 --> 00:47:31.140]   For example, in a super Mario if we eat some mushroom and the player like become bigger.\n",
      "[00:47:31.140 --> 00:47:37.140]   And if we like jump into some like some dip and then like the player just died.\n",
      "[00:47:37.140 --> 00:47:40.140]   And so we will see how the environment will transition to the next states.\n",
      "[00:47:40.140 --> 00:47:43.140]   And the environment will also give some immediate reward.\n",
      "[00:47:43.140 --> 00:47:45.140]   This reward is like immediately given at H steps.\n",
      "[00:47:45.140 --> 00:48:03.140]   So, sometimes it's called also called immediate reward.\n",
      "[00:48:03.140 --> 00:48:09.140]   It will be much clearer if we can illustrate this in terms of some diagram and how things works.\n",
      "[00:48:09.140 --> 00:48:13.140]   So, the environment will first start with some state S1.\n",
      "[00:48:13.140 --> 00:48:19.140]   This is like the initial state. Sometimes it's fixed. Sometimes it's random sample from some distribution.\n",
      "[00:48:19.140 --> 00:48:24.140]   And the player will take A1 as the first action.\n",
      "[00:48:24.140 --> 00:48:32.140]   And the S1 and A1 will jointly determine what is the next states the environment goes.\n",
      "[00:48:32.140 --> 00:48:42.140]   And the S1 and A1 will also jointly determine what is the immediate reward I'm going to collect.\n",
      "[00:48:42.140 --> 00:48:45.140]   And then the process of the game goes down.\n",
      "[00:48:45.140 --> 00:48:54.140]   At the second step, essentially I know the game transition to the S2 and I will take another action A2.\n",
      "[00:48:54.140 --> 00:49:03.140]   And the S2 and A2 jointly determines the third states at the third step.\n",
      "[00:49:03.140 --> 00:49:14.140]   And then the S2 and A2 also jointly determine what is the intermediate reward we're going to get at the second step.\n",
      "[00:49:14.140 --> 00:49:17.140]   And then we will say this goes on and goes on and on.\n",
      "[00:49:17.140 --> 00:49:23.140]   And eventually until we reach the final states and in this case we consider the episodic setting.\n",
      "[00:49:23.140 --> 00:49:27.140]   So we just for simplicity we consider each game has only H step.\n",
      "[00:49:27.140 --> 00:49:30.140]   So H is the final horizon of the game.\n",
      "[00:49:30.140 --> 00:49:36.140]   So the game will stop after SH+1 is reached and then this is done.\n",
      "[00:49:36.140 --> 00:49:46.140]   This is like finite length MDP.\n",
      "[00:49:46.140 --> 00:49:55.140]   MDP uses stochastic.\n",
      "[00:49:55.140 --> 00:50:01.140]   The question is whether MDP is stochastic and yes we're just going to talk about it right now.\n",
      "[00:50:01.140 --> 00:50:19.140]   [ Pause ]\n",
      "[00:50:19.140 --> 00:50:25.140]   So at the intuitive level you can think this MDP is really just slightly more complicated\n",
      "[00:50:25.140 --> 00:50:32.140]   in the version of a multi-on-bennett.\n",
      "[00:50:32.140 --> 00:50:38.140]   And now the thing different in MDP is like we now have an additional transition.\n",
      "[00:50:38.140 --> 00:50:45.140]   Essentially how the states is going to change from S1 to the second step to the third step\n",
      "[00:50:45.140 --> 00:50:47.140]   and how the action is going to affect those.\n",
      "[00:50:47.140 --> 00:50:54.140]   Essentially the transition, the P thing is the new thing in this MDP and we will talk more specifically about what are those things.\n",
      "[00:50:54.140 --> 00:51:01.140]   So we will formally define the elements of MDP.\n",
      "[00:51:01.140 --> 00:51:08.140]   So the first S where we set is the set of states.\n",
      "[00:51:08.140 --> 00:51:15.140]   This set can be anything, it can be like 1, 2, 3, 4, 2 and N states or like you can say it's blue, yellow or something.\n",
      "[00:51:15.140 --> 00:51:20.140]   It doesn't really matter but it's just whatever the states you have and it's a set of states.\n",
      "[00:51:20.140 --> 00:51:26.140]   And also A is the set of actions.\n",
      "[00:51:26.140 --> 00:51:32.140]   So what type of actions you can take at each different states.\n",
      "[00:51:32.140 --> 00:51:44.140]   And for simplicity we will talk about R where we say this R is just a reward function.\n",
      "[00:51:44.140 --> 00:51:50.140]   Essentially R is defined as an individual RH.\n",
      "[00:51:50.140 --> 00:51:59.140]   And I think it's RH where RH is from 1 to H.\n",
      "[00:51:59.140 --> 00:52:10.140]   Where RH is a map from state action to something, let's say 0 to 1.\n",
      "[00:52:10.140 --> 00:52:16.140]   So essentially it's a map which take a state and action pair and a map to some real value in 0 and 1.\n",
      "[00:52:16.140 --> 00:52:24.140]   Which this is just normalization condition, we usually say.\n",
      "[00:52:24.140 --> 00:52:30.140]   You can pick some other number but just for normalization we say reward is always restricted to like 0 and 1 range.\n",
      "[00:52:30.140 --> 00:52:45.140]   And essentially the number RH is the reward we're going to receive if we're currently at S, state S and we take action A.\n",
      "[00:52:45.140 --> 00:52:51.140]   And I want to also say for simplicity we consider deterministic reward here.\n",
      "[00:52:51.140 --> 00:53:13.140]   This is like.\n",
      "[00:53:13.140 --> 00:53:18.140]   So we no longer have like reward distribution, we really just reward is just a single function.\n",
      "[00:53:18.140 --> 00:53:23.140]   And so this is like make it slightly different from the multi-arm bandit and connection multi-arm bandit.\n",
      "[00:53:23.140 --> 00:53:34.140]   I think the reason we can usually do this kind of thing here is because we will actually see the transition has some stochastic in it.\n",
      "[00:53:34.140 --> 00:53:42.140]   And usually the transition stochastic in transition is much more difficult to handle than stochastic in the reward.\n",
      "[00:53:42.140 --> 00:53:50.140]   So usually like whatever the results or theoretical analysis we develop later, which works for stochastic transition,\n",
      "[00:53:50.140 --> 00:53:58.140]   will also and stochastic transition and deterministic reward will be easily extended to the case of stochastic transition and stochastic reward.\n",
      "[00:53:58.140 --> 00:54:04.140]   So this is like just for simplicity like we write this, we can definitely handle the stochastic reward.\n",
      "[00:54:04.140 --> 00:54:11.140]   It's just a little bit more complicated notation and we don't want to make everything more complicated.\n",
      "[00:54:11.140 --> 00:54:29.140]   So I think the key important part is this P, which is the transition matrix.\n",
      "[00:54:29.140 --> 00:54:37.140]   So again P is because this is like episodic setting, so we can possibly have H different P's.\n",
      "[00:54:37.140 --> 00:54:48.140]   So each step will have a different transition matrix where the P is really the probability of S' given SA.\n",
      "[00:54:48.140 --> 00:55:03.140]   This is really the probability of I'm going to transition to the next state S' given currently I'm at a state S and taking action A.\n",
      "[00:55:03.140 --> 00:55:07.140]   Okay?\n",
      "[00:55:07.140 --> 00:55:16.140]   I just want to say it again, this is like the probability, maybe I should write it down.\n",
      "[00:55:16.140 --> 00:55:30.140]   To transition to a state S' given currently I'm at a state S.\n",
      "[00:55:30.140 --> 00:55:55.140]   [ Writing on Board ]\n",
      "[00:55:55.140 --> 00:56:24.140]   And finally we have the last element that is H, which is just the lens of the episode.\n",
      "[00:56:24.140 --> 00:56:32.140]   Any questions?\n",
      "[00:56:32.140 --> 00:56:35.140]   It can be different, yes.\n",
      "[00:56:35.140 --> 00:56:40.140]   I think you can also make it the same, but that is basically a special case.\n",
      "[00:56:40.140 --> 00:56:51.140]   Let's just consider it a more general case.\n",
      "[00:56:51.140 --> 00:56:59.140]   So finally I also want to comment a little bit about, this seems like some generic transition, but why it's called a Markov decision process?\n",
      "[00:56:59.140 --> 00:57:03.140]   Like what is the Markov part for it?\n",
      "[00:57:03.140 --> 00:57:08.140]   Like why it's called a Markov decision process?\n",
      "[00:57:08.140 --> 00:57:26.140]   I think a very simple reason is let's just consider this situation for fixed action sequence.\n",
      "[00:57:26.140 --> 00:57:33.140]   Let's say this A1A2 at your age is like fixed.\n",
      "[00:57:33.140 --> 00:57:39.140]   Then if you look at this diagram, if A1A2 are fixed then this basically we just get rid of this.\n",
      "[00:57:39.140 --> 00:57:45.140]   We no longer, this will no longer play a role in this here because we already prefixed this action.\n",
      "[00:57:45.140 --> 00:57:47.140]   And we look at the remaining chain.\n",
      "[00:57:47.140 --> 00:57:59.140]   This is like just S1 goes to S2, S2 goes to S3, and then essentially this remaining state forms a Markov chain.\n",
      "[00:57:59.140 --> 00:58:25.140]   So then the state sequence, like S1 to SH+1 is a Markov chain.\n",
      "[00:58:25.140 --> 00:58:29.140]   For those of you who are not very familiar with stochastic process, I think one very important.\n",
      "[00:58:29.140 --> 00:58:35.140]   Markov chain is like one of the very basic stochastic process you would learn in that class.\n",
      "[00:58:35.140 --> 00:58:42.140]   And I think the most important property about the Markov chain is the independency.\n",
      "[00:58:42.140 --> 00:58:50.140]   So the independency essentially says, for example, if this is just one chain and then conditional, what are happening in the current step?\n",
      "[00:58:50.140 --> 00:58:57.140]   For example, if I conditional S2, whatever happened before and whatever happened after is like independent.\n",
      "[00:58:57.140 --> 00:59:07.140]   So this essentially says conditional, for example SH, whatever happened before S1 to SH-1\n",
      "[00:59:07.140 --> 00:59:15.140]   and whatever happened after SH+1 to the last state SH+1.\n",
      "[00:59:15.140 --> 00:59:23.140]   Those two things, the probability distribution of those two states set are like independent.\n",
      "[00:59:23.140 --> 00:59:34.140]   So which is like a very important property, essentially the current states is already the sufficient statistics of the past if you want to make some prediction for the future.\n",
      "[00:59:34.140 --> 00:59:41.140]   And we will sometimes leverage this property, this Markov property in this MDP.\n",
      "[00:59:41.140 --> 00:59:49.140]   And Markov property is basically baked already in this assumption, like the transition is assumed in this way.\n",
      "[00:59:49.140 --> 00:59:57.140]   Essentially the next state is some distribution will only condition on the current state and action, which is irrelevant for the previous states.\n",
      "[00:59:57.140 --> 01:00:08.140]   This kind of property already assumes this is like a Markov chain, so we already have this Markov property here.\n",
      "[01:00:08.140 --> 01:00:11.140]   Any question about this basically? Yes?\n",
      "[01:00:11.140 --> 01:00:14.140]   [inaudible]\n",
      "[01:00:14.140 --> 01:00:16.140]   It's not necessarily the time.\n",
      "[01:00:16.140 --> 01:00:22.140]   Yeah, it seems transition is different at different age, so it's not necessarily time homogeneous.\n",
      "[01:00:22.140 --> 01:00:50.140]   [inaudible]\n",
      "[01:00:50.140 --> 01:01:00.140]   The question is, does the number of states has to be finite?\n",
      "[01:01:00.140 --> 01:01:09.140]   So I think in this basic formulation, I think for easy understanding you can think it's like a finite set, but in this formulation you can also make the finite set\n",
      "[01:01:09.140 --> 01:01:13.140]   to be like countably infinite or in countably infinite or even like infinite large.\n",
      "[01:01:13.140 --> 01:01:21.140]   And later we will actually dealing with the case, which is infinite large, like a function approximation essentially the topic we were dealing with MDP with infinite large number of states.\n",
      "[01:01:21.140 --> 01:01:34.140]   So the answer is in this formulation, you don't necessarily require the cardinality of the state is like finite or something, it can be infinite large.\n",
      "[01:01:34.140 --> 01:01:52.140]   Okay, so after we introduce the basic formulation, we will talk about a few very important concepts, which we will essentially talk about those concepts throughout the course.\n",
      "[01:01:52.140 --> 01:02:16.140]   Okay, we'll talk about three most important concepts called environment, value and policy.\n",
      "[01:02:16.140 --> 01:02:30.140]   Okay, so first what are the, essentially when we talk about we want to learn the environment or we want to like understand, like explore some unknown environment.\n",
      "[01:02:30.140 --> 01:02:33.140]   So what do we mean by like no environment and unknown environment?\n",
      "[01:02:33.140 --> 01:02:44.140]   In this MDP scenario, usually we mean environment, it just means the two properties, like two very important quantities in this MDP, that is this transition.\n",
      "[01:02:44.140 --> 01:02:54.140]   She and the reward are.\n",
      "[01:02:54.140 --> 01:03:06.140]   So a lot of later, like different scenario will be like divided by like, how are we going to, whether we know the environment and how we're going to interact with the environment.\n",
      "[01:03:06.140 --> 01:03:13.140]   So there are a lot of different settings, one is like no environment, where essentially we will just do planning, which is an easier setting.\n",
      "[01:03:13.140 --> 01:03:21.140]   And then the unknown environment can also be further divided by some like a simulator scenario, we have like some very strong thing.\n",
      "[01:03:21.140 --> 01:03:27.140]   We don't, although we don't know the transition and reward, but we have something very strong, we can easily learn those transition and reward from.\n",
      "[01:03:27.140 --> 01:03:43.140]   Or it's like more challenging setting, we can handle like this online scenario, where we have to do this online interaction, or we have offline scenario, where we can no longer interact with MDP, but we, but we have access to some pre-clacted data.\n",
      "[01:03:43.140 --> 01:03:54.140]   So basically a lot of different setting, like a revolve around the, how are we going to assume about how to access to this environment.\n",
      "[01:03:54.140 --> 01:04:00.140]   Okay, we'll talk about, most specifically about what kind of those setting later.\n",
      "[01:04:00.140 --> 01:04:08.140]   This is like learning from interaction.\n",
      "[01:04:08.140 --> 01:04:23.140]   This is learning from pre-clacted data.\n",
      "[01:04:23.140 --> 01:04:39.140]   Okay, so next we'll talk about the policy.\n",
      "[01:04:52.140 --> 01:05:04.140]   So if you still recall, like we already like kind of encountered a concept of policy in the contextual bandit, where the policy is just a map mapping from states to action or distribution of action.\n",
      "[01:05:04.140 --> 01:05:09.140]   Essentially policy tells you at each state what kind of action you're going to take.\n",
      "[01:05:09.140 --> 01:05:15.140]   So here the MDP is really just a sequential version of this contextual, a contextual multi-ambended.\n",
      "[01:05:15.140 --> 01:05:18.140]   So the policy roughly have like similar structure.\n",
      "[01:05:18.140 --> 01:05:20.140]   So the policy is also a map.\n",
      "[01:05:20.140 --> 01:05:31.140]   And it's mapped not only like taking the states as an input, but also taking which step you are making this action as an input.\n",
      "[01:05:31.140 --> 01:05:44.140]   And it maps from a state space and this H, H is essentially defined as a set of one, two, about QH, a set of integers, which is from one to H.\n",
      "[01:05:44.140 --> 01:05:50.140]   And so this pi is a map from S and H set to the action set.\n",
      "[01:05:50.140 --> 01:05:52.140]   This is for deterministic policy.\n",
      "[01:05:52.140 --> 01:06:02.140]   And you can also say for randomize or for stochastic policy you map this to the probability simplex of A.\n",
      "[01:06:02.140 --> 01:06:12.140]   This is stochastic.\n",
      "[01:06:12.140 --> 01:06:41.140]   So just also want to say equivalently, if you're not comfortable with this notation, we can also use some other notation.\n",
      "[01:06:41.140 --> 01:06:45.140]   We will also use this interchangeably in the class.\n",
      "[01:06:45.140 --> 01:06:52.140]   Equally we can also say this pi is a collection of H different policy.\n",
      "[01:06:52.140 --> 01:06:58.140]   Essentially we have a different policy for different steps.\n",
      "[01:06:58.140 --> 01:07:10.140]   And each pi H is a map from states to either action or the probability simplex of action,\n",
      "[01:07:10.140 --> 01:07:13.140]   depending on deterministic policy or stochastic policy.\n",
      "[01:07:13.140 --> 01:07:15.140]   This is essentially the equivalent formulation.\n",
      "[01:07:15.140 --> 01:07:21.140]   You can either do this as a map from SH or you can say have like H different functions.\n",
      "[01:07:21.140 --> 01:07:31.140]   And H function is just a map from state to action.\n",
      "[01:07:31.140 --> 01:07:43.140]   So I just want to emphasize a little bit this way of talking about policy is actually so-called mark up policy.\n",
      "[01:07:43.140 --> 01:07:53.140]   Or like basically I call the mark up policy because the policy at every step H will only depends on the current states.\n",
      "[01:07:53.140 --> 01:08:06.140]   Essentially I kind of use some mark up property here, but we can also have some general policy.\n",
      "[01:08:06.140 --> 01:08:08.140]   So general policy can be more complicated.\n",
      "[01:08:08.140 --> 01:08:14.140]   Essentially at H steps when I want to make some decision to take some action A, I have some more information.\n",
      "[01:08:14.140 --> 01:08:23.140]   And not only have the current states, but I also have access to, I can also see all the previous states and all the previous action.\n",
      "[01:08:23.140 --> 01:08:26.140]   So general policy is essentially I want to think out of box.\n",
      "[01:08:26.140 --> 01:08:31.140]   Why not I just leverage all the previous information about history to make my decision.\n",
      "[01:08:31.140 --> 01:08:40.140]   So the general policy can be also defined as again like this pi equal to pi H from H1 to H.\n",
      "[01:08:40.140 --> 01:08:49.140]   And the general policy, the only difference is like each policy will no longer be defined a map from state to action,\n",
      "[01:08:49.140 --> 01:08:53.140]   but actually map from entire history to the action.\n",
      "[01:08:53.140 --> 01:09:06.140]   So the history in this case is something S, the set of S times A to the power of H minus Y times S to action and/or the distribution over action.\n",
      "[01:09:06.140 --> 01:09:17.140]   So what is the entire history? The entire history essentially is the S1, A1, S2, A2 and the Laudato finally S-H.\n",
      "[01:09:17.140 --> 01:09:34.140]   So which essentially is in this set.\n",
      "[01:09:34.140 --> 01:09:48.140]   So I want to talk about the general policy here because I just don't want to have some like some initial impression that the policy has to be some form like this.\n",
      "[01:09:48.140 --> 01:09:54.140]   And especially in the later stage, like when we talk about the multi-Asian reinforcement learning or partially observable reinforcement learning.\n",
      "[01:09:54.140 --> 01:09:58.140]   In fact, this is not always good.\n",
      "[01:09:58.140 --> 01:10:04.140]   We need to, in a lot of scenarios, talk about this generic policy. We need to depend on the history.\n",
      "[01:10:04.140 --> 01:10:10.140]   But for MDP, there's a very good property so that we no longer need to talk about this general policy.\n",
      "[01:10:10.140 --> 01:10:15.140]   I think we will introduce in the next lecture, which essentially, if we're going to talk about some optimal policy,\n",
      "[01:10:15.140 --> 01:10:20.140]   that is the best policy, you can actually prove the optimal policy is always Markov.\n",
      "[01:10:20.140 --> 01:10:23.140]   So we will formally talk about this in the next lecture.\n",
      "[01:10:23.140 --> 01:10:36.140]   So that's why without loss of generality, we can always just talk about the Markov policy and we can ignore the general policy for a very big part of the course.\n",
      "[01:10:36.140 --> 01:10:43.140]   Any questions about this difference?\n",
      "[01:10:43.140 --> 01:10:55.140]   Finally, we can talk about the value.\n",
      "[01:10:55.140 --> 01:11:08.140]   So we'll first introduce the value of a policy pi, of some policy pi.\n",
      "[01:11:08.140 --> 01:11:14.140]   Two types of values. One is called V-value. The other one is a Q-value.\n",
      "[01:11:14.140 --> 01:11:21.140]   So we'll denote as a V-value, V-H. Value at each value will be denoted at each different steps.\n",
      "[01:11:21.140 --> 01:11:25.140]   And it will only correspond to one particular policy pi.\n",
      "[01:11:25.140 --> 01:11:35.140]   And V-value is defined on one state, and a Q-value is defined on one state and action pair.\n",
      "[01:11:35.140 --> 01:11:50.140]   So the definition of this V-value is essentially the cumulative reward.\n",
      "[01:11:50.140 --> 01:12:18.140]   So if I start starting from state S, at a step H, and a following policy pi.\n",
      "[01:12:18.140 --> 01:12:23.140]   So you can imagine this would be something similar to the concepts.\n",
      "[01:12:23.140 --> 01:12:30.140]   For example, if you are playing chess, and you really know what is the policy you are playing, so for example, you are fixing some policy.\n",
      "[01:12:30.140 --> 01:12:38.140]   And you will roughly have S made, okay, so currently I'm on this configuration of the board with this different piece on different locations.\n",
      "[01:12:38.140 --> 01:12:46.140]   I would like to know what is the probability that I'm going to win this game starting from the current configuration.\n",
      "[01:12:46.140 --> 01:12:54.140]   So this is basically like the concept of a value, essentially what is the future, what is summation of all the future reward you're going to get\n",
      "[01:12:54.140 --> 01:13:01.140]   if you start from the current state S and keep following policy pi, okay?\n",
      "[01:13:01.140 --> 01:13:06.140]   And similarly, the Q-value is basically the same idea.\n",
      "[01:13:06.140 --> 01:13:15.140]   The only thing different is instead of starting from some state S, you are starting from the state action pair, A, S, A.\n",
      "[01:13:15.140 --> 01:13:26.140]   And also at step H and following policy pi.\n",
      "[01:13:26.140 --> 01:13:32.140]   So the only difference is this is like starting from S, so in the edge step we're starting from the state S.\n",
      "[01:13:32.140 --> 01:13:38.140]   And this is in the edge step we're starting from state S, and immediate action, I'm going to take this A action.\n",
      "[01:13:38.140 --> 01:13:42.140]   Well here, the immediate action I'm going to take is to follow the policy pi.\n",
      "[01:13:42.140 --> 01:13:49.140]   This is the only difference, everything else is the same.\n",
      "[01:13:49.140 --> 01:13:52.140]   So any questions about this definition?\n",
      "[01:13:52.140 --> 01:13:58.140]   The Q-value always will be Q-value comma pi of S.\n",
      "[01:13:58.140 --> 01:14:03.140]   V-value will also be always be Q-value times pi, summation.\n",
      "[01:14:03.140 --> 01:14:05.140]   We'll talk about this relation.\n",
      "[01:14:05.140 --> 01:14:16.140]   Isn't A always equal to pi of S? This A is not necessarily, this A can be any arbitrary.\n",
      "[01:14:16.140 --> 01:14:19.140]   So that's why I'm saying like, yeah.\n",
      "[01:14:19.140 --> 01:14:25.140]   In this definition, the in edge steps the A will always sample from pi, but this A can be anything.\n",
      "[01:14:25.140 --> 01:14:27.140]   So this is the only.\n",
      "[01:14:27.140 --> 01:14:33.140]   Yeah, yeah, yeah, this is correct.\n",
      "[01:14:33.140 --> 01:14:38.140]   We'll talk about this relation in the next lecture.\n",
      "[01:14:38.140 --> 01:14:44.140]   So I think finally I just want to be very quickly talk about the mathematical definition of those two.\n",
      "[01:14:44.140 --> 01:14:49.140]   So this is like a definition by words.\n",
      "[01:14:49.140 --> 01:15:04.140]   So the mathematical definition essentially is this V pi V H pi of S is equal to the expectation of everything.\n",
      "[01:15:04.140 --> 01:15:14.140]   That is expected cumulative reward, which is all the future reward starting from H step to the final step.\n",
      "[01:15:14.140 --> 01:15:22.140]   And the reward I'm going to collect, that is S H prime A H prime at H steps.\n",
      "[01:15:22.140 --> 01:15:24.140]   Sorry, it's A prime.\n",
      "[01:15:24.140 --> 01:15:35.140]   Conditioning currently SH is equal to S. And this is the definition of the Q value, a V value.\n",
      "[01:15:35.140 --> 01:15:42.140]   And a Q value is very similar.\n",
      "[01:15:42.140 --> 01:15:50.140]   It's taking expectation of all the future reward I'm going to collect it from step H.\n",
      "[01:15:50.140 --> 01:16:05.140]   Prime, SH prime, A H prime, condition arm, SH equal to S, and A H equal to A.\n",
      "[01:16:05.140 --> 01:16:10.140]   And following the policy pi.\n",
      "[01:16:10.140 --> 01:16:13.140]   I want to explain a little bit more about this notation.\n",
      "[01:16:13.140 --> 01:16:16.140]   What do we actually take in expectation?\n",
      "[01:16:16.140 --> 01:16:22.140]   In order to understand this thing, we first need to understand from this notation what are the random thing.\n",
      "[01:16:22.140 --> 01:16:27.140]   So we know SH H is already fixed at H step.\n",
      "[01:16:27.140 --> 01:16:32.140]   So everything goes beyond this, everything in the future is actually random, possibly random.\n",
      "[01:16:32.140 --> 01:16:34.140]   And we need to take expectation over.\n",
      "[01:16:34.140 --> 01:16:43.140]   So that includes like SH plus one can be random H plus one can be random and SH plus two.\n",
      "[01:16:43.140 --> 01:16:47.140]   In order to tell SH plus one, capital H plus one.\n",
      "[01:16:47.140 --> 01:16:56.140]   So essentially, we can expand this expectation into, this expectation will be also equal to,\n",
      "[01:16:56.140 --> 01:17:02.140]   we take expectation like each random variable, like one by one.\n",
      "[01:17:02.140 --> 01:17:08.140]   So in the outside, we first take expectation over SH plus one.\n",
      "[01:17:08.140 --> 01:17:17.140]   Where this state is sampled from the transition probability, given the SH and A H, this is well defined.\n",
      "[01:17:17.140 --> 01:17:19.140]   We come first sample SH plus one.\n",
      "[01:17:19.140 --> 01:17:30.140]   And then we will sample H plus one, which is from the pi H, given SH.\n",
      "[01:17:30.140 --> 01:17:37.140]   Okay, so sorry, pi H plus one, SH plus one.\n",
      "[01:17:37.140 --> 01:17:41.140]   I written this way in a sense like, for example, if this is stochastic policy, if it determines the policy,\n",
      "[01:17:41.140 --> 01:17:47.140]   you can just write the A H plus one, which is equal to the map value of pi H plus one.\n",
      "[01:17:47.140 --> 01:17:49.140]   And taking on the SH plus one.\n",
      "[01:17:49.140 --> 01:17:53.140]   And then we will have the next state SH plus two.\n",
      "[01:17:53.140 --> 01:17:59.140]   Again, sampled from the transition, SH plus one, A H plus one.\n",
      "[01:17:59.140 --> 01:18:05.140]   And then we will have A H plus two, sampled from blah, blah, blah, blah.\n",
      "[01:18:05.140 --> 01:18:08.140]   And till the very end.\n",
      "[01:18:08.140 --> 01:18:09.140]   Okay.\n",
      "[01:18:09.140 --> 01:18:12.140]   This essentially is the expansion of this expectation.\n",
      "[01:18:12.140 --> 01:18:41.140]   So with this definition of all the MDP things, we can finally come to the objective or the goal in reinforcement learning.\n",
      "[01:18:41.140 --> 01:18:49.140]   Imagine reinforcement in MDP as a chess game.\n",
      "[01:18:49.140 --> 01:18:56.140]   What we want is we really want to have some policy, the way of playing a chess, so that we can maximize our win rate.\n",
      "[01:18:56.140 --> 01:19:09.140]   So in terms of the mathematics of this, we can essentially say, we want to find some optimal policy.\n",
      "[01:19:09.140 --> 01:19:26.140]   So we want to find the optimal pi star to maximize the value of the initial states.\n",
      "[01:19:26.140 --> 01:19:27.140]   Okay.\n",
      "[01:19:27.140 --> 01:19:30.140]   So essentially, we want to find the optimal pi.\n",
      "[01:19:30.140 --> 01:19:37.140]   So to achieve this, the maximum value for this value.\n",
      "[01:19:37.140 --> 01:19:40.140]   The value at first step and this is initial states.\n",
      "[01:19:40.140 --> 01:19:42.140]   So usually there are several scenarios.\n",
      "[01:19:42.140 --> 01:19:53.140]   Like we can also first consider like one fixed initial states.\n",
      "[01:19:53.140 --> 01:20:06.140]   Or sometimes if you are not happy with fixed initial states, you want to talk about some stochastic initial states.\n",
      "[01:20:06.140 --> 01:20:17.140]   Then you essentially want to maximize something like expectation of S1 and sample from some initial distribution.\n",
      "[01:20:17.140 --> 01:20:22.140]   Mill and V1 pi S1.\n",
      "[01:20:22.140 --> 01:20:23.140]   Okay.\n",
      "[01:20:23.140 --> 01:20:29.140]   But for simplicity for a lot of this class, we will just consider a simple setting where we have fixed initial states.\n",
      "[01:20:29.140 --> 01:20:32.140]   And this is a very objective we want to do.\n",
      "[01:20:32.140 --> 01:20:35.140]   We want to find a policy so that starting from the beginning,\n",
      "[01:20:35.140 --> 01:20:40.140]   we can achieve the maximum value.\n",
      "[01:20:40.140 --> 01:20:41.140]   Okay.\n",
      "[01:20:41.140 --> 01:20:45.140]   And we will talk about, this is essentially some very\n",
      "[01:20:45.140 --> 01:20:51.140]   intro-level explanation of MDP and we will talk about how we're going to compute this value\n",
      "[01:20:51.140 --> 01:20:55.140]   and how we're going to find optimal policy in the next lecture.\n",
      "[01:20:55.140 --> 01:20:57.720]   (upbeat music)\n",
      "[01:20:57.720 --> 01:21:07.720]   [BLANK_AUDIO]\n",
      "\n",
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/QwTBtKWmA0M.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/QwTBtKWmA0M.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/QwTBtKWmA0M.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/QwTBtKWmA0M.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =  1284.46 ms\n",
      "whisper_print_timings:     fallbacks =   4 p /   3 h\n",
      "whisper_print_timings:      mel time =  2790.22 ms\n",
      "whisper_print_timings:   sample time = 27485.85 ms / 69271 runs (    0.40 ms per run)\n",
      "whisper_print_timings:   encode time =   362.51 ms /   199 runs (    1.82 ms per run)\n",
      "whisper_print_timings:   decode time =   982.25 ms /   514 runs (    1.91 ms per run)\n",
      "whisper_print_timings:   batchd time = 35455.22 ms / 67752 runs (    0.52 ms per run)\n",
      "whisper_print_timings:   prompt time = 10019.92 ms / 44203 runs (    0.23 ms per run)\n",
      "whisper_print_timings:    total time = 78841.66 ms\n"
     ]
    }
   ],
   "source": [
    "# bulk transcription of each youtube video:\n",
    "for link in video_links:\n",
    "    VIDEO_LINK = link\n",
    "    # name after linkID\n",
    "    name = link[-11:]\n",
    "    link = widgets.Text(\n",
    "        value=VIDEO_LINK,\n",
    "        placeholder=\"Type link for video\",\n",
    "        description=\"Video:\",\n",
    "        disabled=False\n",
    "    )\n",
    "    link\n",
    "    print(f\"Downloading video {link.value} started\")\n",
    "    print(name)\n",
    "    output_file = Path(directory + name + \".mp4\")\n",
    "    yt = YouTube(link.value)\n",
    "    # set to .get_highest_resolution if you want to download highest resolution video\n",
    "    yt.streams.get_lowest_resolution().download(filename=output_file)\n",
    "    print(f\"Video saved to {output_file}\")\n",
    "\n",
    "    # audio must always be 16hz 16 format .wav:\n",
    "    extracted_audio_file = name + '.wav'\n",
    "    def extract_audio(video_path, audio_path):\n",
    "        yes_command = f'echo \"y\" | '\n",
    "        command = yes_command + \"ffmpeg -i {} -vn -acodec pcm_s16le -ar 16000 -ac 1 {}\".format(video_path, audio_path)\n",
    "        subprocess.call(command, shell=True)\n",
    "    try:\n",
    "        extract_audio(output_file, directory + extracted_audio_file)\n",
    "        print(\"Audio coverted successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Audio convertion failed with error {e.returncode}.\")\n",
    "\n",
    "    # transcribe using the base model (great with CUDA enabled whisper.cpp)\n",
    "    try:\n",
    "        subprocess.run(['transcribe -t 12 -m ' + home_directory + '/machine_learning/whisper.cpp/models/ggml-base.en.bin -f ' \n",
    "                    + directory + extracted_audio_file + ' -otxt -ovtt -osrt -olrc'], shell=True, check=True)\n",
    "        print(\"Transcription executed successfully and saved in \" + directory)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Transcription failed with error {e.returncode}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 minutes to download and transcribe nearly 14 hours of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 11 text files into youtube-transcription_20240320.txt\n"
     ]
    }
   ],
   "source": [
    "# helper utility to combine all transcripts into one file \n",
    "# code assumes you want youtube list sorted reverse chronological order so the oldest video is first\n",
    "# comment out markdown_files.sort() if you want it the other way (newest video first)\n",
    "# most courses will have lesson 1 as the oldest so the default should be fine\n",
    "# ai assisted code modified\n",
    "\n",
    "def combine_markdown_files(output_file_path, input_directory):\n",
    "    # .lrc files have time stamps plus text in easy to read format\n",
    "    markdown_files = glob.glob(input_directory + '*.lrc')\n",
    "\n",
    "    # sort files in reverse timestamp order for youtube lists\n",
    "    # that have latest video first\n",
    "    markdown_files.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        for file_path in markdown_files:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            with open(file_path, 'r') as input_file:\n",
    "                output_file.write(f\"## {file_name}\\n\\n\")\n",
    "                output_file.write(input_file.read())\n",
    "                output_file.write('\\n\\n')  # Add newline between files\n",
    "\n",
    "    print(f\"Combined {len(markdown_files)} text files into {output_file_path}\")\n",
    "\n",
    "# saves combined text files into directory of Jupyter notebook (not the input directory)\n",
    "# Get today's date in the format YYYYMMDD\n",
    "date_string = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "filename = \"youtube-transcription\"\n",
    "# Append date to filename\n",
    "filename_with_date = f\"{filename}_{date_string}.txt\"\n",
    "combine_markdown_files(filename_with_date, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert text file to html because send to kindle joins lines in txt file\n",
    "def convert_txt_to_html(txt_file_path, html_file_path):\n",
    "    with open(txt_file_path, 'r') as txt_file, open(html_file_path, 'w') as html_file:\n",
    "        html_file.write('<!DOCTYPE html>\\n<html>\\n<body>\\n')\n",
    "        for line in txt_file:\n",
    "            html_file.write('<p>{}</p>\\n'.format(line.strip()))\n",
    "        html_file.write('</body>\\n</html>\\n')\n",
    "\n",
    "# convert to html; note file extensions\n",
    "txt_file_path = filename_with_date                      # replace with your text file path\n",
    "html_file_path = filename_with_date + '.html'           # replace with your HTML file path\n",
    "convert_txt_to_html(txt_file_path, html_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
