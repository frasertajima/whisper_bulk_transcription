{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified to handle youtube videos. \n",
    "\n",
    "Prerequisites:\n",
    "torch,\n",
    "cuda,\n",
    "ffmpeg, and \n",
    "distil-whisper at https://github.com/huggingface/distil-whisper.\n",
    "\n",
    "CAUTION: MAKE SURE TO BACKUP AUDIO FILES IF THEY HAVE SPACES IN THEIR NAMES AS THEY WILL BE RENAMED (AND THE METADATA ALTERED). For file names without spaces, the original file is not renamed and a copy is made in a compatible audio format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need cuda for vastly faster transcription\n",
    "# 47 seconds for a full 30 min podcast! 1.9s for the same file that took whisper 4m 44 seconds!\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 06:14:30.433537: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-22 06:14:30.471227: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-22 06:14:31.179551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-22 06:14:31.602838: I itex/core/wrapper/itex_cpu_wrapper.cc:52] Intel Extension for Tensorflow* AVX2 CPU backend is loaded.\n",
      "2024-03-22 06:14:31.605178: W itex/core/wrapper/itex_gpu_wrapper.cc:32] Could not load dynamic library: libimf.so: cannot open shared object file: No such file or directory\n",
      "2024-03-22 06:14:31.636722: W itex/core/ops/op_init.cc:58] Op: _QuantizedMaxPool3D is already registered in Tensorflow\n",
      "2024-03-22 06:14:31.647654: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "2024-03-22 06:14:31.647855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-22 06:14:31.648240: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-03-22 06:14:31.648271: E itex/core/wrapper/itex_gpu_wrapper.cc:49] Could not load Intel Extension for Tensorflow* GPU backend, GPU will not be used.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# conda activate py310\n",
    "import torch\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "import textwrap\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import ipywidgets as widgets\n",
    "from pytube import YouTube\n",
    "from utils import get_audio\n",
    "from utils import prepare_srt\n",
    "from pathlib import Path\n",
    "from pytube import Playlist\n",
    "import datetime\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(device)\n",
    "\n",
    "model_id = \"distil-whisper/distil-large-v2\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True, \n",
    "    use_safetensors=True,\n",
    "    attn_implementation=\"flash_attention_2\")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TcoolNrruwE\n",
      "Downloading video https://youtu.be/TcoolNrruwE?feature=shared started\n",
      "Video saved to /var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_place --cc=/root/miniconda3/envs/conda_bld/conda-bld/ffmpeg_1635335682798/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2024-03-22T02:47:31.000000Z\n",
      "  Duration: 00:08:40.10, start: 0.000000, bitrate: 1103 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 1280x720 [SAR 1:1 DAR 16:9], 972 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-03-22T02:47:31.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 03/21/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-03-22T02:47:31.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 03/21/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "File '/var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.wav' already exists. Overwrite? [y/N] Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2024-03-22T02:47:31.000000Z\n",
      "      handler_name    : ISO Media file produced by Google Inc. Created on: 03/21/2024.\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=       1kB time=00:00:00.00 bitrate=N/A speed=   0x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio coverted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size=   16253kB time=00:08:40.10 bitrate= 256.0kbits/s speed=1.28e+03x    \n",
      "video:0kB audio:16253kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000469%\n"
     ]
    }
   ],
   "source": [
    "# Specify the audio file directory\n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "directory = home_directory + '/machine_learning/whisper.cpp/samples/'\n",
    "\n",
    "# copy and youtube share link below:\n",
    "VIDEO_LINK = \"https://youtu.be/TcoolNrruwE?feature=shared\"\n",
    "name = VIDEO_LINK[17:-15]\n",
    "print(name)\n",
    "\n",
    "link = widgets.Text(\n",
    "    value=VIDEO_LINK,\n",
    "    placeholder=\"Type link for video\",\n",
    "    description=\"Video:\",\n",
    "    disabled=False\n",
    ")\n",
    "link\n",
    "\n",
    "print(f\"Downloading video {link.value} started\")\n",
    "output_file = Path(directory + name + \".mp4\")\n",
    "yt = YouTube(link.value)\n",
    "yt.streams.get_highest_resolution().download(filename=output_file)\n",
    "print(f\"Video saved to {output_file}\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "extracted_audio_file = name + '.wav'\n",
    "\n",
    "def extract_audio(video_path, audio_path):\n",
    "    yes_command = f'echo \"y\" | '\n",
    "    command = yes_command + \"ffmpeg -i {} -vn -acodec pcm_s16le -ar 16000 -ac 1 {}\".format(video_path, audio_path)\n",
    "    subprocess.call(command, shell=True)\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    extract_audio(output_file, directory + extracted_audio_file)\n",
    "    print(\"Audio coverted successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Audio convertion failed with error {e.returncode}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# chunk_lengthS=15 and batch_size=16 is ideal\\npipe = pipeline(\\n    \"automatic-speech-recognition\",\\n    model=model,\\n    tokenizer=processor.tokenizer,\\n    feature_extractor=processor.feature_extractor,\\n    max_new_tokens=128,\\n    chunk_length_s=15,\\n    batch_size=16,\\n    torch_dtype=torch_dtype,\\n    device=device\\n)\\nprint(directory + extracted_audio_file)\\n\\nresult_local = pipe(directory + extracted_audio_file)\\n\\n# output transcription\\nwrapper = textwrap.TextWrapper(width=80,\\n    initial_indent=\" \",\\n    subsequent_indent=\"\",\\n    break_long_words=False,\\n    break_on_hyphens=False)\\nprint(wrapper.fill(result_local[\"text\"]))\\n\\n# save transcript to the samples folder as a .md file\\nsaved_txt=result_local[\"text\"]\\nf = open(directory + extracted_audio_file + \".md\", \"a\")\\nf.write(saved_txt)\\nf.close()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# chunk_lengthS=15 and batch_size=16 is ideal\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=15,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device\n",
    ")\n",
    "print(directory + extracted_audio_file)\n",
    "\n",
    "result_local = pipe(directory + extracted_audio_file)\n",
    "\n",
    "# output transcription\n",
    "wrapper = textwrap.TextWrapper(width=80,\n",
    "    initial_indent=\" \",\n",
    "    subsequent_indent=\"\",\n",
    "    break_long_words=False,\n",
    "    break_on_hyphens=False)\n",
    "print(wrapper.fill(result_local[\"text\"]))\n",
    "\n",
    "# save transcript to the samples folder as a .md file\n",
    "saved_txt=result_local[\"text\"]\n",
    "f = open(directory + extracted_audio_file + \".md\", \"a\")\n",
    "f.write(saved_txt)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting container...                   \t\u001b[32m [ OK ]\n",
      "\u001b[0mInstalling basic packages...            \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up devpts mounts...             \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up read-only mounts...          \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up read-write mounts...         \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up host's sockets integration...\t\u001b[32m [ OK ]\n",
      "\u001b[0mIntegrating host's themes, icons, fonts...\t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up package manager exceptions...\t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up rpm exceptions...            \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up distrobox profile...         \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up sudo...                      \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up user groups...               \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up kerberos integration...      \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up user's group list...         \t\u001b[32m [ OK ]\n",
      "\u001b[0mSetting up user home...                 \t\u001b[32m [ OK ]\n",
      "\u001b[0mEnsuring user's access...               \t\u001b[32m [ OK ]\n",
      "\u001b[0m\n",
      "Container Setup Complete!\n",
      "whisper_init_from_file_with_params_no_state: loading model from '/var/home/fraser/machine_learning/whisper.cpp/models/ggml-base.en.bin'\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A1000 Laptop GPU, compute capability 8.6, VMM: yes\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_model_load:    CUDA0 total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init: using CUDA backend\n",
      "whisper_init_state: kv self size  =   16.52 MB\n",
      "whisper_init_state: kv cross size =   18.43 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.39 MB\n",
      "whisper_init_state: compute buffer (encode) =  132.07 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.78 MB\n",
      "whisper_init_state: compute buffer (decode) =   96.48 MB\n",
      "\n",
      "system_info: n_threads = 12 / 24 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | METAL = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | CUDA = 1 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing '/var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.wav' (8321660 samples, 520.1 sec), 12 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[00:00:00.000 --> 00:00:02.520]   \"It wasn't a roast in any traditional sense,\n",
      "[00:00:02.520 --> 00:00:04.400]   but that did not stop President Joe Biden\n",
      "[00:00:04.400 --> 00:00:06.080]   from getting some laughs at a fundraiser\n",
      "[00:00:06.080 --> 00:00:07.200]   in Dallas last night.\n",
      "[00:00:07.200 --> 00:00:09.880]   And this was the joke that killed.\n",
      "[00:00:09.880 --> 00:00:12.760]   Quote, \"Just the other day a guy came to me and said,\n",
      "[00:00:12.760 --> 00:00:14.440]   \"Mr. President, I need your help.\n",
      "[00:00:14.440 --> 00:00:15.600]   \"I'm being crushed with debt.\n",
      "[00:00:15.600 --> 00:00:17.280]   \"I'm completely wiped out.\n",
      "[00:00:17.280 --> 00:00:20.680]   \"I had to say, Donald, I can't help you.\"\n",
      "[00:00:20.680 --> 00:00:23.720]   As the saying goes, it's funny 'cause it's true.\n",
      "[00:00:23.720 --> 00:00:26.200]   Donald Trump's financial situation is not pretty.\n",
      "[00:00:26.200 --> 00:00:28.400]   There's that looming half a billion dollar bond\n",
      "[00:00:28.400 --> 00:00:31.400]   due on Monday, the $91 million bond he posted\n",
      "[00:00:31.400 --> 00:00:33.760]   for the Eugene Carroll defamation verdict,\n",
      "[00:00:33.760 --> 00:00:36.280]   not to mention the staggering penalties themselves\n",
      "[00:00:36.280 --> 00:00:39.040]   in both cases, the bills, they are slowly\n",
      "[00:00:39.040 --> 00:00:41.200]   and in some cases quickly coming due\n",
      "[00:00:41.200 --> 00:00:44.080]   and they are big, huge amounts of money.\n",
      "[00:00:44.080 --> 00:00:46.760]   It's not just a problem in terms of money going out.\n",
      "[00:00:46.760 --> 00:00:49.040]   It's also a problem in terms of money coming in\n",
      "[00:00:49.040 --> 00:00:50.680]   for the disgraced ex-president.\n",
      "[00:00:50.680 --> 00:00:53.280]   And notably, while Donald Trump was being turned down\n",
      "[00:00:53.280 --> 00:00:56.480]   by 30 shirty brokerages and looking for half a billion\n",
      "[00:00:56.480 --> 00:00:58.720]   dollar bonds between the couch cushions,\n",
      "[00:00:58.720 --> 00:01:02.840]   President Biden was quietly building up a $40 million advantage\n",
      "[00:01:02.840 --> 00:01:07.080]   over the Trump campaign, more than double the $33.5 million\n",
      "[00:01:07.080 --> 00:01:10.200]   in former President Donald Trump's campaign account.\n",
      "[00:01:10.200 --> 00:01:12.600]   That is quite the split screen.\n",
      "[00:01:12.600 --> 00:01:14.400]   We are back with David and with Tim,\n",
      "[00:01:14.400 --> 00:01:17.120]   David here is how the New York Times puts it, quote,\n",
      "[00:01:17.120 --> 00:01:19.120]   \"Mr. Trump's legal battles have been a drain\n",
      "[00:01:19.120 --> 00:01:20.800]   \"on his overall election funds.\n",
      "[00:01:20.800 --> 00:01:22.600]   \"He faces four criminal indictments,\n",
      "[00:01:22.600 --> 00:01:25.560]   \"along with civil cases, which are proven costly.\n",
      "[00:01:25.560 --> 00:01:27.360]   \"Last year, committees backing him spent\n",
      "[00:01:27.360 --> 00:01:29.880]   \"at least $50 million on legal expenses,\n",
      "[00:01:29.880 --> 00:01:31.920]   \"and those costs are likely to balloon\n",
      "[00:01:31.920 --> 00:01:34.400]   \"as he prepares for potential trials this year.\"\n",
      "[00:01:34.400 --> 00:01:38.640]   The point is, David, he's already in a financial crush\n",
      "[00:01:38.640 --> 00:01:40.400]   and then you have the bills piling up.\n",
      "[00:01:40.400 --> 00:01:44.560]   Yeah, and I guess the gold tennis shoes didn't sell so well\n",
      "[00:01:44.560 --> 00:01:48.400]   because he also doesn't have places to find revenue\n",
      "[00:01:48.400 --> 00:01:50.800]   other than the Republican National Committee\n",
      "[00:01:50.800 --> 00:01:54.560]   and the different committees that should be working\n",
      "[00:01:54.560 --> 00:01:57.600]   to elect Republican candidates, including Donald Trump.\n",
      "[00:01:57.600 --> 00:02:00.480]   And part of putting Laura Lee, his daughter-in-law\n",
      "[00:02:00.480 --> 00:02:03.920]   at the RNC, was to make sure that they can continue\n",
      "[00:02:03.920 --> 00:02:07.640]   to siphon off resources from Republican activities\n",
      "[00:02:07.640 --> 00:02:09.440]   instead for Donald Trump's legal bills.\n",
      "[00:02:09.440 --> 00:02:11.360]   That is bound to happen.\n",
      "[00:02:11.360 --> 00:02:14.440]   And as we discussed in the previous segment about,\n",
      "[00:02:14.440 --> 00:02:16.440]   so why won't his rich friends bail him out?\n",
      "[00:02:16.440 --> 00:02:17.800]   Why won't people give him money?\n",
      "[00:02:17.800 --> 00:02:20.080]   Because frankly, in the political space,\n",
      "[00:02:20.080 --> 00:02:21.920]   he wants to give money to the RNC\n",
      "[00:02:21.920 --> 00:02:23.520]   just to pay Donald Trump's legal bills.\n",
      "[00:02:23.520 --> 00:02:25.600]   They want to get Republicans elected.\n",
      "[00:02:25.600 --> 00:02:28.400]   And in this case, that's not where the money is going.\n",
      "[00:02:28.400 --> 00:02:30.960]   Donald Trump and Republicans are in a lot\n",
      "[00:02:30.960 --> 00:02:33.400]   of financial trouble going into November.\n",
      "[00:02:33.400 --> 00:02:36.680]   Joe Biden and the Democrats are firing on all cylinders\n",
      "[00:02:36.680 --> 00:02:38.920]   and we'll see that play out for the next few months.\n",
      "[00:02:38.920 --> 00:02:39.920]   - That's exactly the point, which is,\n",
      "[00:02:39.920 --> 00:02:41.520]   it's not just Donald Trump, right?\n",
      "[00:02:41.520 --> 00:02:43.320]   It is now the Republican Party.\n",
      "[00:02:43.320 --> 00:02:45.440]   It is all those down ticket races\n",
      "[00:02:45.440 --> 00:02:47.520]   that we're going to be relying on the RNC,\n",
      "[00:02:47.520 --> 00:02:49.520]   their infrastructure and their dollars\n",
      "[00:02:49.520 --> 00:02:51.680]   that are now going to be depleted by Donald Trump.\n",
      "[00:02:51.680 --> 00:02:55.840]   - Every candidate who surges in either party\n",
      "[00:02:55.840 --> 00:02:59.600]   has a right to leave their imprint on their operatives.\n",
      "[00:02:59.600 --> 00:03:04.040]   The RNC should be at the disposal of the leading candidate.\n",
      "[00:03:04.040 --> 00:03:06.640]   But that candidate should have the interest, as you note,\n",
      "[00:03:06.640 --> 00:03:08.200]   and as David correctly noted,\n",
      "[00:03:08.200 --> 00:03:12.320]   should have the interest of both the party itself\n",
      "[00:03:12.320 --> 00:03:15.400]   and then down ballot people running in for the Senate,\n",
      "[00:03:15.400 --> 00:03:16.720]   the House, et cetera, et cetera.\n",
      "[00:03:16.720 --> 00:03:20.920]   And Donald Trump's never had any concern even prior to this\n",
      "[00:03:20.920 --> 00:03:23.160]   for down ballot candidates unless they were there\n",
      "[00:03:23.160 --> 00:03:24.520]   as loyal as stamps.\n",
      "[00:03:24.520 --> 00:03:26.800]   I think one of the other interesting things\n",
      "[00:03:26.800 --> 00:03:29.400]   so far in the campaign donation data and we're early on,\n",
      "[00:03:29.400 --> 00:03:31.320]   there's, this is going to be a long slog.\n",
      "[00:03:31.320 --> 00:03:33.880]   We'll probably be talking about this in August in September.\n",
      "[00:03:33.880 --> 00:03:38.240]   But he seems to be bleeding small dollar donors.\n",
      "[00:03:38.240 --> 00:03:39.760]   And I think that's very interesting\n",
      "[00:03:39.760 --> 00:03:41.920]   because the argument for Trump was he had\n",
      "[00:03:41.920 --> 00:03:45.760]   such an incendiary impact on average voters\n",
      "[00:03:45.760 --> 00:03:47.120]   that they made up for the fact\n",
      "[00:03:47.120 --> 00:03:50.120]   that institutional Republican money avoided him.\n",
      "[00:03:50.120 --> 00:03:52.480]   And basically, I think in the primary season,\n",
      "[00:03:52.480 --> 00:03:55.640]   you saw that most institutional Republican dollars\n",
      "[00:03:55.640 --> 00:03:57.600]   in the end tried to go to Nikki Haley.\n",
      "[00:03:57.600 --> 00:04:00.280]   And that meant Trump was even more dependent\n",
      "[00:04:00.280 --> 00:04:01.600]   on the small dollar donors.\n",
      "[00:04:01.600 --> 00:04:03.280]   And those are the people that gave money\n",
      "[00:04:03.280 --> 00:04:06.360]   and do his pack that he spent on his legal defense\n",
      "[00:04:06.360 --> 00:04:09.360]   after January 6th and since then.\n",
      "[00:04:09.360 --> 00:04:11.000]   And maybe they've gotten wise.\n",
      "[00:04:11.000 --> 00:04:14.040]   And if they've gotten wise, that's a warning sign for him.\n",
      "[00:04:14.040 --> 00:04:15.120]   - Yeah, politically it is.\n",
      "[00:04:15.120 --> 00:04:17.200]   - It's a very big warning sign politically.\n",
      "[00:04:17.200 --> 00:04:20.080]   It's the financial equivalent of him bleeding voters\n",
      "[00:04:20.080 --> 00:04:23.320]   to Nikki Haley at the 20% to 30% level,\n",
      "[00:04:23.320 --> 00:04:25.120]   even after she's dropped out of the race.\n",
      "[00:04:25.120 --> 00:04:27.080]   - Here's a reminder you do not need,\n",
      "[00:04:27.080 --> 00:04:29.400]   but it's worth throwing out there,\n",
      "[00:04:29.400 --> 00:04:32.280]   which is that his financial troubles are not new.\n",
      "[00:04:32.280 --> 00:04:33.240]   I want you to take a listen.\n",
      "[00:04:33.240 --> 00:04:36.920]   This is from the Fox News Republican debate in 2015.\n",
      "[00:04:36.920 --> 00:04:41.520]   - Trump corporations, casinos and hotels,\n",
      "[00:04:41.520 --> 00:04:44.160]   have declared bankruptcy four times\n",
      "[00:04:44.160 --> 00:04:45.920]   over the last quarter century.\n",
      "[00:04:45.920 --> 00:04:47.760]   - Questions, sir, with that record,\n",
      "[00:04:47.760 --> 00:04:51.320]   why should we trust you to run the nation's business?\n",
      "[00:04:51.320 --> 00:04:54.720]   - Because I have used the laws of this country,\n",
      "[00:04:54.720 --> 00:04:57.760]   just like the greatest people that you read about\n",
      "[00:04:57.760 --> 00:05:00.760]   every day in business have used the laws of this country,\n",
      "[00:05:00.760 --> 00:05:04.320]   the chapter laws, to do a great job for my company,\n",
      "[00:05:04.320 --> 00:05:07.560]   for myself, for my employees, for my family, et cetera.\n",
      "[00:05:07.560 --> 00:05:09.720]   I have never gone bankrupt, by the way.\n",
      "[00:05:09.720 --> 00:05:12.160]   I have never, but out of hundreds of deals--\n",
      "[00:05:12.160 --> 00:05:14.120]   - No, but sir, excuse me, that's your line,\n",
      "[00:05:14.120 --> 00:05:17.000]   but there are companies that have gone bankrupt.\n",
      "[00:05:17.000 --> 00:05:21.160]   - Okay, so appreciate the fact check there at the end.\n",
      "[00:05:21.160 --> 00:05:23.880]   But I also think it is interesting, Tim,\n",
      "[00:05:23.880 --> 00:05:25.320]   that there's a little bit of, he told us,\n",
      "[00:05:25.320 --> 00:05:28.160]   he bends the system to his will.\n",
      "[00:05:28.160 --> 00:05:29.560]   - And therefore, he's successful.\n",
      "[00:05:29.560 --> 00:05:32.080]   So no, no, no, and no again.\n",
      "[00:05:32.080 --> 00:05:34.600]   Donald Trump couldn't make money running a casino.\n",
      "[00:05:34.600 --> 00:05:37.360]   Casino's are cash registers, right?\n",
      "[00:05:37.360 --> 00:05:39.240]   We could probably do that together, Alicia.\n",
      "[00:05:39.240 --> 00:05:40.720]   We could go to a casino--\n",
      "[00:05:40.720 --> 00:05:41.560]   - I do need a plan.\n",
      "[00:05:41.560 --> 00:05:42.760]   - And make a little bit of money.\n",
      "[00:05:42.760 --> 00:05:44.360]   Yeah, we'll pay for your kids' colleges.\n",
      "[00:05:44.360 --> 00:05:45.920]   He couldn't do that.\n",
      "[00:05:45.920 --> 00:05:47.680]   He went into Atlantic City.\n",
      "[00:05:47.680 --> 00:05:49.520]   He had one of the best locations.\n",
      "[00:05:49.520 --> 00:05:53.600]   He got in there early, and he still put it into bankruptcy.\n",
      "[00:05:53.600 --> 00:05:55.720]   He's a serial bankruptcy artist,\n",
      "[00:05:55.720 --> 00:05:58.400]   because he's undisciplined financially.\n",
      "[00:05:58.400 --> 00:06:01.480]   He has this endless appetite to acquire more,\n",
      "[00:06:01.480 --> 00:06:03.200]   even when he doesn't have the money to do it.\n",
      "[00:06:03.200 --> 00:06:05.080]   And he's simply not disciplined.\n",
      "[00:06:05.080 --> 00:06:09.920]   He personally guarantees loans when he shouldn't.\n",
      "[00:06:09.920 --> 00:06:13.160]   And then, rather than simply say,\n",
      "[00:06:13.160 --> 00:06:15.080]   you know what, I learned from my past mistakes,\n",
      "[00:06:15.080 --> 00:06:17.280]   I come into the White House with more financial discipline\n",
      "[00:06:17.280 --> 00:06:18.720]   than I had as a private citizen,\n",
      "[00:06:18.720 --> 00:06:21.840]   because I've been through, you know, I've been through the fire.\n",
      "[00:06:21.840 --> 00:06:24.280]   Instead, he says, yeah, I got away with it.\n",
      "[00:06:24.280 --> 00:06:26.400]   I, you know, I screwed up.\n",
      "[00:06:26.400 --> 00:06:27.960]   I went into bankruptcy, and you know what?\n",
      "[00:06:27.960 --> 00:06:30.520]   The law's protected me like so many other people,\n",
      "[00:06:30.520 --> 00:06:33.400]   but classically successful business people\n",
      "[00:06:33.400 --> 00:06:36.560]   do not make a habit out of touring through bankruptcy court\n",
      "[00:06:36.560 --> 00:06:38.920]   like it's going to Disneyland, and he did.\n",
      "[00:06:38.920 --> 00:06:41.400]   David, if you want to get cut in on this casino deal,\n",
      "[00:06:41.400 --> 00:06:43.000]   I don't want you to feel that because you weren't here\n",
      "[00:06:43.000 --> 00:06:45.680]   with us on set, it wasn't offered to you as an option\n",
      "[00:06:45.680 --> 00:06:47.040]   to get in at the ground level.\n",
      "[00:06:47.040 --> 00:06:49.400]   Let's talk about the split screen that you have here,\n",
      "[00:06:49.400 --> 00:06:51.840]   'cause you have Trump with, you know, bleeding money,\n",
      "[00:06:51.840 --> 00:06:55.720]   and then you got Biden quietly raising a ton of money.\n",
      "[00:06:55.720 --> 00:06:58.160]   That matters, right?\n",
      "[00:06:58.160 --> 00:07:01.560]   Sometimes we get into sort of the optics and this and that.\n",
      "[00:07:01.560 --> 00:07:05.880]   At the end of the day, campaigns run on these dollars,\n",
      "[00:07:05.880 --> 00:07:08.960]   ads run on these dollars, those yard signs\n",
      "[00:07:08.960 --> 00:07:10.440]   that everybody likes to complain about.\n",
      "[00:07:10.440 --> 00:07:13.280]   They pop up in yards with those dollars.\n",
      "[00:07:13.280 --> 00:07:17.360]   It is hard to get over that structural disadvantage.\n",
      "[00:07:17.360 --> 00:07:19.560]   It also is huge for the Biden campaign\n",
      "[00:07:19.560 --> 00:07:22.520]   that they will have this cash advantage.\n",
      "[00:07:22.520 --> 00:07:24.040]   - Oh, that's exactly right.\n",
      "[00:07:24.040 --> 00:07:26.160]   And you can say that Donald Trump might survive\n",
      "[00:07:26.160 --> 00:07:29.520]   more than other candidates in this financial problem\n",
      "[00:07:29.520 --> 00:07:30.960]   when it comes to campaign resources\n",
      "[00:07:30.960 --> 00:07:34.240]   because of his, you know, 100% name recognition and image.\n",
      "[00:07:34.240 --> 00:07:37.160]   However, where resources really do matter in this race\n",
      "[00:07:37.160 --> 00:07:40.840]   is the ability of Joe Biden to micro target and data target,\n",
      "[00:07:40.840 --> 00:07:43.360]   say the 8% of American voters that he needs\n",
      "[00:07:43.360 --> 00:07:45.440]   that will decide this election,\n",
      "[00:07:45.440 --> 00:07:47.320]   his ability to outspend Donald Trump\n",
      "[00:07:47.320 --> 00:07:50.360]   and the electoral college states that he must win.\n",
      "[00:07:50.360 --> 00:07:52.960]   Joe Biden will go at this with a massive advantage.\n",
      "[00:07:52.960 --> 00:07:55.120]   And I would also say on messaging, you know,\n",
      "[00:07:55.120 --> 00:07:57.320]   the idea that we talked about with Harry\n",
      "[00:07:57.320 --> 00:07:58.600]   about whether or not Donald Trump\n",
      "[00:07:58.600 --> 00:08:00.520]   is a successful businessman or not,\n",
      "[00:08:00.520 --> 00:08:02.640]   the fact is Donald Trump was born on third base\n",
      "[00:08:02.640 --> 00:08:04.640]   and stole second and that is a message\n",
      "[00:08:04.640 --> 00:08:07.480]   that Joe Biden can share with voters.\n",
      "[00:08:07.480 --> 00:08:10.640]   Now, I think the other message that comes in the frame here\n",
      "[00:08:10.640 --> 00:08:13.600]   is how desperate Donald Trump is right now\n",
      "[00:08:13.600 --> 00:08:17.400]   to win this election and why that should terrify all of us.\n",
      "[00:08:17.400 --> 00:08:19.560]   He is facing enormous civil judgments.\n",
      "[00:08:19.560 --> 00:08:21.320]   He is facing criminal trials\n",
      "[00:08:21.320 --> 00:08:23.960]   and now possibly personal bankruptcy.\n",
      "[00:08:23.960 --> 00:08:25.760]   Where does he go if he loses?\n",
      "[00:08:25.760 --> 00:08:28.240]   I mean, he's in a lot of hurt if he loses.\n",
      "[00:08:28.240 --> 00:08:30.640]   And he should be in a lot of hurt if he wins,\n",
      "[00:08:30.640 --> 00:08:33.280]   but he sees it as his ticket to absolution\n",
      "[00:08:33.280 --> 00:08:36.160]   and that is what is making him so desperate right now.\n",
      "[00:08:36.160 --> 00:08:39.280]   Donald Trump knows he must win this in November.\n",
      "\n",
      "Transcription executed successfully and saved in /var/home/fraser/machine_learning/whisper.cpp/samples/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "output_txt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.wav.txt'\n",
      "output_vtt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.wav.vtt'\n",
      "output_srt: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.wav.srt'\n",
      "output_lrc: saving output to '/var/home/fraser/machine_learning/whisper.cpp/samples/TcoolNrruwE.wav.lrc'\n",
      "\n",
      "whisper_print_timings:     load time =   235.10 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =   311.25 ms\n",
      "whisper_print_timings:   sample time =  4641.82 ms / 12235 runs (    0.38 ms per run)\n",
      "whisper_print_timings:   encode time =   100.51 ms /    19 runs (    5.29 ms per run)\n",
      "whisper_print_timings:   decode time =    63.72 ms /    12 runs (    5.31 ms per run)\n",
      "whisper_print_timings:   batchd time =  6343.85 ms / 12129 runs (    0.52 ms per run)\n",
      "whisper_print_timings:   prompt time =   917.61 ms /  3996 runs (    0.23 ms per run)\n",
      "whisper_print_timings:    total time = 12672.22 ms\n"
     ]
    }
   ],
   "source": [
    "# transcribe using the base model (great with CUDA enabled whisper.cpp)\n",
    "# produces subtitles too\n",
    "try:\n",
    "    subprocess.run(['transcribe -t 12 -m ' + home_directory + '/machine_learning/whisper.cpp/models/ggml-base.en.bin -f ' \n",
    "                + directory + extracted_audio_file + ' -otxt -ovtt -osrt -olrc'], shell=True, check=True)\n",
    "    print(\"Transcription executed successfully and saved in \" + directory)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Transcription failed with error {e.returncode}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
