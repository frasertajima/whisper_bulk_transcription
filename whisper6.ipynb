{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frasertajima/whisper_bulk_transcription/blob/main/whisper6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nu7s8ajsV-T"
      },
      "outputs": [],
      "source": [
        "# google colab GPU version is extremely fast\n",
        "\n",
        "# https://github.com/openai/whisper\n",
        "# https://www.assemblyai.com/blog/how-to-run-openais-whisper-speech-recognition-model/\n",
        "\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "import whisper\n",
        "import os\n",
        "\n",
        "# adding files to permit automatic download of files\n",
        "# from colab to local computer\n",
        "from google.colab import files\n",
        "\n",
        "def transcribe_audio(filename):\n",
        "    # can be tiny, base, small, medium and large\n",
        "    # colab crashes on large due to memory so use medium\n",
        "    model = whisper.load_model(\"medium\")\n",
        "\n",
        "    # load audio \n",
        "    # \"/content/\" is the directory used in Colab for file downloads (will be different for locally run notebook)\n",
        "    filename = \"/content/\" + filename\n",
        "    print(filename, \"start.\")\n",
        "    result = model.transcribe(filename)\n",
        "\n",
        "    # print transcription\n",
        "    output=result[\"text\"]\n",
        "    print(output)\n",
        "\n",
        "    # save file as markdown file\n",
        "    filename = filename[:-4]\n",
        "    with open(filename+\".md\", 'w') as f:    # selected markdown but can change to \".txt\" if desired\n",
        "        f.write(output)\n",
        "        files.download(filename+\".md\")      # download from colab to local computer\n",
        "                                            # browser may ask for permission to permit multiple downloads\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read all files in specified directory\n",
        "file_list = os.listdir(path=r\"/content\")\n",
        "print(file_list)\n",
        "# transcribe each audio file in the directory\n",
        "# define acceptable filetypes to filter out colab .config and other non-audio files\n",
        "# the filetypes listed are the ones I am using but there could be others\n",
        "# FLAC requires LAC as filename[-3:]\n",
        "filetype = [\"m4a\", \"M4A\", \"wav\", \"WAV\", \"mp3\", \"MP3\", \"lac\", \"LAC\"]\n",
        "\n",
        "index = 0\n",
        "while index < len(file_list):\n",
        "    filename = file_list[index]\n",
        "    print(filename[-3:])\n",
        "    if filename[-3:] in filetype:\n",
        "      print(filename, \"being transcribed...\")\n",
        "      transcribe_audio(filename)\n",
        "    index += 1\n",
        "    \n",
        "print(\"all files transcribed.\")"
      ],
      "metadata": {
        "id": "F8ijo5TjzVcE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('pytorch-directml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "79849614a1f070daf352e0af667e60a43e9406d771f853c326ca0f3fcc214227"
      }
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}